{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d6f265e-e433-48e9-a783-950e8b12260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "158544ca-2767-4bc9-972f-c58b53a8012d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/alexeyorlov53/.netrc\n"
     ]
    }
   ],
   "source": [
    "!python3 -m wandb login eb7b1964fb84cd81de96b2a273ecf2bb6254aeac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb87046b-24e4-4963-ace0-615a30c7ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_base = 'graphormer-base-pcqm4mv1'\n",
    "model_name = 'clefourrier/graphormer-base-pcqm4mv1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74b46e7f-7cec-434c-bece-7436446b0dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d36cd61-adb3-490a-9082-1697fba25a19",
   "metadata": {},
   "source": [
    "### Upload Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6ef798c-7e97-4a0b-98f9-ff50519ff824",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"data_10k_graph.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c802eac3-0860-4705-8cf2-ee916f02f789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage on NaNs:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "y             0.0\n",
       "Smiles        0.0\n",
       "ecfp1         0.0\n",
       "ecfp2         0.0\n",
       "ecfp3         0.0\n",
       "node_feat     0.0\n",
       "edge_index    0.0\n",
       "edge_attr     0.0\n",
       "num_nodes     0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Percentage on NaNs:')\n",
    "dataframe.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bf55334-c3e7-4a68-9f06-9f1be413e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.drop(columns=['Smiles', 'ecfp1', 'ecfp2', 'ecfp3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edbb0890-2585-41a5-b941-002655ce134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_array_column(df, column):\n",
    "    for row in tqdm(range(len(df))):\n",
    "        str_ints = eval(df.iloc[row][column])\n",
    "        df.at[row, column] = str_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "888040a6-edda-4e71-a758-38046902fbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570efae4813c412aa01a68c2255682f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619d316bfcdd4718912bfb032c3805e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be30dfd31ead415abfa17374ac06cc05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7eeeeab65144a4bbc8651cee92f1203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocess_array_column(dataframe, 'node_feat')\n",
    "preprocess_array_column(dataframe, 'edge_index')\n",
    "preprocess_array_column(dataframe, 'edge_attr')\n",
    "preprocess_array_column(dataframe, 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e537cd3-1e45-4e4a-8d58-251adbe3d9c1",
   "metadata": {},
   "source": [
    "### Normalize target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c7a3c9-2987-4ee5-a10a-7ccbcf5b6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['y'] = dataframe['y'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54622e51-94dd-462b-9370-4187f265b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler() # отображает данные в отрезок [0, 1]\n",
    "dataframe['y'] = scaler.fit_transform(dataframe['y'].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4882a00-5fb1-4f95-897e-ee0d84d57176",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['y'] = dataframe['y'].apply(lambda x: [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99f61b38-6727-4acf-98cd-40ceb8695af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [0.06945573071852935]\n",
       "1         [0.0632708525199993]\n",
       "2        [0.14947089947089948]\n",
       "3       [0.048213458054186156]\n",
       "4        [0.07334422834991665]\n",
       "                 ...          \n",
       "9995    [0.044576268125756185]\n",
       "9996     [0.05272345317851575]\n",
       "9997     [0.05001926190777158]\n",
       "9998     [0.05197855908891177]\n",
       "9999    [0.033902462514521677]\n",
       "Name: y, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d58c5-eb7a-4420-b0db-f743e2167213",
   "metadata": {},
   "source": [
    "### Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d5561e6-2477-4ead-9fae-91cae781bfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['y', 'node_feat', 'edge_index', 'edge_attr', 'num_nodes'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['y', 'node_feat', 'edge_index', 'edge_attr', 'num_nodes'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['y', 'node_feat', 'edge_index', 'edge_attr', 'num_nodes'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataset = Dataset.from_pandas(dataframe)\n",
    "train_testvalid = dataset.train_test_split(test_size=0.2, seed=15)\n",
    "\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=15)\n",
    "\n",
    "# 10% for test, 10 for validation, 80% for train\n",
    "dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'validation': test_valid['train']})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5e5a85d-97b8-44cf-8229-fde39c8a68cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321108cd2e6e4bb888479ffa11e4943a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8c036017804b1c96c6547e6ffb01c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3a291b7bd6407babfc7ec997b2687e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers.models.graphormer.collating_graphormer import preprocess_item, GraphormerDataCollator\n",
    "\n",
    "dataset_processed = dataset.map(preprocess_item, batched=False)\n",
    "# data_loader = GraphormerDataCollator(on_the_fly_processing=True) # либо препроцессинг либо коллайтор с on_the_fly_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67f3e1e0-6ecf-480c-8ab4-7d356aeeb782",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_processed.save_to_disk('dataset_10k_graphormer_preprocessed_normilized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0ccf362-6633-4279-86fb-2799d9ab22ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_from_disk\n",
    "# dataset_processed = load_from_disk('dataset_10k_graphormer_preprocessed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a56144-3102-4a21-b86b-8e851dc97026",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3aa5011-5db5-4ede-b9fc-1f7af8128fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel, AutoConfig\n",
    "\n",
    "# config = AutoConfig.from_pretrained(model_name)\n",
    "# AutoModel.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66254640-38dc-4da6-bccf-634174aca108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel, AutoConfig\n",
    "\n",
    "# class MolecularPropertiesRegression(torch.nn.Module):\n",
    "#     def __init__(self, model_name, num_properties):\n",
    "#         super(MolecularPropertiesRegression, self).__init__()\n",
    "#         self.num_properties = num_properties\n",
    "\n",
    "#         config = AutoConfig.from_pretrained(model_name)\n",
    "#         self.transformer = AutoModel.from_pretrained(model_name, config=config)\n",
    "#         # removing last layer of transformer\n",
    "#         self.transformer.pooler = torch.nn.Identity()\n",
    "#         # freezing transformer weights\n",
    "#         for param in self.transformer.parameters():\n",
    "#             param.requires_grad = False\n",
    "#         self.regressor = torch.nn.Linear(768, num_properties)\n",
    "\n",
    "#     def forward(self, input_ids = None, attention_mask=None):\n",
    "#         outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "#         last_hidden_state = outputs[0]\n",
    "#         # last_hidden_state is the shape of (batch_size=32, input_sequence_length=512, hidden_size=768)\n",
    "#         # so we take only hidden emdedding for [CLS] token (first) as it contains the entire context\n",
    "#         # and would be sufficient for simple downstream tasks such as classification/regression\n",
    "#         predicted_property_values = self.regressor(last_hidden_state[:, 0, : ].view(-1, 768))\n",
    "\n",
    "#         return predicted_property_values\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b0c73-a897-407e-a8c7-158283f97de4",
   "metadata": {},
   "source": [
    "### Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "186e336b-018d-4806-a9b9-8cc1e72d1d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.graphormer.collating_graphormer import GraphormerDataCollator\n",
    "\n",
    "class GraphormerDataCollator_():\n",
    "    def __init__(self):\n",
    "        self.data_collator = GraphormerDataCollator()\n",
    "\n",
    "    def __call__(self, features):\n",
    "        for mol in features:\n",
    "            if mol['num_nodes'] == 1:\n",
    "                features.remove(mol)\n",
    "        return self.data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79747c27-653a-486a-ab4d-3f2c6ec347f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_collator = GraphormerDataCollator_()\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset_processed['train'], shuffle = False, batch_size = batch_size, collate_fn = data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    dataset_processed['validation'], shuffle = False, batch_size = batch_size, collate_fn = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ba7ac61-1655-43f6-892e-46d0d54b05e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\", index=5) if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08055762-1a53-49e6-8cd7-71161852abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GraphormerForGraphClassification\n",
    "\n",
    "model = GraphormerForGraphClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_classes=1,\n",
    "    ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1428e13-d841-45e7-a885-919d4670a102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphormerForGraphClassification(\n",
       "  (encoder): GraphormerModel(\n",
       "    (graph_encoder): GraphormerGraphEncoder(\n",
       "      (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "      (graph_node_feature): GraphormerGraphNodeFeature(\n",
       "        (atom_encoder): Embedding(4609, 768, padding_idx=0)\n",
       "        (in_degree_encoder): Embedding(512, 768, padding_idx=0)\n",
       "        (out_degree_encoder): Embedding(512, 768, padding_idx=0)\n",
       "        (graph_token): Embedding(1, 768)\n",
       "      )\n",
       "      (graph_attn_bias): GraphormerGraphAttnBias(\n",
       "        (edge_encoder): Embedding(1537, 32, padding_idx=0)\n",
       "        (edge_dis_encoder): Embedding(131072, 1)\n",
       "        (spatial_pos_encoder): Embedding(512, 32, padding_idx=0)\n",
       "        (graph_token_virtual_distance): Embedding(1, 32)\n",
       "      )\n",
       "      (emb_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x GraphormerGraphEncoderLayer(\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (activation_dropout_module): Dropout(p=0.1, inplace=False)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn): GraphormerMultiheadAttention(\n",
       "            (attention_dropout_module): Dropout(p=0.1, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (fc2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head_transform_weight): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation_fn): GELUActivation()\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): GraphormerDecoderHead(\n",
       "    (classifier): Linear(in_features=768, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04001413-80f4-4663-ab5d-a107e839c5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "num_training_steps = num_epoch * len(train_dataloader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    'linear',\n",
    "    optimizer = optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = num_training_steps,\n",
    ")\n",
    "\n",
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f78e178-f7c1-4fc0-b8ad-5c5d5e03281f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33morlov-aleksei53\u001b[0m (\u001b[33mmoleculary-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alexeyorlov53/Transformers-for-Molecules/graphormer/wandb/run-20240412_005415-uxqlwcb8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/moleculary-ai/graphormer/runs/uxqlwcb8' target=\"_blank\">Graphormer Simple Classification on MolecularWeight 10k 10_epochs</a></strong> to <a href='https://wandb.ai/moleculary-ai/graphormer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/moleculary-ai/graphormer' target=\"_blank\">https://wandb.ai/moleculary-ai/graphormer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/moleculary-ai/graphormer/runs/uxqlwcb8' target=\"_blank\">https://wandb.ai/moleculary-ai/graphormer/runs/uxqlwcb8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/moleculary-ai/graphormer/runs/uxqlwcb8?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4ff8539810>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"graphormer\",\n",
    "    name=\"Graphormer Simple Classification on MolecularWeight 10k 100_epochs\",\n",
    "    config={}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a951a1-7c75-47bd-b275-9693eb22fe95",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f0f67d5-c7f4-4ea0-959f-b1a990387a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e6e837bd38644d49ed3aac39a0ad99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d72772c12324adebd766941216cdb42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar_train = tqdm(range(num_training_steps))\n",
    "progress_bar_eval = tqdm(range(num_epoch * len(eval_dataloader)))\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    model.train()\n",
    "    train_epoch_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_batch = { k: v.to(device) for k, v in batch.items() }\n",
    "        \n",
    "        outputs = model(**input_batch)\n",
    "        \n",
    "        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "        loss.backward()\n",
    "        train_epoch_loss += loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar_train.update(1)\n",
    "\n",
    "    model.eval()\n",
    "    eval_epoch_loss = 0\n",
    "    for batch in eval_dataloader:\n",
    "        input_batch = { k: v.to(device) for k, v in batch.items() }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**input_batch)\n",
    "\n",
    "        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "        eval_epoch_loss += loss.item()\n",
    "\n",
    "        progress_bar_eval.update(1)\n",
    "    \n",
    "    wandb.log({\"loss/train\": train_epoch_loss / len(train_dataloader), \"loss/validation\": eval_epoch_loss / len(eval_dataloader)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1148d2f3-1419-4f4d-a17d-56139aa6064e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages/transformers/models/graphormer/modeling_graphormer.py:399: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not (embedding_dim == self.embedding_dim):\n",
      "/home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages/transformers/models/graphormer/modeling_graphormer.py:404: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not (list(query.size()) == [tgt_len, bsz, embedding_dim]):\n",
      "/home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages/transformers/models/graphormer/modeling_graphormer.py:410: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (key_bsz != bsz) or (value is None) or not (src_len, bsz == value.shape[:2]):\n",
      "/home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages/transformers/models/graphormer/modeling_graphormer.py:427: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (k is None) or not (k.size(1) == src_len):\n",
      "/home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages/transformers/models/graphormer/modeling_graphormer.py:436: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if key_padding_mask.size(0) != bsz or key_padding_mask.size(1) != src_len:\n",
      "/home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages/transformers/models/graphormer/modeling_graphormer.py:443: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if list(attn_weights.size()) != [bsz * self.num_heads, tgt_len, src_len]:\n",
      "/home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages/transformers/models/graphormer/modeling_graphormer.py:471: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if list(attn.size()) != [bsz * self.num_heads, tgt_len, self.head_dim]:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/alexeyorlov53/Transformers-for-Molecules/graphormer/wandb/run-20240412_005415-uxqlwcb8/files/graphormer-base-pcqm4mv1_10k_10_epochs.onnx']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.onnx.export(model, input_batch, model_name_base + \"_10k_100_epochs.onnx\")\n",
    "wandb.save(model_name_base + \"_10k_100_epochs.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da956be4-4edd-4123-aa7b-34b7dfb2973b",
   "metadata": {},
   "source": [
    "## Post Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "472d65db-87eb-4dfb-99bb-412126824fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac73879d61b4000a4bb6451088399b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    dataset_processed['test'], batch_size = batch_size, collate_fn = data_collator\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "epoch_loss = 0\n",
    "for batch in tqdm(test_dataloader):\n",
    "        input_batch = { k: v.to(device) for k, v in batch.items() }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**input_batch)\n",
    "\n",
    "        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "wandb.log({\"loss/test\": epoch_loss / len(test_dataloader)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a0f7733-440e-48e2-bd6a-f88e362ce757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='181.616 MB of 181.616 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss/test</td><td>▁</td></tr><tr><td>loss/train</td><td>█▃▂▂▂▂▁▁▁▁</td></tr><tr><td>loss/validation</td><td>█▃▂▂▅▃▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss/test</td><td>38086.50964</td></tr><tr><td>loss/train</td><td>35185.91918</td></tr><tr><td>loss/validation</td><td>46093.69094</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Graphormer Simple Classification on MolecularWeight 10k 10_epochs</strong> at: <a href='https://wandb.ai/moleculary-ai/graphormer/runs/uxqlwcb8' target=\"_blank\">https://wandb.ai/moleculary-ai/graphormer/runs/uxqlwcb8</a><br/> View job at <a href='https://wandb.ai/moleculary-ai/graphormer/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2MTI3OTkxMQ==/version_details/v6' target=\"_blank\">https://wandb.ai/moleculary-ai/graphormer/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2MTI3OTkxMQ==/version_details/v6</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240412_005415-uxqlwcb8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fb0858-1e3f-4458-99b5-7b141756ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_name_base + '_10k_100epochs unnormalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a38ad7b9-f82b-4936-bd4f-26266cbb6b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, model_name_base + '_10k_10epochs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57a96b63-ec67-4374-968e-bd71ff08c91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af68b4a-8c8a-44b8-9cb2-345f1abdda1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
