{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 'ecfp0'\n",
    "folder = 'ecfps_full'\n",
    "filename = folder + '/' + radius\n",
    "samples_count = '10M'\n",
    "model_name = f'molberto_{radius}_{samples_count}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename + '.txt', 'r') as fp:\n",
    "    text = fp.read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save this data to file as several *plaintext* files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "def split_into_many_files(filename: str, text: list):\n",
    "    \"\"\"\n",
    "    Cuts big file 'text' into small files with 10000 sentences.\n",
    "    These small files will be fed into tokenizer (to train it).\n",
    "    File 'text' should consist of sentences on each line.\n",
    "    \"\"\"\n",
    "    if not os.path.exists('data'):\n",
    "        os.mkdir('data')\n",
    "    if not os.path.exists(folder):\n",
    "        os.mkdir(folder)\n",
    "    if not os.path.exists(f'data/{filename}'):\n",
    "        os.mkdir(f'data/{filename}')\n",
    "        \n",
    "    text_data = []\n",
    "    file_count = 0\n",
    "\n",
    "    for sample in tqdm(text):\n",
    "        sample = sample.replace('\\n', '')\n",
    "        text_data.append(sample)\n",
    "        if len(text_data) == 10_000:\n",
    "            # once we git the 10K mark, save to file\n",
    "            with open(f'data/{filename}/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "                fp.write('\\n'.join(text_data))\n",
    "            text_data = []\n",
    "            file_count += 1\n",
    "    with open(f'data/{filename}/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "        fp.write('\\n'.join(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3601cdf2f54fbba6dd5b782245aac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "split_into_many_files(filename, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "paths = [str(x) for x in Path(f'data/{filename}').glob('*.txt')]\n",
    "\n",
    "len(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move onto training the tokenizer. We use a byte-level Byte-pair encoding (BPE) tokenizer. This allows us to build the vocabulary from an alphabet of single bytes, meaning all words will be decomposable into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(files=paths, vocab_size=30_522, min_frequency=2,\n",
    "                special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['molberto_efcp1_10M/vocab.json', 'molberto_efcp1_10M/merges.txt']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir(model_name)\n",
    "\n",
    "tokenizer.save_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two files that outline our new tokenizer:\n",
    "\n",
    "* the *vocab.json* - a mapping file between tokens to token IDs\n",
    "\n",
    "* and *merges.txt* - which describes which characters/set of characters can be decomposed/composed smaller/larger tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test our tokenizer on a simple sentence\n",
    "tokens = tokenizer('2246728737 864674487 3217380708 2041434490 3218693969 3217380708 847961216 2246699815 864942730 847961216 3217380708 3218693969 2041434490 3217380708 3218693969 3218693969 3217380708 2245900962 847433064 3218693969 3217380708 3217380708 2092489639 2968968094 2968968094 3189457552 2968968094 2976033787 2246728737 3218693969 3217380708 1016841875 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 344, 348, 279, 333, 273, 279, 337, 318, 307, 337, 279, 273, 333, 279, 273, 273, 279, 430, 462, 273, 279, 279, 341, 294, 294, 368, 294, 327, 320, 273, 279, 394, 225, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 344,\n",
       " 348,\n",
       " 279,\n",
       " 333,\n",
       " 273,\n",
       " 279,\n",
       " 337,\n",
       " 318,\n",
       " 307,\n",
       " 337,\n",
       " 279,\n",
       " 273,\n",
       " 333,\n",
       " 279,\n",
       " 273,\n",
       " 273,\n",
       " 279,\n",
       " 430,\n",
       " 462,\n",
       " 273,\n",
       " 279,\n",
       " 279,\n",
       " 341,\n",
       " 294,\n",
       " 294,\n",
       " 368,\n",
       " 294,\n",
       " 327,\n",
       " 320,\n",
       " 273,\n",
       " 279,\n",
       " 394,\n",
       " 225,\n",
       " 2]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that our **<s\\>** token is now placed at the beginning of our sequences using token ID *0*. At the end of the sequence we see the **<s\\\\>** token represented by *2*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
