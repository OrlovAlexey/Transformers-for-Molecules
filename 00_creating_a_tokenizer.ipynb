{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 'ecfp0'\n",
    "folder = 'ecfps_full'\n",
    "filename = folder + '/' + radius\n",
    "samples_count = '2M'\n",
    "model_name = f'molberto_{radius}_{samples_count}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename + '.txt', 'r') as fp:\n",
    "    text = fp.read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save this data to file as several *plaintext* files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "folder_cutted = 'ecfps_cutted'\n",
    "folder_with_paths = folder_cutted + '/' + radius\n",
    "\n",
    "def split_into_many_files(filename: str, text: list):\n",
    "    \"\"\"\n",
    "    Cuts big file 'text' into small files with 10000 sentences.\n",
    "    These small files will be fed into tokenizer (to train it).\n",
    "    File 'text' should consist of sentences on each line.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_cutted):\n",
    "        os.mkdir(folder_cutted)\n",
    "    if not os.path.exists(folder_with_paths):\n",
    "        os.mkdir(folder_with_paths)\n",
    "        \n",
    "    text_data = []\n",
    "    file_count = 0\n",
    "\n",
    "    for sample in tqdm(text):\n",
    "        sample = sample.replace('\\n', '')\n",
    "        text_data.append(sample)\n",
    "        if len(text_data) == 10_000:\n",
    "            # once we git the 10K mark, save to file\n",
    "            with open(f'{folder_with_paths}/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "                fp.write('\\n'.join(text_data))\n",
    "            text_data = []\n",
    "            file_count += 1\n",
    "    with open(f'{folder_with_paths}/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "        fp.write('\\n'.join(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_into_many_files(filename, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "paths = [str(x) for x in Path(folder_with_paths).glob('*.txt')]\n",
    "\n",
    "len(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move onto training the tokenizer. We use a byte-level Byte-pair encoding (BPE) tokenizer. This allows us to build the vocabulary from an alphabet of single bytes, meaning all words will be decomposable into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files=paths, vocab_size=30_522, min_frequency=2,\n",
    "                special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.mkdir(model_name)\n",
    "\n",
    "# tokenizer.save_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two files that outline our new tokenizer:\n",
    "\n",
    "* the *vocab.json* - a mapping file between tokens to token IDs\n",
    "\n",
    "* and *merges.txt* - which describes which characters/set of characters can be decomposed/composed smaller/larger tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test our tokenizer on a simple sentence\n",
    "tokens = tokenizer('2246728737 864674487 3217380708 3218693969 3218693969 3218693969 3218693969 3217380708 2245900962 847433064')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that our **<s\\>** token is now placed at the beginning of our sequences using token ID *0*. At the end of the sequence we see the **<s\\\\>** token represented by *2*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
