{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install tensorflow\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import os\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m wandb login eb7b1964fb84cd81de96b2a273ecf2bb6254aeac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 'ecfp0'\n",
    "folder = 'ecfps_cutted'\n",
    "folder_with_paths = folder + '/' + radius\n",
    "samples_count = '2M'\n",
    "model_name = f'molberto_{radius}_{samples_count}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the tokenizer using the tokenizer we initialized and saved to file\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm(tensor):\n",
    "    # create random array of floats with equal dims to tensor\n",
    "    rand = torch.rand(tensor.shape)\n",
    "    # mask random 15% where token is not 0 <s>, 1 <pad>, or 2 <s/>\n",
    "    mask_arr = (rand < .15) * (tensor != 0) * (tensor != 1) * (tensor != 2)\n",
    "    # loop through each row in tensor (cannot do in parallel)\n",
    "    for i in range(tensor.shape[0]):\n",
    "        # get indices of mask positions from mask array\n",
    "        selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        # mask tensor\n",
    "        tensor[i, selection] = 4\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [str(x) for x in Path(folder_with_paths).glob('*.txt')]\n",
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take sentences for training\n",
    "def tokenize_ecfps(start, end):\n",
    "    input_ids = []\n",
    "    mask = []\n",
    "    labels = []\n",
    "    for path in tqdm(paths[start:end]):\n",
    "        with open(path, 'r', encoding='utf-8') as fp:\n",
    "            lines = fp.read().split('\\n')\n",
    "        sample = tokenizer(lines, max_length=512, truncation=True, padding='max_length')\n",
    "        labels.append(torch.tensor(sample.input_ids))\n",
    "        mask.append(torch.tensor(sample.attention_mask))\n",
    "        input_ids.append(mlm(labels[-1].detach().clone())) # mask ~15% of tokens to create inputs\n",
    "    \n",
    "    input_ids = torch.cat(input_ids)\n",
    "    mask = torch.cat(mask)\n",
    "    labels = torch.cat(labels)\n",
    "    return input_ids, mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids, train_mask, train_labels = tokenize_ecfps(0, 200)\n",
    "validation_input_ids, validation_mask, validation_labels = tokenize_ecfps(200, 220)\n",
    "test_input_ids, test_mask, test_labels = tokenize_ecfps(220, 238)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_input_ids.shape)\n",
    "print(validation_input_ids.shape)\n",
    "print(validation_input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(input_ids, 'molberto_training/input_ids.pt')\n",
    "# torch.save(mask, 'molberto_training/attention_mask.pt')\n",
    "# torch.save(labels, 'molberto_training/labels.pt')\n",
    "\n",
    "# del input_ids, mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.load('molberto_training/input_ids.pt')\n",
    "# mask = torch.load('molberto_training/attention_mask.pt')\n",
    "# labels = torch.load('molberto_training/labels.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {key: tensor[i] for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset({'input_ids': train_input_ids, 'attention_mask': train_mask, 'labels': train_labels})\n",
    "validation_dataset = Dataset({'input_ids': validation_input_ids, 'attention_mask': validation_mask, 'labels': validation_labels})\n",
    "test_dataset = Dataset({'input_ids': test_input_ids, 'attention_mask': test_mask, 'labels': test_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And move onto building our model, we first need to create a RoBERTa config object, which will describe which features we want to initialize our RoBERTa model with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=30_522,  # we align this to the tokenizer vocab set in previous notebook\n",
    "    max_position_embeddings=514,\n",
    "    hidden_size=768,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we import and initialize a RoBERTa model with a language modeling head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we move onto training. First we setup GPU/CPU usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda', index=2) if torch.cuda.is_available() else torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate the training mode of our model, and initialize our optimizer (Adam with weighted decay - reduces chance of overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AdamW\n",
    "\n",
    "# activate training mode\n",
    "model.train()\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"efcp_transformer\",\n",
    "    name=\"RobertaForMLM on ecpf0 training (2M)\",\n",
    "    config={}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move onto the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # for our progress bar\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "epochs = 1\n",
    "step = 0\n",
    "\n",
    "validation_iterator = iter(validation_loader)\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        true_labels = batch['labels'].numpy().flatten()\n",
    "        pred_labels = torch.nn.functional.softmax(logits, dim=1).argmax(axis=-1).cpu().detach().numpy().flatten()\n",
    "\n",
    "        # write down loss and metrics\n",
    "        wandb.log({\"loss/train\": loss}, step=step)\n",
    "        wandb.log({\"accuracy/train\": accuracy_score(true_labels, pred_labels)}, step=step)\n",
    "        wandb.log({\"f1/train\": f1_score(true_labels, pred_labels, average='micro')}, step=step)\n",
    "        wandb.log({\"precision/train\": precision_score(true_labels, pred_labels, average='micro')}, step=step)\n",
    "        wandb.log({\"recall/train\": recall_score(true_labels, pred_labels, average='micro')}, step=step)\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                validation_batch = next(validation_iterator)\n",
    "            except StopIteration:\n",
    "                validation_dataset = Dataset({'input_ids': validation_input_ids, 'attention_mask': validation_mask, 'labels': validation_labels})\n",
    "                validation_batch = iter(validation_dataset)\n",
    "                validation_batch = next(validation_iterator)\n",
    "            \n",
    "            input_ids = validation_batch['input_ids'].to(device)\n",
    "            attention_mask = validation_batch['attention_mask'].to(device)\n",
    "            labels = validation_batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            true_labels = batch['labels'].numpy().flatten()\n",
    "            pred_labels = torch.nn.functional.softmax(logits, dim=1).argmax(axis=-1).cpu().detach().numpy().flatten()\n",
    "    \n",
    "            # write down loss and metrics\n",
    "            wandb.log({\"loss/validation\": loss}, step=step)\n",
    "            wandb.log({\"accuracy/validation\": accuracy_score(true_labels, pred_labels)}, step=step)\n",
    "            wandb.log({\"f1/validation\": f1_score(true_labels, pred_labels, average='micro')}, step=step)\n",
    "            wandb.log({\"precision/validation\": precision_score(true_labels, pred_labels, average='micro')}, step=step)\n",
    "            wandb.log({\"recall/validation\": recall_score(true_labels, pred_labels, average='micro')}, step=step)\n",
    "            \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        if step > 2500:\n",
    "            break\n",
    "        step += len(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"efcp_transformer\",\n",
    "    name=\"RobertaForMLM on ecpf0 testing (2M)\",\n",
    "    config={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    loop = tqdm(test_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        true_labels = batch['labels'].numpy().flatten()\n",
    "        pred_labels = torch.nn.functional.softmax(logits, dim=1).argmax(axis=-1).cpu().detach().numpy().flatten()\n",
    "\n",
    "        # write down loss and metrics\n",
    "        wandb.log({\"loss/test\": loss}, step=step)\n",
    "        wandb.log({\"accuracy/test\": accuracy_score(true_labels, pred_labels)}, step=step)\n",
    "        wandb.log({\"f1/test\": f1_score(true_labels, pred_labels, average='micro')}, step=step)\n",
    "        wandb.log({\"precision/test\": precision_score(true_labels, pred_labels, average='micro')}, step=step)\n",
    "        wandb.log({\"recall/test\": recall_score(true_labels, pred_labels, average='micro')}, step=step)\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        step += len(batch)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f'molberto_{radius}_2.5K_dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device('cuda', index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "016d71ae2c9b42a99972518d58922afe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0183f80dbd7147e59c5ea167fd358eb4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "10961ed56fee4f00990a89a330176da9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_67ff7a37543e4f0f84570ccf77272c03",
       "style": "IPY_MODEL_e2ae3258df59414dae4afde9a4403c05",
       "value": " 200/200 [14:20&lt;00:00,  4.25s/it]"
      }
     },
     "11a85dd86a6b4560a42daaed52d8218c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "15619546dc27480a932cb5c48e9c1cec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1cb7b69d7cbc48c49e31bb548e44da16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_11a85dd86a6b4560a42daaed52d8218c",
       "style": "IPY_MODEL_ea6fe0a3f0cc4e1cb801b3bbb41b09e7",
       "value": "0.022 MB of 0.022 MB uploaded\r"
      }
     },
     "2b00bca2e8ab458381cbdeaed09cf754": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_0183f80dbd7147e59c5ea167fd358eb4",
       "max": 18,
       "style": "IPY_MODEL_7dda36467f614588b648a2c90eac692d",
       "value": 18
      }
     },
     "2d67a2617f2643a58b2d06ac815c444d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "32a5a124dce74b0bafda4d8dd3b5d8f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "33801982d7264e02bc591c630109a1fc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "33fcbfefec8846b78026ecc62b815c28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_9ed8665e6c474791818061623eacc200",
       "style": "IPY_MODEL_8d8dec0c60fc43e1b0cabca1aba9b1f4"
      }
     },
     "37e2302bd81146d291ba0dbae8606773": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_91e17ac3f7aa4a06bb769e8539a5786f",
       "style": "IPY_MODEL_77d330bb92884b58b259347522fecefe",
       "value": "100%"
      }
     },
     "3a0b7a5fa41645c699a41be696930434": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3ad1efef5ec24e4192dc0c8eb2eab8e3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3c2574af8b204a37a7bf6d46a4920e38": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "410a0e376aea429a8d015b50dc2f3773": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_849a4469d8884b729e6576bfeefbd8bb",
       "style": "IPY_MODEL_a49cd6dbba8f4171860ee48312dd4d74",
       "value": " 20/20 [01:27&lt;00:00,  4.31s/it]"
      }
     },
     "4122ef6e9d4a471a948e401971ab22ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "45f6bff1e0b8498bb61c5bd399554901": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "47e227db38954ae3a916d9c9bd6daa30": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "598cad7b23ee44dba5e2448a7ec5bb15": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5a01d2993a03407083c9a7e78a0565d1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "612668fcfa6c4cbb9fb9f5f7600230dc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "67ff7a37543e4f0f84570ccf77272c03": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6a72d97f0dac4b2d866147984599fc2f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6bb85d875d634f25bce7d31dd4dca0ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6bd5ea25a0bc468d8e2fc914328c77f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2d67a2617f2643a58b2d06ac815c444d",
       "style": "IPY_MODEL_8f4b03505897465e9cfe11bc435db318",
       "value": "100%"
      }
     },
     "6f7611f663cc4e65a0121dfbd0f760be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "73c804b326b948c6a4f4b97a97993827": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_e260280ac6be4bd08ac2ff86a1787739",
       "max": 1,
       "style": "IPY_MODEL_32a5a124dce74b0bafda4d8dd3b5d8f5",
       "value": 1
      }
     },
     "75d2cc150cee4d1582a47384496aa28c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_33fcbfefec8846b78026ecc62b815c28",
        "IPY_MODEL_c1e1bffc30e34a6ba6cce4329936cecb"
       ],
       "layout": "IPY_MODEL_4122ef6e9d4a471a948e401971ab22ec"
      }
     },
     "77d330bb92884b58b259347522fecefe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7849bc613cac48c7ab165e6e6459a40f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_33801982d7264e02bc591c630109a1fc",
       "max": 1,
       "style": "IPY_MODEL_15619546dc27480a932cb5c48e9c1cec",
       "value": 1
      }
     },
     "7dda36467f614588b648a2c90eac692d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "81ac781da132449eb050c9190ef59390": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_cb01188c649047eaa816b92283041b92",
        "IPY_MODEL_bf745de79ba4426892b29f8bf747ded8",
        "IPY_MODEL_10961ed56fee4f00990a89a330176da9"
       ],
       "layout": "IPY_MODEL_3a0b7a5fa41645c699a41be696930434"
      }
     },
     "849a4469d8884b729e6576bfeefbd8bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "84c90b055f5142678abf90ece8508953": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5a01d2993a03407083c9a7e78a0565d1",
       "style": "IPY_MODEL_9da6b11567da4b19a0e758d7c683b273",
       "value": " 18/18 [01:17&lt;00:00,  4.29s/it]"
      }
     },
     "8d8dec0c60fc43e1b0cabca1aba9b1f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "8f4b03505897465e9cfe11bc435db318": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "91e17ac3f7aa4a06bb769e8539a5786f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9da6b11567da4b19a0e758d7c683b273": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9ed8665e6c474791818061623eacc200": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9ee20ef5bb79497e93620df5b711355e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a36c55dd05494729a67800f5f75288cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_6a72d97f0dac4b2d866147984599fc2f",
       "style": "IPY_MODEL_3c2574af8b204a37a7bf6d46a4920e38",
       "value": "Waiting for wandb.init()...\r"
      }
     },
     "a49cd6dbba8f4171860ee48312dd4d74": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a6624e47796f4041ad942302e898f5b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a9037d9450cd422eb3f11697a5e4a2bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_6bd5ea25a0bc468d8e2fc914328c77f0",
        "IPY_MODEL_2b00bca2e8ab458381cbdeaed09cf754",
        "IPY_MODEL_84c90b055f5142678abf90ece8508953"
       ],
       "layout": "IPY_MODEL_bc69603a2dd54fdb9db2b94f43949ce1"
      }
     },
     "b52d99bedb5449fd80e213d7bcfff5c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "bc69603a2dd54fdb9db2b94f43949ce1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bf745de79ba4426892b29f8bf747ded8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_3ad1efef5ec24e4192dc0c8eb2eab8e3",
       "max": 200,
       "style": "IPY_MODEL_d6dc0a9d6c17440c9016a5ef7ce5a525",
       "value": 200
      }
     },
     "c030b1777b4847eb8678c94309687ca5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_47e227db38954ae3a916d9c9bd6daa30",
       "max": 1,
       "style": "IPY_MODEL_45f6bff1e0b8498bb61c5bd399554901",
       "value": 1
      }
     },
     "c1e1bffc30e34a6ba6cce4329936cecb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_016d71ae2c9b42a99972518d58922afe",
       "max": 1,
       "style": "IPY_MODEL_6bb85d875d634f25bce7d31dd4dca0ad"
      }
     },
     "c7117a32d22e44ccbcea51699d64ea50": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_a6624e47796f4041ad942302e898f5b5",
       "max": 20,
       "style": "IPY_MODEL_e0e5d6c96e7847169ecc5f9680fe9882",
       "value": 20
      }
     },
     "c774cd85cec0413f889a536f559caaa0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_37e2302bd81146d291ba0dbae8606773",
        "IPY_MODEL_c7117a32d22e44ccbcea51699d64ea50",
        "IPY_MODEL_410a0e376aea429a8d015b50dc2f3773"
       ],
       "layout": "IPY_MODEL_f5fbded3e0ef4f129a7efec86d9204eb"
      }
     },
     "cb01188c649047eaa816b92283041b92": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9ee20ef5bb79497e93620df5b711355e",
       "style": "IPY_MODEL_fef2d80b54564bdc9ca6de3149b106da",
       "value": "100%"
      }
     },
     "d6dc0a9d6c17440c9016a5ef7ce5a525": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e0e5d6c96e7847169ecc5f9680fe9882": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e260280ac6be4bd08ac2ff86a1787739": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e2ae3258df59414dae4afde9a4403c05": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e3932ebdb7984792be4f28674365e815": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a36c55dd05494729a67800f5f75288cd",
        "IPY_MODEL_c030b1777b4847eb8678c94309687ca5"
       ],
       "layout": "IPY_MODEL_6f7611f663cc4e65a0121dfbd0f760be"
      }
     },
     "ea6fe0a3f0cc4e1cb801b3bbb41b09e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "f2f64772bf2c4b5ab69a81a5ac310661": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_612668fcfa6c4cbb9fb9f5f7600230dc",
       "style": "IPY_MODEL_b52d99bedb5449fd80e213d7bcfff5c6",
       "value": "0.026 MB of 0.026 MB uploaded (0.004 MB deduped)\r"
      }
     },
     "f5fbded3e0ef4f129a7efec86d9204eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fd27d4856a6f4c9aa40bfd5d24f43109": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fef2d80b54564bdc9ca6de3149b106da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
