{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (1.23.5)\n",
      "Requirement already satisfied: torch in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (2.2.1)\n",
      "Requirement already satisfied: filelock in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: tensorflow in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (2.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (0.4.24)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (1.48.2)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (2.12.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from jax>=0.3.15->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: scipy>=1.9 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from jax>=0.3.15->tensorflow) (1.11.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.6.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.5.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.6.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: wandb in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (0.16.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from wandb) (3.1.42)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from wandb) (1.40.6)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: setproctitle in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install tensorflow\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import os\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/alexeyorlov53/.netrc\n"
     ]
    }
   ],
   "source": [
    "!python3 -m wandb login eb7b1964fb84cd81de96b2a273ecf2bb6254aeac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 'ecfp0'\n",
    "folder = 'ecfps_cutted'\n",
    "folder_with_paths = folder + '/' + radius\n",
    "samples_count = '2M'\n",
    "model_name = f'molberto_{radius}_{samples_count}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the tokenizer using the tokenizer we initialized and saved to file\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm(tensor):\n",
    "    # create random array of floats with equal dims to tensor\n",
    "    rand = torch.rand(tensor.shape)\n",
    "    # mask random 15% where token is not 0 <s>, 1 <pad>, or 2 <s/>\n",
    "    mask_arr = (rand < .15) * (tensor != 0) * (tensor != 1) * (tensor != 2)\n",
    "    # loop through each row in tensor (cannot do in parallel)\n",
    "    for i in range(tensor.shape[0]):\n",
    "        # get indices of mask positions from mask array\n",
    "        selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        # mask tensor\n",
    "        tensor[i, selection] = 4\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = [str(x) for x in Path(folder_with_paths).glob('*.txt')]\n",
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take sentences for training\n",
    "def tokenize_ecfps(start, end):\n",
    "    input_ids = []\n",
    "    mask = []\n",
    "    labels = []\n",
    "    for path in tqdm(paths[start:end]):\n",
    "        with open(path, 'r', encoding='utf-8') as fp:\n",
    "            lines = fp.read().split('\\n')\n",
    "        sample = tokenizer(lines, max_length=512, truncation=True, padding='max_length')\n",
    "        labels.append(torch.tensor(sample['input_ids']))\n",
    "        mask.append(torch.tensor(sample['attention_mask']))\n",
    "        input_ids.append(mlm(labels[-1].detach().clone())) # mask ~15% of tokens to create inputs\n",
    "    \n",
    "    input_ids = torch.cat(input_ids)\n",
    "    mask = torch.cat(mask)\n",
    "    labels = torch.cat(labels)\n",
    "    return input_ids, mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ac781da132449eb050c9190ef59390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c774cd85cec0413f889a536f559caaa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9037d9450cd422eb3f11697a5e4a2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_input_ids, train_mask, train_labels = tokenize_ecfps(0, 200)\n",
    "validation_input_ids, validation_mask, validation_labels = tokenize_ecfps(200, 220)\n",
    "test_input_ids, test_mask, test_labels = tokenize_ecfps(220, 238)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1992675, 512])\n",
      "torch.Size([200000, 512])\n",
      "torch.Size([200000, 512])\n"
     ]
    }
   ],
   "source": [
    "print(train_input_ids.shape)\n",
    "print(validation_input_ids.shape)\n",
    "print(validation_input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0, 344, 348, 279, 273, 279, 298, 348, 318, 307, 414, 320, 320, 320,\n",
       "        279, 279, 279, 298, 348, 318, 307, 414, 320, 320, 320, 273, 279, 348,\n",
       "        320, 279, 279, 368, 294, 368, 279, 279, 368, 294, 368, 225,   2,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(input_ids, 'molberto_training/input_ids.pt')\n",
    "# torch.save(mask, 'molberto_training/attention_mask.pt')\n",
    "# torch.save(labels, 'molberto_training/labels.pt')\n",
    "\n",
    "# del input_ids, mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.load('molberto_training/input_ids.pt')\n",
    "# mask = torch.load('molberto_training/attention_mask.pt')\n",
    "# labels = torch.load('molberto_training/labels.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {key: tensor[i] for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset({'input_ids': train_input_ids, 'attention_mask': train_mask, 'labels': train_labels})\n",
    "validation_dataset = Dataset({'input_ids': validation_input_ids, 'attention_mask': validation_mask, 'labels': validation_labels})\n",
    "test_dataset = Dataset({'input_ids': test_input_ids, 'attention_mask': test_mask, 'labels': test_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And move onto building our model, we first need to create a RoBERTa config object, which will describe which features we want to initialize our RoBERTa model with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=30_522,  # we align this to the tokenizer vocab set in previous notebook\n",
    "    max_position_embeddings=514,\n",
    "    hidden_size=768,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we import and initialize a RoBERTa model with a language modeling head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we move onto training. First we setup GPU/CPU usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda', index=2) if torch.cuda.is_available() else torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate the training mode of our model, and initialize our optimizer (Adam with weighted decay - reduces chance of overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 09:38:25.288448: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AdamW\n",
    "\n",
    "# activate training mode\n",
    "model.train()\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33morlov-aleksei53\u001b[0m (\u001b[33mmoleculary-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alexeyorlov53/Transformers-for-Molecules/wandb/run-20240301_093828-b3kag3qw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/moleculary-ai/efcp_transformer/runs/b3kag3qw' target=\"_blank\">RobertaForMLM on ecpf0 training (2M)</a></strong> to <a href='https://wandb.ai/moleculary-ai/efcp_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/moleculary-ai/efcp_transformer' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/moleculary-ai/efcp_transformer/runs/b3kag3qw' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer/runs/b3kag3qw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/moleculary-ai/efcp_transformer/runs/b3kag3qw?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fc83a580290>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"efcp_transformer\",\n",
    "    name=\"RobertaForMLM on ecpf0 training (2M)\",\n",
    "    config={}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move onto the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|▉                                                                         | 834/62272 [29:39<36:25:20,  2.13s/it, loss=0.418]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # for our progress bar\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "epochs = 1\n",
    "step = 0\n",
    "\n",
    "validation_iterator = iter(validation_loader)\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        true_labels = batch['labels'].numpy().flatten()\n",
    "        pred_labels = torch.nn.functional.softmax(logits, dim=1).argmax(axis=-1).cpu().detach().numpy().flatten()\n",
    "\n",
    "        # write down loss and metrics\n",
    "        wandb.log({\"loss/train\": loss}, step=step)\n",
    "        wandb.log({\"accuracy/train\": accuracy_score(true_labels, pred_labels)}, step=step)\n",
    "        wandb.log({\"f1/train\": f1_score(true_labels, pred_labels, average='micro')}, step=step)\n",
    "        wandb.log({\"precision/train\": precision_score(true_labels, pred_labels, average='micro')}, step=step)\n",
    "        wandb.log({\"recall/train\": recall_score(true_labels, pred_labels, average='micro')}, step=step)\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                validation_batch = next(validation_iterator)\n",
    "            except StopIteration:\n",
    "                validation_dataset = Dataset({'input_ids': validation_input_ids, 'attention_mask': validation_mask, 'labels': validation_labels})\n",
    "                validation_batch = iter(validation_dataset)\n",
    "                validation_batch = next(validation_iterator)\n",
    "            \n",
    "            input_ids = validation_batch['input_ids'].to(device)\n",
    "            attention_mask = validation_batch['attention_mask'].to(device)\n",
    "            labels = validation_batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            true_labels = batch['labels'].numpy().flatten()\n",
    "            pred_labels = torch.nn.functional.softmax(logits, dim=1).argmax(axis=-1).cpu().detach().numpy().flatten()\n",
    "    \n",
    "            # write down loss and metrics\n",
    "            wandb.log({\"loss/validation\": loss}, step=step)\n",
    "            wandb.log({\"accuracy/validation\": accuracy_score(true_labels, pred_labels)}, step=step)\n",
    "            wandb.log({\"f1/validation\": f1_score(true_labels, pred_labels, average='micro')}, step=step)\n",
    "            wandb.log({\"precision/validation\": precision_score(true_labels, pred_labels, average='micro')}, step=step)\n",
    "            wandb.log({\"recall/validation\": recall_score(true_labels, pred_labels, average='micro')}, step=step)\n",
    "            \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        if step > 2500:\n",
    "            break\n",
    "        step += len(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy/train</td><td>▁▁▁▁▁▁▁▁▁▂▁▁▂▁▂▂▂▄▅▆▆▇▇▇████████████████</td></tr><tr><td>accuracy/validation</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▃▅▅▆▇▇█████████████████</td></tr><tr><td>f1/train</td><td>▁▁▁▁▁▁▁▁▁▂▁▁▂▁▂▂▂▄▅▆▆▇▇▇████████████████</td></tr><tr><td>f1/validation</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▃▅▅▆▇▇█████████████████</td></tr><tr><td>loss/train</td><td>███▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/validation</td><td>███▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>precision/train</td><td>▁▁▁▁▁▁▁▁▁▂▁▁▂▁▂▂▂▄▅▆▆▇▇▇████████████████</td></tr><tr><td>precision/validation</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▃▅▅▆▇▇█████████████████</td></tr><tr><td>recall/train</td><td>▁▁▁▁▁▁▁▁▁▂▁▁▂▁▂▂▂▄▅▆▆▇▇▇████████████████</td></tr><tr><td>recall/validation</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▃▅▅▆▇▇█████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy/train</td><td>0.98883</td></tr><tr><td>accuracy/validation</td><td>0.92694</td></tr><tr><td>f1/train</td><td>0.98883</td></tr><tr><td>f1/validation</td><td>0.92694</td></tr><tr><td>loss/train</td><td>0.41928</td></tr><tr><td>loss/validation</td><td>0.41759</td></tr><tr><td>precision/train</td><td>0.98883</td></tr><tr><td>precision/validation</td><td>0.92694</td></tr><tr><td>recall/train</td><td>0.98883</td></tr><tr><td>recall/validation</td><td>0.92694</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">RobertaForMLM on ecpf0 training (2M)</strong> at: <a href='https://wandb.ai/moleculary-ai/efcp_transformer/runs/b3kag3qw' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer/runs/b3kag3qw</a><br/> View job at <a href='https://wandb.ai/moleculary-ai/efcp_transformer/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NDA4OTg4Mg==/version_details/v3' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NDA4OTg4Mg==/version_details/v3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240301_093828-b3kag3qw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3932ebdb7984792be4f28674365e815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113502711264624, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alexeyorlov53/Transformers-for-Molecules/wandb/run-20240301_100829-p6sxk73y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/moleculary-ai/efcp_transformer/runs/p6sxk73y' target=\"_blank\">RobertaForMLM on ecpf0 testing (2M)</a></strong> to <a href='https://wandb.ai/moleculary-ai/efcp_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/moleculary-ai/efcp_transformer' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/moleculary-ai/efcp_transformer/runs/p6sxk73y' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer/runs/p6sxk73y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/moleculary-ai/efcp_transformer/runs/p6sxk73y?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fc83a5e5d90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"efcp_transformer\",\n",
    "    name=\"RobertaForMLM on ecpf0 testing (2M)\",\n",
    "    config={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   8%|██████▎                                                                       | 459/5625 [04:53<55:02,  1.56it/s, loss=0.416]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     14\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m---> 15\u001b[0m pred_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# write down loss and metrics\u001b[39;00m\n\u001b[1;32m     18\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss/test\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss}, step\u001b[38;5;241m=\u001b[39mstep)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    loop = tqdm(test_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        true_labels = batch['labels'].numpy().flatten()\n",
    "        pred_labels = torch.nn.functional.softmax(logits, dim=1).argmax(axis=-1).cpu().detach().numpy().flatten()\n",
    "\n",
    "        # write down loss and metrics\n",
    "        wandb.log({\"loss/test\": loss}, step=step)\n",
    "        wandb.log({\"accuracy/test\": accuracy_score(true_labels, pred_labels)}, step=step)\n",
    "        wandb.log({\"f1/test\": f1_score(true_labels, pred_labels, average='micro')}, step=step)\n",
    "        wandb.log({\"precision/test\": precision_score(true_labels, pred_labels, average='micro')}, step=step)\n",
    "        wandb.log({\"recall/test\": recall_score(true_labels, pred_labels, average='micro')}, step=step)\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        step += len(batch)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 14.1%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy/test</td><td>▆▅█▆▁▇▇▇▅▅▄▆▇▇▆▅▇▇▆▆█▆▇▃▆▄▅▆▇▇▇▇▆▇█▆▅▆▆▅</td></tr><tr><td>f1/test</td><td>▆▅█▆▁▇▇▇▅▅▄▆▇▇▆▅▇▇▆▆█▆▇▃▆▄▅▆▇▇▇▇▆▇█▆▅▆▆▅</td></tr><tr><td>loss/test</td><td>██▄▅█▄▅▁█▅▃▇▄▄▇▇▆▄▅▅▂▇▇▄▇▆▇▆▄▃▆▃▆▄▂▄█▅▆▄</td></tr><tr><td>precision/test</td><td>▆▅█▆▁▇▇▇▅▅▄▆▇▇▆▅▇▇▆▆█▆▇▃▆▄▅▆▇▇▇▇▆▇█▆▅▆▆▅</td></tr><tr><td>recall/test</td><td>▆▅█▆▁▇▇▇▅▅▄▆▇▇▆▅▇▇▆▆█▆▇▃▆▄▅▆▇▇▇▇▆▇█▆▅▆▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy/test</td><td>0.99158</td></tr><tr><td>f1/test</td><td>0.99158</td></tr><tr><td>loss/test</td><td>0.41582</td></tr><tr><td>precision/test</td><td>0.99158</td></tr><tr><td>recall/test</td><td>0.99158</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">RobertaForMLM on ecpf0 testing (2M)</strong> at: <a href='https://wandb.ai/moleculary-ai/efcp_transformer/runs/p6sxk73y' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer/runs/p6sxk73y</a><br/> View job at <a href='https://wandb.ai/moleculary-ai/efcp_transformer/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NDA4OTg4Mg==/version_details/v4' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NDA4OTg4Mg==/version_details/v4</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240301_100829-p6sxk73y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f'molberto_{radius}_2.5K_dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device('cuda', index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
