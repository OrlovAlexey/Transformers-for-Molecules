{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d6f265e-e433-48e9-a783-950e8b12260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f53265f1-214e-4d58-814f-6f7c7b05f907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/alexeyorlov53/.netrc\n"
     ]
    }
   ],
   "source": [
    "!python3 -m wandb login eb7b1964fb84cd81de96b2a273ecf2bb6254aeac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb87046b-24e4-4963-ace0-615a30c7ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'ecfp0'\n",
    "samples_count1 = '10M'\n",
    "model_name1 = f'molberto_{filename}_{samples_count1}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7535600-f27b-409f-8c87-a0b48c56f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fb29dab-286e-42c3-bb72-3ddd749252a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_number=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d58c5-eb7a-4420-b0db-f743e2167213",
   "metadata": {},
   "source": [
    "### Upload and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d5561e6-2477-4ead-9fae-91cae781bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"BBBP-2k-ecfp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0acb3637-c146-45dc-a1d3-d9ef1363d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe = dataframe.drop(columns=['Unnamed: 0', 'Smiles', 'ecfp2', 'ecfp3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cae64520-4a9c-453f-949a-737bbd58bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_dataset(df, column):\n",
    "    for row in tqdm(range(len(df))):\n",
    "        str_ints = eval(df.iloc[row][column])\n",
    "        str_fingerprint = ' '.join(str_ints[0])\n",
    "        df.at[row, column] = str_fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1eb1370-13e9-4505-88da-b216213a873f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f49d589dd7f416d9cde459f965fc421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1945 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocess_data_dataset(dataframe, 'ecfp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e9f902e-7691-4256-883a-bd70f484a5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "      <th>ecfp</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[N+](=NCC(=O)N[C@@H]([C@H](O)C1=CC=C([N+]([O-]...</td>\n",
       "      <td>849271271 847336149 2245384272 2246699815 8649...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C1=C(OC)C(=CC2=C1C(=[N+](C(=C2CC)C)[NH-])C3=CC...</td>\n",
       "      <td>3218693969 3217380708 864674487 2246728737 321...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[O+]1=N[N](C=C1[N-]C(NC2=CC=CC=C2)=O)C(CC3=CC=...</td>\n",
       "      <td>3189554341 2041434490 2092489639 3218693969 32...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[C@H]3([N]2C1=C(C(=NC=N1)N)N=C2)[C@@H]([C@@H](...</td>\n",
       "      <td>2976033787 2092489639 3217380708 3217380708 32...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1=C(Cl)C(=C(C2=C1NC(=O)C(N2)=O)[N+](=O)[O-])Cl</td>\n",
       "      <td>3218693969 3217380708 1016841875 3217380708 32...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>CCN1CCN(C(=O)N[C@@H](C(=O)N[C@H]2[C@H]3SCC(=C(...</td>\n",
       "      <td>2246728737 2245384272 2092489639 2968968094 29...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>Cc1onc(c2ccccc2Cl)c1C(=O)N[C@H]3[C@H]4SC(C)(C)...</td>\n",
       "      <td>2246728737 3217380708 3189457552 2041434490 32...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C</td>\n",
       "      <td>2968968094 2968968094 2968968094 2092489639 29...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO...</td>\n",
       "      <td>3217380708 3217380708 3217380708 2092489639 29...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl</td>\n",
       "      <td>2246699815 864942730 864674487 2245277810 2246...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1945 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Smiles  \\\n",
       "0     [N+](=NCC(=O)N[C@@H]([C@H](O)C1=CC=C([N+]([O-]...   \n",
       "1     C1=C(OC)C(=CC2=C1C(=[N+](C(=C2CC)C)[NH-])C3=CC...   \n",
       "2     [O+]1=N[N](C=C1[N-]C(NC2=CC=CC=C2)=O)C(CC3=CC=...   \n",
       "3     [C@H]3([N]2C1=C(C(=NC=N1)N)N=C2)[C@@H]([C@@H](...   \n",
       "4       C1=C(Cl)C(=C(C2=C1NC(=O)C(N2)=O)[N+](=O)[O-])Cl   \n",
       "...                                                 ...   \n",
       "1940  CCN1CCN(C(=O)N[C@@H](C(=O)N[C@H]2[C@H]3SCC(=C(...   \n",
       "1941  Cc1onc(c2ccccc2Cl)c1C(=O)N[C@H]3[C@H]4SC(C)(C)...   \n",
       "1942                   C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C   \n",
       "1943  c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO...   \n",
       "1944           C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl   \n",
       "\n",
       "                                                   ecfp  target  \n",
       "0     849271271 847336149 2245384272 2246699815 8649...       1  \n",
       "1     3218693969 3217380708 864674487 2246728737 321...       1  \n",
       "2     3189554341 2041434490 2092489639 3218693969 32...       1  \n",
       "3     2976033787 2092489639 3217380708 3217380708 32...       1  \n",
       "4     3218693969 3217380708 1016841875 3217380708 32...       1  \n",
       "...                                                 ...     ...  \n",
       "1940  2246728737 2245384272 2092489639 2968968094 29...       1  \n",
       "1941  2246728737 3217380708 3189457552 2041434490 32...       1  \n",
       "1942  2968968094 2968968094 2968968094 2092489639 29...       1  \n",
       "1943  3217380708 3217380708 3217380708 2092489639 29...       1  \n",
       "1944  2246699815 864942730 864674487 2245277810 2246...       1  \n",
       "\n",
       "[1945 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "628bf71f-7af4-4eab-9fa2-e1bec5c7d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Percentage on NaNs:')\n",
    "# dataframe.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9131af22-72dd-4fee-b5b4-9a5b159030cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows_with_nans = dataframe['Molecular Weight'].isna() | \\\n",
    "#                  dataframe['Bioactivities'].isna() | \\\n",
    "#                  dataframe['AlogP'].isna() | \\\n",
    "#                  dataframe['Polar Surface Area'].isna() | \\\n",
    "#                  dataframe['CX Acidic pKa'].isna() | \\\n",
    "#                  dataframe['CX Basic pKa'].isna()\n",
    "# print(f'Count of rows without NaNs: {dataframe.shape[0] - dataframe.loc[rows_with_nans].shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeec7baa-b6bb-47a1-bf4a-66e94049750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 2 last properties to reduce NaN counts\n",
    "# molecular_properties = ['Molecular Weight', 'Bioactivities', 'AlogP', 'Polar Surface Area']\n",
    "# dataframe = dataframe.drop(columns=['CX Acidic pKa', 'CX Basic pKa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1964f7f1-2efc-4333-81bc-3ebbdcbec934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN's\n",
    "# dataframe = dataframe.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facbc3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name1)\n",
    "\n",
    "tokenizer.model_max_len=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b3b5a83-5050-48d4-8813-490c01fbc1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'849271271 847336149 2245384272 2246699815 864942730 847961216 2245273601 2245273601 864662311 3217380708 3218693969 3218693969 3217380708 848127915 864942795 864942730 3218693969 3218693969 2245384272 864662311 847433129'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['ecfp'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de2bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(dataframe['ecfp'][0], truncation=True, max_length=512, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5376cc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8afde96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"ecfp\"], truncation=True, max_length=512, padding='max_length')\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e96ad73-2190-48f4-a1c3-ba4b5325f62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Smiles', 'ecfp', 'target'],\n",
       "        num_rows: 1556\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Smiles', 'ecfp', 'target'],\n",
       "        num_rows: 195\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Smiles', 'ecfp', 'target'],\n",
       "        num_rows: 194\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataset = Dataset.from_pandas(dataframe)\n",
    "train_testvalid = dataset.train_test_split(test_size=0.2, seed=15)\n",
    "\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=15)\n",
    "\n",
    "# 10% for test, 10 for validation, 80% for train\n",
    "dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'validation': test_valid['train']})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63f3e06-7933-457f-a51d-0373c34f413a",
   "metadata": {},
   "source": [
    "### Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7720fb53-0a36-452f-8d5a-f23f0c5755d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name1)\n",
    "\n",
    "tokenizer.model_max_len=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f924833-477c-4a24-936e-d62f0ca5dd8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6cfd8761b04db4a2b4c79f0de9d9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d796801b28248df8f1d0e0572dc4988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/195 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bafd9858db473cb8884dd88330251a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/194 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Smiles', 'ecfp', 'target', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1556\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Smiles', 'ecfp', 'target', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 195\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Smiles', 'ecfp', 'target', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 194\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "  return tokenizer(batch[\"ecfp\"], truncation=True, max_length=512, padding='max_length')\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "434caf18-e9b1-42ac-b40e-05c1b75c538a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask', 'target']\n"
     ]
    }
   ],
   "source": [
    "columns = [\"input_ids\", \"attention_mask\"]\n",
    "columns.extend(['target']) # our labels\n",
    "print(columns)\n",
    "tokenized_dataset.set_format('torch', columns=columns)\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a56144-3102-4a21-b86b-8e851dc97026",
   "metadata": {},
   "source": [
    "### Create Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66254640-38dc-4da6-bccf-634174aca108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "class MolecularPropertiesClassification(torch.nn.Module):\n",
    "    def __init__(self, model_name1):\n",
    "        super(MolecularPropertiesClassification, self).__init__()\n",
    "\n",
    "        config1 = AutoConfig.from_pretrained(model_name1)\n",
    "        self.transformer1 = AutoModel.from_pretrained(model_name1, config=config1)\n",
    "        # removing last layer of transformer\n",
    "        self.transformer1.pooler = torch.nn.Identity()\n",
    "        # freezing transformer weights\n",
    "        for param in self.transformer1.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.linear1 = torch.nn.Linear(768, 768, bias=True)\n",
    "        self.linear2 = torch.nn.Linear(768, 2, bias=True)\n",
    "\n",
    "    def forward(self, input_ids = None, attention_mask=None):\n",
    "        outputs1 = self.transformer1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state1 = outputs1[0]\n",
    "        \n",
    "        first_linear_out = self.linear1(last_hidden_state1[:, 0, : ].view(-1, 768))\n",
    "        logits = self.linear2(torch.nn.functional.sigmoid(first_linear_out))\n",
    "\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b0c73-a897-407e-a8c7-158283f97de4",
   "metadata": {},
   "source": [
    "### Create PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79747c27-653a-486a-ab4d-3f2c6ec347f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset['train'], shuffle = True, batch_size = batch_size, collate_fn = data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_dataset['validation'], shuffle = True, batch_size = batch_size, collate_fn = data_collator\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_dataset['test'], shuffle = True, batch_size = batch_size, collate_fn = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08055762-1a53-49e6-8cd7-71161852abe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at molberto_ecfp0_10M and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\", index=gpu_number) if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = MolecularPropertiesClassification(model_name1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1428e13-d841-45e7-a885-919d4670a102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MolecularPropertiesClassification(\n",
       "  (transformer1): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Identity()\n",
       "  )\n",
       "  (linear1): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (linear2): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04001413-80f4-4663-ab5d-a107e839c5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexeyorlov53/anaconda3/envs/myenv/lib/python3.9/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "num_training_steps = num_epoch * len(train_dataloader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    'linear',\n",
    "    optimizer = optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = num_training_steps,\n",
    ")\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54ef89d4-b7ac-4a86-ab14-67383fd0f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33morlov-aleksei53\u001b[0m (\u001b[33mmoleculary-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e007aa193574727b921f9dde964e60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114075955831342, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alexeyorlov53/Transformers-for-Molecules/classifications/BBBP/wandb/run-20240925_164643-b41nki17</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/moleculary-ai/efcp_transformer/runs/b41nki17' target=\"_blank\">ECFP-BERT-10M-BBBP</a></strong> to <a href='https://wandb.ai/moleculary-ai/efcp_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/moleculary-ai/efcp_transformer' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/moleculary-ai/efcp_transformer/runs/b41nki17' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer/runs/b41nki17</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/moleculary-ai/efcp_transformer/runs/b41nki17?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f5929851040>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"efcp_transformer\",\n",
    "    name='ECFP-BERT-' + samples_count1 + \"-BBBP\",\n",
    "    config={}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a951a1-7c75-47bd-b275-9693eb22fe95",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f0f67d5-c7f4-4ea0-959f-b1a990387a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19101369a0464fe09a34ed52b63d40dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025188f5c8f5427f896f79bbffabc6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n",
    "\n",
    "progress_bar_train = tqdm(range(num_training_steps))\n",
    "progress_bar_eval = tqdm(range(num_epoch * len(eval_dataloader)))\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    model.train()\n",
    "    total_pred_labels = []\n",
    "    total_true_labels = []\n",
    "    epoch_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_batch = { k: v.to(device) for k, v in batch.items() if k in ['input_ids', 'attention_mask'] }\n",
    "        batch['target'] = batch['target'].to(device)\n",
    "        \n",
    "        logits = model(**input_batch)\n",
    "        \n",
    "        loss = loss_func(logits.view(-1, 2), batch['target'].view(-1))\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        pred_labels = torch.argmax(logits, dim=-1)\n",
    "        true_labels = batch['target']\n",
    "        total_pred_labels.append(pred_labels)\n",
    "        total_true_labels.append(true_labels)\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar_train.update(1)\n",
    "\n",
    "    total_pred_labels = torch.cat(total_pred_labels).cpu().detach().numpy()\n",
    "    total_true_labels = torch.cat(total_true_labels).cpu().detach().numpy()\n",
    "    \n",
    "    wandb.log({\"loss/train\": epoch_loss / len(train_dataloader)})\n",
    "    wandb.log({\"accuracy/train\": accuracy_score(total_true_labels, total_pred_labels)})\n",
    "    wandb.log({\"f1/train\": f1_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"precision/train\": precision_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"recall/train\": recall_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"roc_auc_score/train\": roc_auc_score(total_true_labels, total_pred_labels)})\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    total_pred_labels = []\n",
    "    total_true_labels = []\n",
    "    epoch_loss = 0\n",
    "    for batch in eval_dataloader:\n",
    "        input_batch = { k: v.to(device) for k, v in batch.items() if k in ['input_ids', 'attention_mask'] }\n",
    "        batch['target'] = batch['target'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(**input_batch)\n",
    "            loss = loss_func(logits.view(-1, 2), batch['target'].view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            pred_labels = torch.argmax(logits, dim=-1)\n",
    "            true_labels = batch['target']\n",
    "            total_pred_labels.append(pred_labels)\n",
    "            total_true_labels.append(true_labels)\n",
    "        \n",
    "        progress_bar_eval.update(1)\n",
    "\n",
    "    total_pred_labels = torch.cat(total_pred_labels).cpu().detach().numpy()\n",
    "    total_true_labels = torch.cat(total_true_labels).cpu().detach().numpy()\n",
    "    \n",
    "    wandb.log({\"loss/validation\": epoch_loss / len(eval_dataloader)})\n",
    "    wandb.log({\"accuracy/validation\": accuracy_score(total_true_labels, total_pred_labels)})\n",
    "    wandb.log({\"f1/validation\": f1_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"precision/validation\": precision_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"recall/validation\": recall_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"roc_auc_score/validation\": roc_auc_score(total_true_labels, total_pred_labels)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6871715-a1b0-4562-9e5d-8678ce7dfee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop():\n",
    "    model.eval()\n",
    "    total_pred_labels = []\n",
    "    total_true_labels = []\n",
    "    epoch_loss = 0\n",
    "    for batch in test_dataloader:\n",
    "        input_batch = { k: v.to(device) for k, v in batch.items() if k in ['input_ids', 'attention_mask'] }\n",
    "        batch['target'] = batch['target'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(**input_batch)\n",
    "            loss = loss_func(logits.view(-1, 2), batch['target'].view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            pred_labels = torch.argmax(logits, dim=-1)\n",
    "            true_labels = batch['target']\n",
    "            total_pred_labels.append(pred_labels)\n",
    "            total_true_labels.append(true_labels)\n",
    "        \n",
    "        progress_bar_eval.update(1)\n",
    "\n",
    "    total_pred_labels = torch.cat(total_pred_labels).cpu().detach().numpy()\n",
    "    total_true_labels = torch.cat(total_true_labels).cpu().detach().numpy()\n",
    "    \n",
    "    wandb.log({\"loss/test\": epoch_loss / len(test_dataloader)})\n",
    "    wandb.log({\"accuracy/test\": accuracy_score(total_true_labels, total_pred_labels)})\n",
    "    wandb.log({\"f1/test\": f1_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"precision/test\": precision_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"recall/test\": recall_score(total_true_labels, total_pred_labels, average='micro')})\n",
    "    wandb.log({\"roc_auc_score/test\": roc_auc_score(total_true_labels, total_pred_labels)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec03cbba-325e-4649-af90-0730d4acb280",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c59bd8ec-528b-4b88-946f-85308c74081d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 12.1%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy/test</td><td>▁</td></tr><tr><td>accuracy/train</td><td>▁▁▂▃▅▆▆▆▆▇▇▇▇██▇███▇█▇▇█▇▇█▇▇█████▇█████</td></tr><tr><td>accuracy/validation</td><td>▁▁▂▄▇▇██████▇█▇█▇█▇█████▇████████▇█▇████</td></tr><tr><td>f1/test</td><td>▁</td></tr><tr><td>f1/train</td><td>▁▁▂▃▅▆▆▆▆▇▇▇▇██▇███▇█▇▇█▇▇█▇▇█████▇█████</td></tr><tr><td>f1/validation</td><td>▁▁▂▄▇▇██████▇█▇█▇█▇█████▇████████▇█▇████</td></tr><tr><td>loss/test</td><td>▁</td></tr><tr><td>loss/train</td><td>█▇▅▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/validation</td><td>█▆▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision/test</td><td>▁</td></tr><tr><td>precision/train</td><td>▁▁▂▃▅▆▆▆▆▇▇▇▇██▇███▇█▇▇█▇▇█▇▇█████▇█████</td></tr><tr><td>precision/validation</td><td>▁▁▂▄▇▇██████▇█▇█▇█▇█████▇████████▇█▇████</td></tr><tr><td>recall/test</td><td>▁</td></tr><tr><td>recall/train</td><td>▁▁▂▃▅▆▆▆▆▇▇▇▇██▇███▇█▇▇█▇▇█▇▇█████▇█████</td></tr><tr><td>recall/validation</td><td>▁▁▂▄▇▇██████▇█▇█▇█▇█████▇████████▇█▇████</td></tr><tr><td>roc_auc_score/test</td><td>▁</td></tr><tr><td>roc_auc_score/train</td><td>▁▁▂▃▄▅▆▆▆▇▇▇▇██▇█████▇▇█████████████████</td></tr><tr><td>roc_auc_score/validation</td><td>▁▁▁▃▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██▇██████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy/test</td><td>0.90256</td></tr><tr><td>accuracy/train</td><td>0.88753</td></tr><tr><td>accuracy/validation</td><td>0.85567</td></tr><tr><td>f1/test</td><td>0.90256</td></tr><tr><td>f1/train</td><td>0.88753</td></tr><tr><td>f1/validation</td><td>0.85567</td></tr><tr><td>loss/test</td><td>0.21754</td></tr><tr><td>loss/train</td><td>0.28768</td></tr><tr><td>loss/validation</td><td>0.32483</td></tr><tr><td>precision/test</td><td>0.90256</td></tr><tr><td>precision/train</td><td>0.88753</td></tr><tr><td>precision/validation</td><td>0.85567</td></tr><tr><td>recall/test</td><td>0.90256</td></tr><tr><td>recall/train</td><td>0.88753</td></tr><tr><td>recall/validation</td><td>0.85567</td></tr><tr><td>roc_auc_score/test</td><td>0.8287</td></tr><tr><td>roc_auc_score/train</td><td>0.81114</td></tr><tr><td>roc_auc_score/validation</td><td>0.80622</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ECFP-BERT-10M-BBBP</strong> at: <a href='https://wandb.ai/moleculary-ai/efcp_transformer/runs/b41nki17' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer/runs/b41nki17</a><br/> View project at: <a href='https://wandb.ai/moleculary-ai/efcp_transformer' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240925_164643-b41nki17/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57a96b63-ec67-4374-968e-bd71ff08c91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ad7b9-f82b-4936-bd4f-26266cbb6b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed2e0dc-f8bc-4866-bfa3-af4032dfb9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
