{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d6f265e-e433-48e9-a783-950e8b12260c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:24.102550Z",
     "iopub.status.busy": "2024-02-29T21:50:24.102384Z",
     "iopub.status.idle": "2024-02-29T21:50:26.051808Z",
     "shell.execute_reply": "2024-02-29T21:50:26.033539Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f53265f1-214e-4d58-814f-6f7c7b05f907",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:26.056251Z",
     "iopub.status.busy": "2024-02-29T21:50:26.055854Z",
     "iopub.status.idle": "2024-02-29T21:50:27.787399Z",
     "shell.execute_reply": "2024-02-29T21:50:27.785646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/alexeyorlov53/.netrc\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m wandb login eb7b1964fb84cd81de96b2a273ecf2bb6254aeac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb87046b-24e4-4963-ace0-615a30c7ddeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:27.791313Z",
     "iopub.status.busy": "2024-02-29T21:50:27.791079Z",
     "iopub.status.idle": "2024-02-29T21:50:27.794285Z",
     "shell.execute_reply": "2024-02-29T21:50:27.793850Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'ecfp0'\n",
    "samples_count1 = '10M'\n",
    "samples_count2 = '2M'\n",
    "model_name1 = f'molberto_{filename}_{samples_count1}'\n",
    "model_name2 = f'molberto_{filename}_{samples_count2}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "effd7294-42cf-4de5-91d4-d6ab3fec2127",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:27.796163Z",
     "iopub.status.busy": "2024-02-29T21:50:27.796007Z",
     "iopub.status.idle": "2024-02-29T21:50:27.798253Z",
     "shell.execute_reply": "2024-02-29T21:50:27.797819Z"
    }
   },
   "outputs": [],
   "source": [
    "# molecular_properties = ['Molecular Weight', 'Bioactivities', 'AlogP', 'Polar Surface Area', 'CX Acidic pKa', 'CX Basic pKa']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d58c5-eb7a-4420-b0db-f743e2167213",
   "metadata": {},
   "source": [
    "### Upload and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d5561e6-2477-4ead-9fae-91cae781bfe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:27.801164Z",
     "iopub.status.busy": "2024-02-29T21:50:27.800859Z",
     "iopub.status.idle": "2024-02-29T21:50:27.859728Z",
     "shell.execute_reply": "2024-02-29T21:50:27.834873Z"
    }
   },
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"BBBP-2k-ecfp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0acb3637-c146-45dc-a1d3-d9ef1363d2ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:27.862824Z",
     "iopub.status.busy": "2024-02-29T21:50:27.862482Z",
     "iopub.status.idle": "2024-02-29T21:50:27.866904Z",
     "shell.execute_reply": "2024-02-29T21:50:27.866270Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataframe = dataframe.drop(columns=['Unnamed: 0', 'Smiles', 'ecfp2', 'ecfp3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cae64520-4a9c-453f-949a-737bbd58bc06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:27.869335Z",
     "iopub.status.busy": "2024-02-29T21:50:27.868884Z",
     "iopub.status.idle": "2024-02-29T21:50:27.872508Z",
     "shell.execute_reply": "2024-02-29T21:50:27.871953Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data_dataset(df, column):\n",
    "    for row in tqdm(range(len(df))):\n",
    "        str_ints = eval(df.iloc[row][column])\n",
    "        str_fingerprint = ' '.join(str_ints[0])\n",
    "        df.at[row, column] = str_fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1eb1370-13e9-4505-88da-b216213a873f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:27.874596Z",
     "iopub.status.busy": "2024-02-29T21:50:27.874336Z",
     "iopub.status.idle": "2024-02-29T21:50:28.214389Z",
     "shell.execute_reply": "2024-02-29T21:50:28.213870Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f3005b737d41648c4ea20241ca74a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1945 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocess_data_dataset(dataframe, 'ecfp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e9f902e-7691-4256-883a-bd70f484a5c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:28.224996Z",
     "iopub.status.busy": "2024-02-29T21:50:28.217211Z",
     "iopub.status.idle": "2024-02-29T21:50:28.274678Z",
     "shell.execute_reply": "2024-02-29T21:50:28.273862Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "      <th>ecfp</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[N+](=NCC(=O)N[C@@H]([C@H](O)C1=CC=C([N+]([O-]...</td>\n",
       "      <td>849271271 847336149 2245384272 2246699815 8649...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C1=C(OC)C(=CC2=C1C(=[N+](C(=C2CC)C)[NH-])C3=CC...</td>\n",
       "      <td>3218693969 3217380708 864674487 2246728737 321...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[O+]1=N[N](C=C1[N-]C(NC2=CC=CC=C2)=O)C(CC3=CC=...</td>\n",
       "      <td>3189554341 2041434490 2092489639 3218693969 32...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[C@H]3([N]2C1=C(C(=NC=N1)N)N=C2)[C@@H]([C@@H](...</td>\n",
       "      <td>2976033787 2092489639 3217380708 3217380708 32...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1=C(Cl)C(=C(C2=C1NC(=O)C(N2)=O)[N+](=O)[O-])Cl</td>\n",
       "      <td>3218693969 3217380708 1016841875 3217380708 32...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>CCN1CCN(C(=O)N[C@@H](C(=O)N[C@H]2[C@H]3SCC(=C(...</td>\n",
       "      <td>2246728737 2245384272 2092489639 2968968094 29...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>Cc1onc(c2ccccc2Cl)c1C(=O)N[C@H]3[C@H]4SC(C)(C)...</td>\n",
       "      <td>2246728737 3217380708 3189457552 2041434490 32...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C</td>\n",
       "      <td>2968968094 2968968094 2968968094 2092489639 29...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO...</td>\n",
       "      <td>3217380708 3217380708 3217380708 2092489639 29...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl</td>\n",
       "      <td>2246699815 864942730 864674487 2245277810 2246...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1945 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Smiles  \\\n",
       "0     [N+](=NCC(=O)N[C@@H]([C@H](O)C1=CC=C([N+]([O-]...   \n",
       "1     C1=C(OC)C(=CC2=C1C(=[N+](C(=C2CC)C)[NH-])C3=CC...   \n",
       "2     [O+]1=N[N](C=C1[N-]C(NC2=CC=CC=C2)=O)C(CC3=CC=...   \n",
       "3     [C@H]3([N]2C1=C(C(=NC=N1)N)N=C2)[C@@H]([C@@H](...   \n",
       "4       C1=C(Cl)C(=C(C2=C1NC(=O)C(N2)=O)[N+](=O)[O-])Cl   \n",
       "...                                                 ...   \n",
       "1940  CCN1CCN(C(=O)N[C@@H](C(=O)N[C@H]2[C@H]3SCC(=C(...   \n",
       "1941  Cc1onc(c2ccccc2Cl)c1C(=O)N[C@H]3[C@H]4SC(C)(C)...   \n",
       "1942                   C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C   \n",
       "1943  c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO...   \n",
       "1944           C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl   \n",
       "\n",
       "                                                   ecfp  target  \n",
       "0     849271271 847336149 2245384272 2246699815 8649...       1  \n",
       "1     3218693969 3217380708 864674487 2246728737 321...       1  \n",
       "2     3189554341 2041434490 2092489639 3218693969 32...       1  \n",
       "3     2976033787 2092489639 3217380708 3217380708 32...       1  \n",
       "4     3218693969 3217380708 1016841875 3217380708 32...       1  \n",
       "...                                                 ...     ...  \n",
       "1940  2246728737 2245384272 2092489639 2968968094 29...       1  \n",
       "1941  2246728737 3217380708 3189457552 2041434490 32...       1  \n",
       "1942  2968968094 2968968094 2968968094 2092489639 29...       1  \n",
       "1943  3217380708 3217380708 3217380708 2092489639 29...       1  \n",
       "1944  2246699815 864942730 864674487 2245277810 2246...       1  \n",
       "\n",
       "[1945 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "628bf71f-7af4-4eab-9fa2-e1bec5c7d682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:28.277601Z",
     "iopub.status.busy": "2024-02-29T21:50:28.277264Z",
     "iopub.status.idle": "2024-02-29T21:50:28.281223Z",
     "shell.execute_reply": "2024-02-29T21:50:28.280544Z"
    }
   },
   "outputs": [],
   "source": [
    "# print('Percentage on NaNs:')\n",
    "# dataframe.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9131af22-72dd-4fee-b5b4-9a5b159030cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:28.283890Z",
     "iopub.status.busy": "2024-02-29T21:50:28.283310Z",
     "iopub.status.idle": "2024-02-29T21:50:28.286121Z",
     "shell.execute_reply": "2024-02-29T21:50:28.285612Z"
    }
   },
   "outputs": [],
   "source": [
    "# rows_with_nans = dataframe['Molecular Weight'].isna() | \\\n",
    "#                  dataframe['Bioactivities'].isna() | \\\n",
    "#                  dataframe['AlogP'].isna() | \\\n",
    "#                  dataframe['Polar Surface Area'].isna() | \\\n",
    "#                  dataframe['CX Acidic pKa'].isna() | \\\n",
    "#                  dataframe['CX Basic pKa'].isna()\n",
    "# print(f'Count of rows without NaNs: {dataframe.shape[0] - dataframe.loc[rows_with_nans].shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeec7baa-b6bb-47a1-bf4a-66e94049750f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:28.288692Z",
     "iopub.status.busy": "2024-02-29T21:50:28.288382Z",
     "iopub.status.idle": "2024-02-29T21:50:28.291567Z",
     "shell.execute_reply": "2024-02-29T21:50:28.291079Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove 2 last properties to reduce NaN counts\n",
    "# molecular_properties = ['Molecular Weight', 'Bioactivities', 'AlogP', 'Polar Surface Area']\n",
    "# dataframe = dataframe.drop(columns=['CX Acidic pKa', 'CX Basic pKa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1964f7f1-2efc-4333-81bc-3ebbdcbec934",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:28.293724Z",
     "iopub.status.busy": "2024-02-29T21:50:28.293557Z",
     "iopub.status.idle": "2024-02-29T21:50:28.295799Z",
     "shell.execute_reply": "2024-02-29T21:50:28.295366Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop NaN's\n",
    "# dataframe = dataframe.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b3b5a83-5050-48d4-8813-490c01fbc1e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:28.297721Z",
     "iopub.status.busy": "2024-02-29T21:50:28.297422Z",
     "iopub.status.idle": "2024-02-29T21:50:28.299561Z",
     "shell.execute_reply": "2024-02-29T21:50:28.299181Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e96ad73-2190-48f4-a1c3-ba4b5325f62b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:28.301486Z",
     "iopub.status.busy": "2024-02-29T21:50:28.301199Z",
     "iopub.status.idle": "2024-02-29T21:50:28.674601Z",
     "shell.execute_reply": "2024-02-29T21:50:28.659094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Smiles', 'ecfp', 'target'],\n",
       "        num_rows: 1556\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Smiles', 'ecfp', 'target'],\n",
       "        num_rows: 195\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Smiles', 'ecfp', 'target'],\n",
       "        num_rows: 194\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataset = Dataset.from_pandas(dataframe)\n",
    "train_testvalid = dataset.train_test_split(test_size=0.2, seed=15)\n",
    "\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=15)\n",
    "\n",
    "# 10% for test, 10 for validation, 80% for train\n",
    "dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'validation': test_valid['train']})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63f3e06-7933-457f-a51d-0373c34f413a",
   "metadata": {},
   "source": [
    "### Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7720fb53-0a36-452f-8d5a-f23f0c5755d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:28.678050Z",
     "iopub.status.busy": "2024-02-29T21:50:28.677680Z",
     "iopub.status.idle": "2024-02-29T21:50:28.862589Z",
     "shell.execute_reply": "2024-02-29T21:50:28.861923Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name1)\n",
    "\n",
    "tokenizer.model_max_len=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f924833-477c-4a24-936e-d62f0ca5dd8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:28.865269Z",
     "iopub.status.busy": "2024-02-29T21:50:28.864973Z",
     "iopub.status.idle": "2024-02-29T21:50:29.351257Z",
     "shell.execute_reply": "2024-02-29T21:50:29.350701Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8867b2dda1e54819a566916066ad7cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c482b0b746c149989f58e9d3dc70878b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/195 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee399d43d0d487a8c918e6f5d917f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/194 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Smiles', 'ecfp', 'target', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1556\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Smiles', 'ecfp', 'target', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 195\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Smiles', 'ecfp', 'target', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 194\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "  return tokenizer(batch[\"ecfp\"], truncation=True, max_length=512, padding='max_length')\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "434caf18-e9b1-42ac-b40e-05c1b75c538a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:29.353493Z",
     "iopub.status.busy": "2024-02-29T21:50:29.353238Z",
     "iopub.status.idle": "2024-02-29T21:50:31.369652Z",
     "shell.execute_reply": "2024-02-29T21:50:31.368688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask', 'target']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 00:50:29.783503: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "columns = [\"input_ids\", \"attention_mask\"]\n",
    "columns.extend(['target']) # our labels\n",
    "print(columns)\n",
    "tokenized_dataset.set_format('torch', columns=columns)\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a56144-3102-4a21-b86b-8e851dc97026",
   "metadata": {},
   "source": [
    "### Create Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66254640-38dc-4da6-bccf-634174aca108",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:31.373908Z",
     "iopub.status.busy": "2024-02-29T21:50:31.373142Z",
     "iopub.status.idle": "2024-02-29T21:50:31.389729Z",
     "shell.execute_reply": "2024-02-29T21:50:31.389275Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "class MolecularPropertiesClassification(torch.nn.Module):\n",
    "    def __init__(self, model_name1, model_name2):\n",
    "        super(MolecularPropertiesClassification, self).__init__()\n",
    "\n",
    "        config1 = AutoConfig.from_pretrained(model_name1)\n",
    "        self.transformer1 = AutoModel.from_pretrained(model_name1, config=config1)\n",
    "        # removing last layer of transformer\n",
    "        self.transformer1.pooler = torch.nn.Identity()\n",
    "        # freezing transformer weights\n",
    "        for param in self.transformer1.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        config2 = AutoConfig.from_pretrained(model_name2)\n",
    "        self.transformer2 = AutoModel.from_pretrained(model_name2, config=config2)\n",
    "        # removing last layer of transformer\n",
    "        self.transformer2.pooler = torch.nn.Identity()\n",
    "        # freezing transformer weights\n",
    "        for param in self.transformer2.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(768 * 2, 768, bias=True)\n",
    "        self.linear2 = torch.nn.Linear(768, 2, bias=True)\n",
    "\n",
    "    def forward(self, input_ids = None, attention_mask=None):\n",
    "        outputs1 = self.transformer1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        outputs2 = self.transformer2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state1 = outputs1[0]\n",
    "        last_hidden_state2 = outputs2[0]\n",
    "        \n",
    "        first_linear_out = self.linear1( \\\n",
    "            torch.cat((last_hidden_state1[:, 0, : ], last_hidden_state2[:, 0, : ]), dim=-1).view(-1, 2 * 768))\n",
    "        logits = self.linear2(torch.nn.functional.sigmoid(first_linear_out))\n",
    "\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b0c73-a897-407e-a8c7-158283f97de4",
   "metadata": {},
   "source": [
    "### Create PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79747c27-653a-486a-ab4d-3f2c6ec347f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:31.403903Z",
     "iopub.status.busy": "2024-02-29T21:50:31.394038Z",
     "iopub.status.idle": "2024-02-29T21:50:31.407815Z",
     "shell.execute_reply": "2024-02-29T21:50:31.407269Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset['train'], shuffle = True, batch_size = 64, collate_fn = data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_dataset['validation'], shuffle = True, batch_size = 64, collate_fn = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08055762-1a53-49e6-8cd7-71161852abe8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:31.431856Z",
     "iopub.status.busy": "2024-02-29T21:50:31.418788Z",
     "iopub.status.idle": "2024-02-29T21:50:36.868548Z",
     "shell.execute_reply": "2024-02-29T21:50:36.867767Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at molberto_ecfp0_10M and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at molberto_ecfp0_2M and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\", index=4) if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = MolecularPropertiesClassification(model_name1, model_name2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1428e13-d841-45e7-a885-919d4670a102",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:36.874867Z",
     "iopub.status.busy": "2024-02-29T21:50:36.872881Z",
     "iopub.status.idle": "2024-02-29T21:50:36.882013Z",
     "shell.execute_reply": "2024-02-29T21:50:36.881477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MolecularPropertiesClassification(\n",
       "  (transformer1): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Identity()\n",
       "  )\n",
       "  (transformer2): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Identity()\n",
       "  )\n",
       "  (linear1): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  (linear2): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04001413-80f4-4663-ab5d-a107e839c5c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:36.885304Z",
     "iopub.status.busy": "2024-02-29T21:50:36.884529Z",
     "iopub.status.idle": "2024-02-29T21:50:37.391925Z",
     "shell.execute_reply": "2024-02-29T21:50:37.378332Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexeyorlov53/anaconda3/envs/test/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "num_training_steps = num_epoch * len(train_dataloader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    'linear',\n",
    "    optimizer = optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = num_training_steps,\n",
    ")\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54ef89d4-b7ac-4a86-ab14-67383fd0f726",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:37.396738Z",
     "iopub.status.busy": "2024-02-29T21:50:37.395709Z",
     "iopub.status.idle": "2024-02-29T21:50:43.006530Z",
     "shell.execute_reply": "2024-02-29T21:50:43.005907Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33morlov-aleksei53\u001b[0m (\u001b[33mmoleculary-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/alexeyorlov53/Transformers-for-Molecules/wandb/run-20240301_005038-nwiqbjs7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mDoubleTransformer with LinearClassifier BBBP training \u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/moleculary-ai/efcp_transformer\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/moleculary-ai/efcp_transformer/runs/nwiqbjs7\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/moleculary-ai/efcp_transformer/runs/nwiqbjs7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f5406fe3890>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"efcp_transformer\",\n",
    "    name=\"DoubleTransformer with LinearClassifier BBBP training \",\n",
    "    config={}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a951a1-7c75-47bd-b275-9693eb22fe95",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f0f67d5-c7f4-4ea0-959f-b1a990387a1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T21:50:43.010764Z",
     "iopub.status.busy": "2024-02-29T21:50:43.009497Z",
     "iopub.status.idle": "2024-02-29T22:26:01.934576Z",
     "shell.execute_reply": "2024-02-29T22:26:01.933693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e968d4e52eaa4f83822744a4334d0cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812000c39d0a473e98eae968e6e5c3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "progress_bar_train = tqdm(range(num_training_steps))\n",
    "progress_bar_eval = tqdm(range(num_epoch * len(eval_dataloader)))\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    model.train()\n",
    "    total_pred_labels = []\n",
    "    total_true_labels = []\n",
    "    epoch_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_batch = { k: v.to(device) for k, v in batch.items() if k in ['input_ids', 'attention_mask'] }\n",
    "        batch['target'] = batch['target'].to(device)\n",
    "        \n",
    "        logits = model(**input_batch)\n",
    "        \n",
    "        loss = loss_func(logits.view(-1, 2), batch['target'].view(-1))\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        pred_labels = torch.argmax(logits, dim=-1)\n",
    "        true_labels = batch['target']\n",
    "        total_pred_labels.append(pred_labels)\n",
    "        total_true_labels.append(true_labels)\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar_train.update(1)\n",
    "\n",
    "    total_pred_labels = torch.cat(total_pred_labels).cpu().detach().numpy()\n",
    "    total_true_labels = torch.cat(total_true_labels).cpu().detach().numpy()\n",
    "    \n",
    "    wandb.log({\"loss/train\": epoch_loss / len(train_dataloader)}, step=epoch)\n",
    "    wandb.log({\"accuracy/train\": accuracy_score(total_true_labels, total_pred_labels)}, step=epoch)\n",
    "    wandb.log({\"f1/train\": f1_score(total_true_labels, total_pred_labels, average='micro')}, step=epoch)\n",
    "    wandb.log({\"precision/train\": precision_score(total_true_labels, total_pred_labels, average='micro')}, step=epoch)\n",
    "    wandb.log({\"recall/train\": recall_score(total_true_labels, total_pred_labels, average='micro')}, step=epoch)\n",
    "\n",
    "    model.eval()\n",
    "    total_pred_labels = []\n",
    "    total_true_labels = []\n",
    "    epoch_loss = 0\n",
    "    for batch in eval_dataloader:\n",
    "        input_batch = { k: v.to(device) for k, v in batch.items() if k in ['input_ids', 'attention_mask'] }\n",
    "        batch['target'] = batch['target'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(**input_batch)\n",
    "            loss = loss_func(logits.view(-1, 2), batch['target'].view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            pred_labels = torch.argmax(logits, dim=-1)\n",
    "            true_labels = batch['target']\n",
    "            total_pred_labels.append(pred_labels)\n",
    "            total_true_labels.append(true_labels)\n",
    "        \n",
    "        progress_bar_eval.update(1)\n",
    "\n",
    "    total_pred_labels = torch.cat(total_pred_labels).cpu().detach().numpy()\n",
    "    total_true_labels = torch.cat(total_true_labels).cpu().detach().numpy()\n",
    "    \n",
    "    wandb.log({\"loss/validation\": epoch_loss / len(eval_dataloader)}, step=epoch)\n",
    "    wandb.log({\"accuracy/validation\": accuracy_score(total_true_labels, total_pred_labels)}, step=epoch)\n",
    "    wandb.log({\"f1/validation\": f1_score(total_true_labels, total_pred_labels, average='micro')}, step=epoch)\n",
    "    wandb.log({\"precision/validation\": precision_score(total_true_labels, total_pred_labels, average='micro')}, step=epoch)\n",
    "    wandb.log({\"recall/validation\": recall_score(total_true_labels, total_pred_labels, average='micro')}, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c59bd8ec-528b-4b88-946f-85308c74081d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T22:26:01.956361Z",
     "iopub.status.busy": "2024-02-29T22:26:01.945977Z",
     "iopub.status.idle": "2024-02-29T22:26:19.576581Z",
     "shell.execute_reply": "2024-02-29T22:26:19.575966Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       accuracy/train â–â–‚â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  accuracy/validation â–â–‚â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             f1/train â–â–‚â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        f1/validation â–â–‚â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           loss/train â–ˆâ–†â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      loss/validation â–…â–„â–ƒâ–†â–„â–ƒâ–‡â–…â–â–‚â–â–â–ƒâ–ƒâ–‡â–â–â–ˆâ–‚â–‚â–â–‚â–â–â–â–…â–†â–‚â–â–â–‡â–â–…â–â–ƒâ–â–â–‚â–„â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      precision/train â–â–‚â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: precision/validation â–â–‚â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         recall/train â–â–‚â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    recall/validation â–â–‚â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       accuracy/train 0.89267\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  accuracy/validation 0.86082\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             f1/train 0.89267\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        f1/validation 0.86082\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           loss/train 0.26662\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      loss/validation 0.26937\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      precision/train 0.89267\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: precision/validation 0.86082\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         recall/train 0.89267\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    recall/validation 0.86082\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mDoubleTransformer with LinearClassifier BBBP training \u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/moleculary-ai/efcp_transformer/runs/nwiqbjs7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240301_005038-nwiqbjs7/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "607de5a0-ec7b-490c-b188-4ef0ad28b8c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T22:26:19.579606Z",
     "iopub.status.busy": "2024-02-29T22:26:19.579395Z",
     "iopub.status.idle": "2024-02-29T22:26:22.694552Z",
     "shell.execute_reply": "2024-02-29T22:26:22.693656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5328f4c11647a5905f9fd9daec89b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.log()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m total_pred_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(total_pred_labels)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     24\u001b[0m total_true_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(total_true_labels)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 26\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss/validation\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(eval_dataloader)}, step\u001b[38;5;241m=\u001b[39mepoch)\n\u001b[1;32m     27\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy/validation\u001b[39m\u001b[38;5;124m\"\u001b[39m: accuracy_score(total_true_labels, total_pred_labels)}, step\u001b[38;5;241m=\u001b[39mepoch)\n\u001b[1;32m     28\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1/validation\u001b[39m\u001b[38;5;124m\"\u001b[39m: f1_score(total_true_labels, total_pred_labels, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m'\u001b[39m)}, step\u001b[38;5;241m=\u001b[39mepoch)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/wandb/sdk/lib/preinit.py:36\u001b[0m, in \u001b[0;36mPreInitCallable.<locals>.preinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreinit_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must call wandb.init() before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    tokenized_dataset['test'], batch_size = 64, collate_fn = data_collator\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "total_pred_labels = []\n",
    "total_true_labels = []\n",
    "epoch_loss = 0\n",
    "for batch in tqdm(test_dataloader):\n",
    "    input_batch = { k: v.to(device) for k, v in batch.items() if k in ['input_ids', 'attention_mask'] }\n",
    "    batch['target'] = batch['target'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**input_batch)\n",
    "        loss = loss_func(logits.view(-1, 2), batch['target'].view(-1))\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        pred_labels = torch.argmax(logits, dim=-1)\n",
    "        true_labels = batch['target']\n",
    "        total_pred_labels.append(pred_labels)\n",
    "        total_true_labels.append(true_labels)\n",
    "\n",
    "total_pred_labels = torch.cat(total_pred_labels).cpu().detach().numpy()\n",
    "total_true_labels = torch.cat(total_true_labels).cpu().detach().numpy()\n",
    "\n",
    "wandb.log({\"loss/validation\": epoch_loss / len(eval_dataloader)}, step=epoch)\n",
    "wandb.log({\"accuracy/validation\": accuracy_score(total_true_labels, total_pred_labels)}, step=epoch)\n",
    "wandb.log({\"f1/validation\": f1_score(total_true_labels, total_pred_labels, average='micro')}, step=epoch)\n",
    "wandb.log({\"precision/validation\": precision_score(total_true_labels, total_pred_labels, average='micro')}, step=epoch)\n",
    "wandb.log({\"recall/validation\": recall_score(total_true_labels, total_pred_labels, average='micro')}, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57a96b63-ec67-4374-968e-bd71ff08c91b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T22:26:22.698055Z",
     "iopub.status.busy": "2024-02-29T22:26:22.697611Z",
     "iopub.status.idle": "2024-02-29T22:26:22.718538Z",
     "shell.execute_reply": "2024-02-29T22:26:22.717944Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ad7b9-f82b-4936-bd4f-26266cbb6b25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
