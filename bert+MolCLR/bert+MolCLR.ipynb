{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d6f265e-e433-48e9-a783-950e8b12260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1e3494b-0296-4bb8-9b07-48a55533413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.require(\"service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f53265f1-214e-4d58-814f-6f7c7b05f907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/alexeyorlov53/.netrc\n"
     ]
    }
   ],
   "source": [
    "!python3 -m wandb login eb7b1964fb84cd81de96b2a273ecf2bb6254aeac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d91b4-c4dc-4dba-a04c-e786601aab87",
   "metadata": {},
   "source": [
    "### Upload config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4478096-04a8-4fe1-b1b9-8a8720750818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 16, 'warm_up': 10, 'epochs': 100, 'load_model': 'pretrained_gcn', 'save_every_n_epochs': 5, 'fp16_precision': False, 'init_lr': 0.0005, 'weight_decay': '1e-5', 'gpu': 'cuda:5', 'model_type': 'gin', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 768, 'drop_ratio': 0, 'pool': 'mean'}, 'aug': 'node', 'dataset': {'num_workers': 12, 'valid_size': 0.1, 'data_path': 'data/pubchem-10m-clean.txt'}, 'loss': {'temperature': 0.1, 'use_cosine_similarity': True}, 'loss_params': {'alpha': 1.0, 'beta': 1.0, 'gamma': 1.0}, 'pretrained_roberta_name': 'molberto_ecfp0_2M'}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "config = yaml.load(open(\"config.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4b16779-4235-42fe-9091-531e93496fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 16\n"
     ]
    }
   ],
   "source": [
    "print('batch_size =', config['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8caa6fa0-9b19-4665-81b8-b122a0c18ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device: cuda:5\n"
     ]
    }
   ],
   "source": [
    "print('running on device:', config['gpu'])\n",
    "device = torch.device(config['gpu']) if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe4d265d-3a4d-4ec1-b616-18805240e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_config_file(config, log_dir):\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    with open(os.path.join(log_dir, 'config.yml'), 'w') as outfile:\n",
    "        yaml.dump(config, outfile, default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d58c5-eb7a-4420-b0db-f743e2167213",
   "metadata": {},
   "source": [
    "### Upload and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d5561e6-2477-4ead-9fae-91cae781bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"data_10k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0acb3637-c146-45dc-a1d3-d9ef1363d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.drop(columns=['ecfp2', 'ecfp3', 'Molecular Weight', 'Bioactivities', 'AlogP', 'Polar Surface Area', 'CX Acidic pKa', 'CX Basic pKa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97ce4a4d-2911-4103-aca3-40843f0745e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "      <th>ecfp1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COc1cc(C2(C)CCCc3nc(SCc4ncccn4)n(-c4ccc(F)cc4)...</td>\n",
       "      <td>['2246728737', '864674487', '3217380708', '321...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COC(=O)c1sc(NC(=O)C2c3ccccc3Oc3ccccc32)c(C(=O)...</td>\n",
       "      <td>['2246728737', '864674487', '2246699815', '864...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC[C@H]1OC(=O)C[C@@H](O)[C@H](C)[C@@H](O[C@@H]...</td>\n",
       "      <td>['2246728737', '2245384272', '2976033787', '31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cc1cccc(-n2cc(C(=O)N3CCC[C@@H]([n+]4cc[nH]c4)C...</td>\n",
       "      <td>['2246728737', '3217380708', '3218693969', '32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCOC(=O)[C@H](C1CC1)N1C(=O)[C@@H](CC(=O)O)C[C@...</td>\n",
       "      <td>['2246728737', '2245384272', '864674487', '224...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>CCN1CCN(CC(O)c2ccc(Br)cc2)CC1</td>\n",
       "      <td>['2246728737', '2245384272', '2092489639', '29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>O=C(O)CNC(=O)CNC(=O)CNC(=O)CSC(=O)c1ccccc1</td>\n",
       "      <td>['864942730', '2246699815', '864662311', '2245...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>O=C(N[C@@]12CCC[C@@](C#Cc3ccccn3)(CC1)C2)c1ccc...</td>\n",
       "      <td>['864942730', '2246699815', '847961216', '2976...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>CCOc1ccccc1-c1cc(C(=O)N2CCOCC2)c2ccccc2n1</td>\n",
       "      <td>['2246728737', '2245384272', '864674487', '321...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>C(=N/Nc1nc2ccccc2[nH]1)\\c1cccs1</td>\n",
       "      <td>['2246703798', '847336149', '847961216', '3217...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Smiles  \\\n",
       "0     COc1cc(C2(C)CCCc3nc(SCc4ncccn4)n(-c4ccc(F)cc4)...   \n",
       "1     COC(=O)c1sc(NC(=O)C2c3ccccc3Oc3ccccc32)c(C(=O)...   \n",
       "2     CC[C@H]1OC(=O)C[C@@H](O)[C@H](C)[C@@H](O[C@@H]...   \n",
       "3     Cc1cccc(-n2cc(C(=O)N3CCC[C@@H]([n+]4cc[nH]c4)C...   \n",
       "4     CCOC(=O)[C@H](C1CC1)N1C(=O)[C@@H](CC(=O)O)C[C@...   \n",
       "...                                                 ...   \n",
       "9995                      CCN1CCN(CC(O)c2ccc(Br)cc2)CC1   \n",
       "9996         O=C(O)CNC(=O)CNC(=O)CNC(=O)CSC(=O)c1ccccc1   \n",
       "9997  O=C(N[C@@]12CCC[C@@](C#Cc3ccccn3)(CC1)C2)c1ccc...   \n",
       "9998          CCOc1ccccc1-c1cc(C(=O)N2CCOCC2)c2ccccc2n1   \n",
       "9999                    C(=N/Nc1nc2ccccc2[nH]1)\\c1cccs1   \n",
       "\n",
       "                                                  ecfp1  \n",
       "0     ['2246728737', '864674487', '3217380708', '321...  \n",
       "1     ['2246728737', '864674487', '2246699815', '864...  \n",
       "2     ['2246728737', '2245384272', '2976033787', '31...  \n",
       "3     ['2246728737', '3217380708', '3218693969', '32...  \n",
       "4     ['2246728737', '2245384272', '864674487', '224...  \n",
       "...                                                 ...  \n",
       "9995  ['2246728737', '2245384272', '2092489639', '29...  \n",
       "9996  ['864942730', '2246699815', '864662311', '2245...  \n",
       "9997  ['864942730', '2246699815', '847961216', '2976...  \n",
       "9998  ['2246728737', '2245384272', '864674487', '321...  \n",
       "9999  ['2246703798', '847336149', '847961216', '3217...  \n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cae64520-4a9c-453f-949a-737bbd58bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this because pandas thinks columns with arrays are strings\n",
    "def preprocess_data_dataset(df, column):\n",
    "    for row in tqdm(range(len(df))):\n",
    "        str_ints = eval(df.iloc[row][column])\n",
    "        str_fingerprint = ' '.join(str_ints)\n",
    "        df.at[row, column] = str_fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1eb1370-13e9-4505-88da-b216213a873f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 10830.84it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocess_data_dataset(dataframe, 'ecfp1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8141fce2-4cd2-4e8d-a38b-92703ec5c240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2246728737 864674487 3217380708 3218693969 3217380708 2976816164 2246728737 2968968094 2968968094 2968968094 3217380708 2041434490 3217380708 1026928756 2245384272 3217380708 2041434490 3218693969 3218693969 3218693969 2041434490 2092489639 3217380708 3218693969 3218693969 3217380708 882399112 3218693969 3218693969 3217380708 3218693969 3218693969 3217380708 882399112'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['ecfp1'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287894eb-5d8b-4d23-ab1c-9d3a63a35b8c",
   "metadata": {},
   "source": [
    "### Create Molecule Dataset\n",
    "##### It will generate torch_geometric.data.Data objects for both bert and GIN/GCN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c61aaf7e-b46e-42f0-8981-67d2006f7f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "ATOM_LIST = list(range(1,119))\n",
    "CHIRALITY_LIST = [\n",
    "    Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "    Chem.rdchem.ChiralType.CHI_OTHER\n",
    "]\n",
    "BOND_LIST = [\n",
    "    Chem.rdchem.BondType.SINGLE, \n",
    "    Chem.rdchem.BondType.DOUBLE, \n",
    "    Chem.rdchem.BondType.TRIPLE, \n",
    "    Chem.rdchem.BondType.AROMATIC\n",
    "]\n",
    "BONDDIR_LIST = [\n",
    "    Chem.rdchem.BondDir.NONE,\n",
    "    Chem.rdchem.BondDir.ENDUPRIGHT,\n",
    "    Chem.rdchem.BondDir.ENDDOWNRIGHT\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5d0bdc0-89c4-494d-a76a-5cc7071cca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from torch_geometric.data import Data, Dataset\n",
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, dataset: pd.DataFrame, tokenizer, node_mask_percent=0.25, edge_mask_percent=0.25):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.node_mask_percent = node_mask_percent\n",
    "        self.edge_mask_percent = edge_mask_percent\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.model_max_len = 512\n",
    "\n",
    "    def get_graph_from_smiles(self, smiles):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return torch.tensor([[], []], dtype=torch.long), \\\n",
    "                    torch.tensor(np.array([]), dtype=torch.long), \\\n",
    "                    torch.tensor(np.array([]), dtype=torch.long), \\\n",
    "                    0\n",
    "    \n",
    "        N = mol.GetNumAtoms()\n",
    "        M = mol.GetNumBonds()\n",
    "    \n",
    "        type_idx = []\n",
    "        chirality_idx = []\n",
    "        atomic_number = []\n",
    "        \n",
    "        for atom in mol.GetAtoms():\n",
    "            type_idx.append(ATOM_LIST.index(atom.GetAtomicNum()))\n",
    "            chirality_idx.append(CHIRALITY_LIST.index(atom.GetChiralTag()))\n",
    "            atomic_number.append(atom.GetAtomicNum())\n",
    "        \n",
    "        x1 = torch.tensor(type_idx, dtype=torch.long).view(-1,1)\n",
    "        x2 = torch.tensor(chirality_idx, dtype=torch.long).view(-1,1)\n",
    "        node_feat = torch.cat([x1, x2], dim=-1)\n",
    "    \n",
    "        row, col, edge_feat = [], [], []\n",
    "        for bond in mol.GetBonds():\n",
    "            start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            row += [start, end]\n",
    "            col += [end, start]\n",
    "            \n",
    "            edge_feat.append([\n",
    "                BOND_LIST.index(bond.GetBondType()),\n",
    "                BONDDIR_LIST.index(bond.GetBondDir())\n",
    "            ])\n",
    "            edge_feat.append([\n",
    "                BOND_LIST.index(bond.GetBondType()),\n",
    "                BONDDIR_LIST.index(bond.GetBondDir())\n",
    "            ])\n",
    "    \n",
    "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "        edge_attr = torch.tensor(edge_feat, dtype=torch.long)\n",
    "        num_nodes = N\n",
    "        num_edges = M\n",
    "        return node_feat, edge_index, edge_attr, num_nodes, num_edges\n",
    "\n",
    "    def get_augmented_graph_copy(self, node_feat, edge_index, edge_attr, N, M):\n",
    "        num_mask_nodes = max([1, math.floor(self.node_mask_percent * N)])\n",
    "        num_mask_edges = max([0, math.floor(self.edge_mask_percent * M)])\n",
    "        \n",
    "        mask_nodes = random.sample(list(range(N)), num_mask_nodes)\n",
    "        mask_edges_single = random.sample(list(range(M)), num_mask_edges)\n",
    "        mask_edges = [2*i for i in mask_edges_single] + [2*i+1 for i in mask_edges_single]\n",
    "\n",
    "        node_feat_new = deepcopy(node_feat)\n",
    "        for atom_idx in mask_nodes:\n",
    "            node_feat_new[atom_idx, :] = torch.tensor([len(ATOM_LIST), 0])\n",
    "        edge_index_new = torch.zeros((2, 2*(M - num_mask_edges)), dtype=torch.long)\n",
    "        edge_attr_new = torch.zeros((2*(M - num_mask_edges), 2), dtype=torch.long)\n",
    "        count = 0\n",
    "        for bond_idx in range(2*M):\n",
    "            if bond_idx not in mask_edges:\n",
    "                edge_index_new[:, count] = edge_index[:, bond_idx]\n",
    "                edge_attr_new[count, :] = edge_attr[bond_idx, :]\n",
    "                count += 1\n",
    "        return Data(x=node_feat_new, edge_index=edge_index_new, edge_attr=edge_attr_new)\n",
    "\n",
    "    def tokenize(self, item):\n",
    "        return self.tokenizer(item, truncation=True, max_length=512, padding='max_length')\n",
    "\n",
    "    def mlm(self, tensor):\n",
    "        rand = torch.rand(tensor.shape)\n",
    "        # mask random 15% where token is not 0 <s>, 1 <pad>, or 2 <s/>\n",
    "        mask_arr = (rand < .15) * (tensor != 0) * (tensor != 1) * (tensor != 2)\n",
    "        selection = torch.flatten(mask_arr.nonzero()).tolist()\n",
    "        # mask tensor, token == 4 is our mask token\n",
    "        tensor[selection] = 4\n",
    "        return tensor\n",
    "\n",
    "    def apply_mlm(self, sample):\n",
    "        labels = torch.tensor(sample.input_ids)\n",
    "        attention_mask = torch.tensor(sample.attention_mask)\n",
    "        input_ids = self.mlm(labels.detach().clone())\n",
    "        return Data(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    def get_ecfp(smiles, radius=0):\n",
    "        mol = AllChem.MolFromSmiles(smiles, sanitize=False)\n",
    "        mol = rdMolStandardize.Normalize(mol)\n",
    "        radii = list(range(int(radius) + 1))\n",
    "        info = {}\n",
    "        _ = AllChem.GetMorganFingerprint(mol, radius, bitInfo=info)  # info: dictionary identifier, atom_idx, radius\n",
    "        \n",
    "        \n",
    "        mol_atoms = [a.GetIdx() for a in mol.GetAtoms()]\n",
    "        dict_atoms = {x: {r: None for r in radii} for x in mol_atoms}\n",
    "        \n",
    "        for element in info:\n",
    "          for atom_idx, radius_at in info[element]:\n",
    "              dict_atoms[atom_idx][radius_at] = element  # {atom number: {fp radius: identifier}}\n",
    "        \n",
    "          # iterate over all atoms and radii\n",
    "        identifier_sentences = []\n",
    "        \n",
    "        for r in radii:  # iterate over radii to get one sentence per radius\n",
    "            identifiers = []\n",
    "            for atom in dict_atoms:  # iterate over atoms\n",
    "                # get one sentence per radius\n",
    "                identifiers.append(dict_atoms[atom][r])\n",
    "            identifier_sentences.append(list(map(str, [x for x in identifiers if x])))\n",
    "        return list(identifier_sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        node_feat, edge_index, edge_attr, num_nodes, num_edges = self.get_graph_from_smiles(self.dataset['Smiles'][index])\n",
    "\n",
    "        data_i = self.get_augmented_graph_copy(node_feat, edge_index, edge_attr, num_nodes, num_edges)\n",
    "        data_j = self.get_augmented_graph_copy(node_feat, edge_index, edge_attr, num_nodes, num_edges)\n",
    "\n",
    "        ecfp = get_ecfp(self.dataset['Smiles'][index])\n",
    "        data_for_bert = self.apply_mlm(self.tokenize(ecfp))\n",
    "        return data_for_bert, data_i, data_j\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def get(self):\n",
    "        pass\n",
    "    def len(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "990d0346-4204-4edd-a947-de80e85a685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name_bert = 'molberto_ecfp0_2M'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_bert)\n",
    "dataset = MoleculeDataset(dataframe, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef882144-9701-4c8e-b169-0e11113172b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "num_train = len(dataset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = int(np.floor(config['dataset']['valid_size'] * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset, batch_size=config['batch_size'], sampler=train_sampler,\n",
    "    num_workers=config['dataset']['num_workers'], drop_last=True\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    dataset, batch_size=config['batch_size'], sampler=valid_sampler,\n",
    "    num_workers=config['dataset']['num_workers'], drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a56144-3102-4a21-b86b-8e851dc97026",
   "metadata": {},
   "source": [
    "### Create Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66254640-38dc-4da6-bccf-634174aca108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "if config['model_type'] == 'gin':\n",
    "    from MolCLR.models.ginet_molclr import GINet as GraphModel\n",
    "elif config['model_type'] == 'gcn':\n",
    "    from MolCLR.models.gcn_molclr import GCN as GraphModel\n",
    "else:\n",
    "    raise ValueError('GNN model is not defined in config.')\n",
    "from MolCLR.utils.nt_xent import NTXentLoss\n",
    "\n",
    "class MolecularBertGraph(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MolecularBertGraph, self).__init__()\n",
    "        self.batch_size = config['batch_size']\n",
    "\n",
    "        roberta_config = RobertaConfig(\n",
    "            vocab_size=30_522,\n",
    "            max_position_embeddings=514,\n",
    "            hidden_size=768,\n",
    "            num_attention_heads=12,\n",
    "            num_hidden_layers=6,\n",
    "            type_vocab_size=1\n",
    "        )\n",
    "        self.bert = RobertaForMaskedLM(roberta_config).from_pretrained(config['pretrained_roberta_name'], \\\n",
    "                                                                       config=roberta_config)\n",
    "        self.graph_model = GraphModel(**config['model'])\n",
    "        self.graph_model = self._load_graph_pretrained_weights(self.graph_model)\n",
    "\n",
    "        self.out_graph_linear = torch.nn.Linear(768 * 2, 768, bias=True)\n",
    "\n",
    "        # contrastive loss for MolCLR\n",
    "        self.nt_xent_criterion = NTXentLoss(device, self.batch_size, **config['loss'])\n",
    "        # cosine distance as loss between models\n",
    "        self.cosine_sim = torch.nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, bert_batch, graph_batch1, graph_batch2):\n",
    "        bert_output = self.bert(input_ids=bert_batch['input_ids'].view(self.batch_size, -1), \n",
    "                                 attention_mask=bert_batch['attention_mask'].view(self.batch_size, -1),\n",
    "                                 labels=bert_batch['labels'].view(self.batch_size, -1), output_hidden_states=True)\n",
    "        bert_loss = bert_output.loss\n",
    "        bert_emb = bert_output.hidden_states[0][:, 0, :] # take emb for CLS token\n",
    "\n",
    "        graph_loss, hidden_states_1, hidden_states_2 = self.graph_step(graph_batch1, graph_batch2)\n",
    "        graph_emb = self.out_graph_linear(torch.cat((hidden_states_1, hidden_states_2), dim=-1))\n",
    "\n",
    "        bimodal_loss = ((1 - self.cosine_sim(bert_emb, graph_emb))**2).mean()\n",
    "        # bimodal_loss = self.nt_xent_criterion(bert_emb, graph_emb)\n",
    "        return bert_loss, graph_loss, bimodal_loss\n",
    "\n",
    "    def graph_step(self, xis, xjs):\n",
    "        # get the representations and the projections\n",
    "        ris, zis = self.graph_model(xis)  # [N,C]\n",
    "    \n",
    "        # get the representations and the projections\n",
    "        rjs, zjs = self.graph_model(xjs)  # [N,C]\n",
    "    \n",
    "        # normalize projection feature vectors\n",
    "        zis = torch.nn.functional.normalize(zis, dim=1)\n",
    "        zjs = torch.nn.functional.normalize(zjs, dim=1)\n",
    "    \n",
    "        loss = self.nt_xent_criterion(zis, zjs)\n",
    "        return loss, ris, rjs\n",
    "        \n",
    "    def _load_graph_pretrained_weights(self, model):\n",
    "        try:\n",
    "            checkpoints_folder = os.path.join('MolCLR', 'ckpt', config['load_model'], 'checkpoints')\n",
    "            print(os.path.join(checkpoints_folder, 'model.pth'))\n",
    "            state_dict = torch.load(os.path.join(checkpoints_folder, 'model.pth'))\n",
    "            \n",
    "            model.load_state_dict(state_dict)\n",
    "            print(\"Loaded pre-trained model with success.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Pre-trained weights not found. Training from scratch.\")\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08055762-1a53-49e6-8cd7-71161852abe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MolCLR/ckpt/pretrained_gcn/checkpoints/model.pth\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GINet:\n\tMissing key(s) in state_dict: \"gnns.0.mlp.0.weight\", \"gnns.0.mlp.0.bias\", \"gnns.0.mlp.2.weight\", \"gnns.0.mlp.2.bias\", \"gnns.1.mlp.0.weight\", \"gnns.1.mlp.0.bias\", \"gnns.1.mlp.2.weight\", \"gnns.1.mlp.2.bias\", \"gnns.2.mlp.0.weight\", \"gnns.2.mlp.0.bias\", \"gnns.2.mlp.2.weight\", \"gnns.2.mlp.2.bias\", \"gnns.3.mlp.0.weight\", \"gnns.3.mlp.0.bias\", \"gnns.3.mlp.2.weight\", \"gnns.3.mlp.2.bias\", \"gnns.4.mlp.0.weight\", \"gnns.4.mlp.0.bias\", \"gnns.4.mlp.2.weight\", \"gnns.4.mlp.2.bias\". \n\tUnexpected key(s) in state_dict: \"gnns.0.weight\", \"gnns.0.bias\", \"gnns.1.weight\", \"gnns.1.bias\", \"gnns.2.weight\", \"gnns.2.bias\", \"gnns.3.weight\", \"gnns.3.bias\", \"gnns.4.weight\", \"gnns.4.bias\". \n\tsize mismatch for gnns.0.edge_embedding1.weight: copying a param with shape torch.Size([5, 1]) from checkpoint, the shape in current model is torch.Size([5, 300]).\n\tsize mismatch for gnns.0.edge_embedding2.weight: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([3, 300]).\n\tsize mismatch for gnns.1.edge_embedding1.weight: copying a param with shape torch.Size([5, 1]) from checkpoint, the shape in current model is torch.Size([5, 300]).\n\tsize mismatch for gnns.1.edge_embedding2.weight: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([3, 300]).\n\tsize mismatch for gnns.2.edge_embedding1.weight: copying a param with shape torch.Size([5, 1]) from checkpoint, the shape in current model is torch.Size([5, 300]).\n\tsize mismatch for gnns.2.edge_embedding2.weight: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([3, 300]).\n\tsize mismatch for gnns.3.edge_embedding1.weight: copying a param with shape torch.Size([5, 1]) from checkpoint, the shape in current model is torch.Size([5, 300]).\n\tsize mismatch for gnns.3.edge_embedding2.weight: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([3, 300]).\n\tsize mismatch for gnns.4.edge_embedding1.weight: copying a param with shape torch.Size([5, 1]) from checkpoint, the shape in current model is torch.Size([5, 300]).\n\tsize mismatch for gnns.4.edge_embedding2.weight: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([3, 300]).\n\tsize mismatch for feat_lin.weight: copying a param with shape torch.Size([512, 300]) from checkpoint, the shape in current model is torch.Size([768, 300]).\n\tsize mismatch for feat_lin.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for out_lin.0.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for out_lin.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for out_lin.2.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([384, 768]).\n\tsize mismatch for out_lin.2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMolecularBertGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[17], line 28\u001b[0m, in \u001b[0;36mMolecularBertGraph.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert \u001b[38;5;241m=\u001b[39m RobertaForMaskedLM(roberta_config)\u001b[38;5;241m.\u001b[39mfrom_pretrained(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrained_roberta_name\u001b[39m\u001b[38;5;124m'\u001b[39m], \\\n\u001b[1;32m     26\u001b[0m                                                                config\u001b[38;5;241m=\u001b[39mroberta_config)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_model \u001b[38;5;241m=\u001b[39m GraphModel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_graph_pretrained_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_graph_linear \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m768\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m768\u001b[39m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# contrastive loss for MolCLR\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 71\u001b[0m, in \u001b[0;36mMolecularBertGraph._load_graph_pretrained_weights\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoints_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     69\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoints_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 71\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded pre-trained model with success.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GINet:\n\tMissing key(s) in state_dict: \"gnns.0.mlp.0.weight\", \"gnns.0.mlp.0.bias\", \"gnns.0.mlp.2.weight\", \"gnns.0.mlp.2.bias\", \"gnns.1.mlp.0.weight\", \"gnns.1.mlp.0.bias\", \"gnns.1.mlp.2.weight\", \"gnns.1.mlp.2.bias\", \"gnns.2.mlp.0.weight\", \"gnns.2.mlp.0.bias\", \"gnns.2.mlp.2.weight\", \"gnns.2.mlp.2.bias\", \"gnns.3.mlp.0.weight\", \"gnns.3.mlp.0.bias\", \"gnns.3.mlp.2.weight\", \"gnns.3.mlp.2.bias\", \"gnns.4.mlp.0.weight\", \"gnns.4.mlp.0.bias\", \"gnns.4.mlp.2.weight\", \"gnns.4.mlp.2.bias\". \n\tUnexpected key(s) in state_dict: \"gnns.0.weight\", \"gnns.0.bias\", \"gnns.1.weight\", \"gnns.1.bias\", \"gnns.2.weight\", \"gnns.2.bias\", \"gnns.3.weight\", \"gnns.3.bias\", \"gnns.4.weight\", \"gnns.4.bias\". \n\tsize mismatch for gnns.0.edge_embedding1.weight: copying a param with shape torch.Size([5, 1]) from checkpoint, the shape in current model is torch.Size([5, 300]).\n\tsize mismatch for gnns.0.edge_embedding2.weight: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([3, 300]).\n\tsize mismatch for gnns.1.edge_embedding1.weight: copying a param with shape torch.Size([5, 1]) from checkpoint, the shape in current model is torch.Size([5, 300]).\n\tsize mismatch for gnns.1.edge_embedding2.weight: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([3, 300]).\n\tsize mismatch for gnns.2.edge_embedding1.weight: copying a param with shape torch.Size([5, 1]) from checkpoint, the shape in current model is torch.Size([5, 300]).\n\tsize mismatch for gnns.2.edge_embedding2.weight: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([3, 300]).\n\tsize mismatch for gnns.3.edge_embedding1.weight: copying a param with shape torch.Size([5, 1]) from checkpoint, the shape in current model is torch.Size([5, 300]).\n\tsize mismatch for gnns.3.edge_embedding2.weight: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([3, 300]).\n\tsize mismatch for gnns.4.edge_embedding1.weight: copying a param with shape torch.Size([5, 1]) from checkpoint, the shape in current model is torch.Size([5, 300]).\n\tsize mismatch for gnns.4.edge_embedding2.weight: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([3, 300]).\n\tsize mismatch for feat_lin.weight: copying a param with shape torch.Size([512, 300]) from checkpoint, the shape in current model is torch.Size([768, 300]).\n\tsize mismatch for feat_lin.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for out_lin.0.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for out_lin.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for out_lin.2.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([384, 768]).\n\tsize mismatch for out_lin.2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384])."
     ]
    }
   ],
   "source": [
    "model = MolecularBertGraph().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b0c73-a897-407e-a8c7-158283f97de4",
   "metadata": {},
   "source": [
    "### Define utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04001413-80f4-4663-ab5d-a107e839c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = config['epochs']\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), config['init_lr'], \n",
    "    weight_decay=eval(config['weight_decay'])\n",
    ")\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "#     optimizer, T_max=config['epochs']-config['warm_up'], \n",
    "#     eta_min=0, last_epoch=-1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ef89d4-b7ac-4a86-ab14-67383fd0f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"efcp_transformer\",\n",
    "    name=\"Pretrained RobertaForMaskedLM + pretrained MolCLR (GIN) 10k equal_coeffs\",\n",
    "    config={}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a951a1-7c75-47bd-b275-9693eb22fe95",
   "metadata": {},
   "source": [
    "### Training (with validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f660cb2-97a3-4102-b85e-c6e23c84429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = config['loss_params']['alpha']\n",
    "beta = config['loss_params']['beta']\n",
    "gamma = config['loss_params']['gamma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ea2e1e-8d54-4f03-914f-6b6328b5da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96388fce-0dc6-43a3-b800-5133ea476cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    train_tqdm = tqdm(train_dataloader, unit=\"batch\")\n",
    "    train_tqdm.set_description(f'Epoch {epoch_counter}')\n",
    "    bert_loss_sum, graph_model_loss_sum, bimodal_loss_sum, loss_sum = 0, 0, 0, 0\n",
    "    \n",
    "    model.train()\n",
    "    for (bert_batch, graph_batch1, graph_batch2) in train_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        bert_batch = bert_batch.to(device)\n",
    "        graph_batch1 = graph_batch1.to(device)\n",
    "        graph_batch2 = graph_batch2.to(device)\n",
    "\n",
    "        bert_loss, graph_loss, bimodal_loss = model(bert_batch, graph_batch1, graph_batch2)\n",
    "\n",
    "        loss = alpha * bert_loss + beta * graph_loss + gamma * bimodal_loss\n",
    "        loss.backward()\n",
    "\n",
    "        bert_loss_sum += bert_loss.item()\n",
    "        graph_model_loss_sum += graph_loss.item()\n",
    "        bimodal_loss_sum += bimodal_loss.item()\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_tqdm.set_postfix(loss=loss.item(), bert_loss=bert_loss.item(), graph_loss=graph_loss.item(), bimodal_loss=bimodal_loss.item())\n",
    "    return bert_loss_sum / len(train_dataloader), graph_model_loss_sum / len(train_dataloader), bimodal_loss_sum / len(train_dataloader), loss_sum / len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af92a73-77ad-4bd7-9727-7bb5337fd001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop():\n",
    "    eval_tqdm = tqdm(eval_dataloader, unit=\"batch\")\n",
    "    eval_tqdm.set_description(f'Epoch {epoch_counter}')\n",
    "    bert_loss_sum, graph_model_loss_sum, bimodal_loss_sum, loss_sum = 0, 0, 0, 0\n",
    "    \n",
    "    model.eval()\n",
    "    for (bert_batch, graph_batch1, graph_batch2) in eval_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        bert_batch = bert_batch.to(device)\n",
    "        graph_batch1 = graph_batch1.to(device)\n",
    "        graph_batch2 = graph_batch2.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bert_loss, graph_loss, bimodal_loss = model(bert_batch, graph_batch1, graph_batch2)\n",
    "\n",
    "        loss = alpha * bert_loss + beta * graph_loss + gamma * bimodal_loss\n",
    "\n",
    "        bert_loss_sum += bert_loss.item()\n",
    "        graph_model_loss_sum += graph_loss.item()\n",
    "        bimodal_loss_sum += bimodal_loss.item()\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        eval_tqdm.set_postfix(loss=loss.item(), bert_loss=bert_loss.item(), graph_loss=graph_loss.item(), bimodal_loss=bimodal_loss.item())\n",
    "    return bert_loss_sum / len(eval_dataloader), graph_model_loss_sum / len(eval_dataloader), bimodal_loss_sum / len(eval_dataloader), loss_sum / len(eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb042f-7934-4e54-b202-fe736e4396d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_loss, graph_loss, bimodal_loss, loss = eval_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc9568-bd68-4dbd-a797-d756f697c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('bert_loss =', bert_loss)\n",
    "print('graph_loss = ', graph_loss)\n",
    "print('bimodal_loss =', bimodal_loss)\n",
    "print('sum of losses =', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd36a8-610d-4760-a984-79bf17e5b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "model_checkpoints_folder = os.path.join('ckpts')\n",
    "dir_name = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "log_dir = os.path.join(model_checkpoints_folder, dir_name)\n",
    "_save_config_file(config, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a426df-461e-4180-9186-c5471b4df374",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 0\n",
    "valid_n_iter = 0\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "for epoch_counter in range(num_epoch):\n",
    "    bert_loss, graph_loss, bimodal_loss, loss = train_loop()\n",
    "\n",
    "    wandb.log({\"bert_loss/train\": bert_loss}, step=epoch_counter)\n",
    "    wandb.log({\"graph_loss/train\": graph_loss}, step=epoch_counter)\n",
    "    wandb.log({\"bimodal_loss/train\": bimodal_loss}, step=epoch_counter)\n",
    "    wandb.log({\"loss/train\": loss}, step=epoch_counter)\n",
    "\n",
    "    bert_loss, graph_loss, bimodal_loss, loss = eval_loop()\n",
    "\n",
    "    wandb.log({\"bert_loss/eval\": bert_loss}, step=epoch_counter)\n",
    "    wandb.log({\"graph_loss/eval\": graph_loss}, step=epoch_counter)\n",
    "    wandb.log({\"bimodal_loss/eval\": bimodal_loss}, step=epoch_counter)\n",
    "    wandb.log({\"loss/eval\": loss}, step=epoch_counter)\n",
    "    \n",
    "    if loss < best_valid_loss:\n",
    "        best_valid_loss = loss\n",
    "        torch.save(model.state_dict(), os.path.join(log_dir, 'model.pth'))\n",
    "    \n",
    "    if (epoch_counter + 1) % config['save_every_n_epochs'] == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(log_dir, 'model_{}.pth'.format(str(epoch_counter))))\n",
    "\n",
    "    # # warmup for the first few epochs\n",
    "    # if epoch_counter >= config['warm_up']:\n",
    "        # wandb.log({\"cosine_lr_decay\": scheduler.get_last_lr()[0]}, step=epoch_counter)\n",
    "        # scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ad7b9-f82b-4936-bd4f-26266cbb6b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fac871-4da6-496e-b1a6-e4e21259c1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe8dbd0-e453-4cd9-aeb3-4f88a4590b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dd4c9d-9edf-4d11-9d74-cf0c1bf90ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c9ed63-eaf7-4e39-b91b-cb8d9aa00ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_batch = bert_batch.to('cpu')\n",
    "# graph_batch1 = graph_batch1.to('cpu')\n",
    "# graph_batch2 = graph_batch2.to('cpu')\n",
    "# del bert_batch, graph_batch1, graph_batch2\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
