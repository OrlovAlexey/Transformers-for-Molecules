{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d6f265e-e433-48e9-a783-950e8b12260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1e3494b-0296-4bb8-9b07-48a55533413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.require(\"service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f53265f1-214e-4d58-814f-6f7c7b05f907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/alexeyorlov53/.netrc\n"
     ]
    }
   ],
   "source": [
    "!python3 -m wandb login eb7b1964fb84cd81de96b2a273ecf2bb6254aeac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d91b4-c4dc-4dba-a04c-e786601aab87",
   "metadata": {},
   "source": [
    "### Upload config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4478096-04a8-4fe1-b1b9-8a8720750818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 32, 'warm_up': 10, 'epochs': 100, 'load_model': 'None', 'save_every_n_epochs': 5, 'fp16_precision': False, 'init_lr': 0.0005, 'weight_decay': '1e-5', 'gpu': 'cuda:1', 'model_type': 'gcn', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 768, 'drop_ratio': 0, 'pool': 'mean'}, 'aug': 'node', 'dataset': {'num_workers': 12, 'valid_size': 0.1, 'data_path': 'data/pubchem-10m-clean.txt'}, 'loss': {'temperature': 0.1, 'use_cosine_similarity': True}, 'loss_params': {'alpha': 1.0, 'beta': 1.0, 'gamma': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "config = yaml.load(open(\"config.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4b16779-4235-42fe-9091-531e93496fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 32\n"
     ]
    }
   ],
   "source": [
    "print('batch_size =', config['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8caa6fa0-9b19-4665-81b8-b122a0c18ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "print('running on device:', config['gpu'])\n",
    "device = torch.device(config['gpu']) if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe4d265d-3a4d-4ec1-b616-18805240e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_config_file(config, log_dir):\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    with open(os.path.join(log_dir, 'config.yml'), 'w') as outfile:\n",
    "        yaml.dump(config, outfile, default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d58c5-eb7a-4420-b0db-f743e2167213",
   "metadata": {},
   "source": [
    "### Upload and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d5561e6-2477-4ead-9fae-91cae781bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"data_10k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0acb3637-c146-45dc-a1d3-d9ef1363d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.drop(columns=['ecfp2', 'ecfp3', 'Molecular Weight', 'Bioactivities', 'AlogP', 'Polar Surface Area', 'CX Acidic pKa', 'CX Basic pKa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97ce4a4d-2911-4103-aca3-40843f0745e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Smiles</th>\n",
       "      <th>ecfp1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COc1cc(C2(C)CCCc3nc(SCc4ncccn4)n(-c4ccc(F)cc4)...</td>\n",
       "      <td>['2246728737', '864674487', '3217380708', '321...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COC(=O)c1sc(NC(=O)C2c3ccccc3Oc3ccccc32)c(C(=O)...</td>\n",
       "      <td>['2246728737', '864674487', '2246699815', '864...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC[C@H]1OC(=O)C[C@@H](O)[C@H](C)[C@@H](O[C@@H]...</td>\n",
       "      <td>['2246728737', '2245384272', '2976033787', '31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cc1cccc(-n2cc(C(=O)N3CCC[C@@H]([n+]4cc[nH]c4)C...</td>\n",
       "      <td>['2246728737', '3217380708', '3218693969', '32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCOC(=O)[C@H](C1CC1)N1C(=O)[C@@H](CC(=O)O)C[C@...</td>\n",
       "      <td>['2246728737', '2245384272', '864674487', '224...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>CCN1CCN(CC(O)c2ccc(Br)cc2)CC1</td>\n",
       "      <td>['2246728737', '2245384272', '2092489639', '29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>O=C(O)CNC(=O)CNC(=O)CNC(=O)CSC(=O)c1ccccc1</td>\n",
       "      <td>['864942730', '2246699815', '864662311', '2245...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>O=C(N[C@@]12CCC[C@@](C#Cc3ccccn3)(CC1)C2)c1ccc...</td>\n",
       "      <td>['864942730', '2246699815', '847961216', '2976...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>CCOc1ccccc1-c1cc(C(=O)N2CCOCC2)c2ccccc2n1</td>\n",
       "      <td>['2246728737', '2245384272', '864674487', '321...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>C(=N/Nc1nc2ccccc2[nH]1)\\c1cccs1</td>\n",
       "      <td>['2246703798', '847336149', '847961216', '3217...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Smiles  \\\n",
       "0     COc1cc(C2(C)CCCc3nc(SCc4ncccn4)n(-c4ccc(F)cc4)...   \n",
       "1     COC(=O)c1sc(NC(=O)C2c3ccccc3Oc3ccccc32)c(C(=O)...   \n",
       "2     CC[C@H]1OC(=O)C[C@@H](O)[C@H](C)[C@@H](O[C@@H]...   \n",
       "3     Cc1cccc(-n2cc(C(=O)N3CCC[C@@H]([n+]4cc[nH]c4)C...   \n",
       "4     CCOC(=O)[C@H](C1CC1)N1C(=O)[C@@H](CC(=O)O)C[C@...   \n",
       "...                                                 ...   \n",
       "9995                      CCN1CCN(CC(O)c2ccc(Br)cc2)CC1   \n",
       "9996         O=C(O)CNC(=O)CNC(=O)CNC(=O)CSC(=O)c1ccccc1   \n",
       "9997  O=C(N[C@@]12CCC[C@@](C#Cc3ccccn3)(CC1)C2)c1ccc...   \n",
       "9998          CCOc1ccccc1-c1cc(C(=O)N2CCOCC2)c2ccccc2n1   \n",
       "9999                    C(=N/Nc1nc2ccccc2[nH]1)\\c1cccs1   \n",
       "\n",
       "                                                  ecfp1  \n",
       "0     ['2246728737', '864674487', '3217380708', '321...  \n",
       "1     ['2246728737', '864674487', '2246699815', '864...  \n",
       "2     ['2246728737', '2245384272', '2976033787', '31...  \n",
       "3     ['2246728737', '3217380708', '3218693969', '32...  \n",
       "4     ['2246728737', '2245384272', '864674487', '224...  \n",
       "...                                                 ...  \n",
       "9995  ['2246728737', '2245384272', '2092489639', '29...  \n",
       "9996  ['864942730', '2246699815', '864662311', '2245...  \n",
       "9997  ['864942730', '2246699815', '847961216', '2976...  \n",
       "9998  ['2246728737', '2245384272', '864674487', '321...  \n",
       "9999  ['2246703798', '847336149', '847961216', '3217...  \n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cae64520-4a9c-453f-949a-737bbd58bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this because pandas thinks columns with arrays are strings\n",
    "def preprocess_data_dataset(df, column):\n",
    "    for row in tqdm(range(len(df))):\n",
    "        str_ints = eval(df.iloc[row][column])\n",
    "        str_fingerprint = ' '.join(str_ints)\n",
    "        df.at[row, column] = str_fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1eb1370-13e9-4505-88da-b216213a873f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:01<00:00, 8716.30it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocess_data_dataset(dataframe, 'ecfp1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287894eb-5d8b-4d23-ab1c-9d3a63a35b8c",
   "metadata": {},
   "source": [
    "### Create Molecule Dataset\n",
    "##### It will generate torch_geometric.data.Data objects for both bert and GIN/GCN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c61aaf7e-b46e-42f0-8981-67d2006f7f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "ATOM_LIST = list(range(1,119))\n",
    "CHIRALITY_LIST = [\n",
    "    Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "    Chem.rdchem.ChiralType.CHI_OTHER\n",
    "]\n",
    "BOND_LIST = [\n",
    "    Chem.rdchem.BondType.SINGLE, \n",
    "    Chem.rdchem.BondType.DOUBLE, \n",
    "    Chem.rdchem.BondType.TRIPLE, \n",
    "    Chem.rdchem.BondType.AROMATIC\n",
    "]\n",
    "BONDDIR_LIST = [\n",
    "    Chem.rdchem.BondDir.NONE,\n",
    "    Chem.rdchem.BondDir.ENDUPRIGHT,\n",
    "    Chem.rdchem.BondDir.ENDDOWNRIGHT\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5d0bdc0-89c4-494d-a76a-5cc7071cca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from torch_geometric.data import Data, Dataset\n",
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, dataset: pd.DataFrame, tokenizer, node_mask_percent=0.25, edge_mask_percent=0.25):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.node_mask_percent = node_mask_percent\n",
    "        self.edge_mask_percent = edge_mask_percent\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.model_max_len = 512\n",
    "\n",
    "    def get_graph_from_smiles(self, smiles):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return torch.tensor([[], []], dtype=torch.long), \\\n",
    "                    torch.tensor(np.array([]), dtype=torch.long), \\\n",
    "                    torch.tensor(np.array([]), dtype=torch.long), \\\n",
    "                    0\n",
    "    \n",
    "        N = mol.GetNumAtoms()\n",
    "        M = mol.GetNumBonds()\n",
    "    \n",
    "        type_idx = []\n",
    "        chirality_idx = []\n",
    "        atomic_number = []\n",
    "        \n",
    "        for atom in mol.GetAtoms():\n",
    "            type_idx.append(ATOM_LIST.index(atom.GetAtomicNum()))\n",
    "            chirality_idx.append(CHIRALITY_LIST.index(atom.GetChiralTag()))\n",
    "            atomic_number.append(atom.GetAtomicNum())\n",
    "        \n",
    "        x1 = torch.tensor(type_idx, dtype=torch.long).view(-1,1)\n",
    "        x2 = torch.tensor(chirality_idx, dtype=torch.long).view(-1,1)\n",
    "        node_feat = torch.cat([x1, x2], dim=-1)\n",
    "    \n",
    "        row, col, edge_feat = [], [], []\n",
    "        for bond in mol.GetBonds():\n",
    "            start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            row += [start, end]\n",
    "            col += [end, start]\n",
    "            \n",
    "            edge_feat.append([\n",
    "                BOND_LIST.index(bond.GetBondType()),\n",
    "                BONDDIR_LIST.index(bond.GetBondDir())\n",
    "            ])\n",
    "            edge_feat.append([\n",
    "                BOND_LIST.index(bond.GetBondType()),\n",
    "                BONDDIR_LIST.index(bond.GetBondDir())\n",
    "            ])\n",
    "    \n",
    "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "        edge_attr = torch.tensor(edge_feat, dtype=torch.long)\n",
    "        num_nodes = N\n",
    "        num_edges = M\n",
    "        return node_feat, edge_index, edge_attr, num_nodes, num_edges\n",
    "\n",
    "    def get_augmented_graph_copy(self, node_feat, edge_index, edge_attr, N, M):\n",
    "        num_mask_nodes = max([1, math.floor(self.node_mask_percent * N)])\n",
    "        num_mask_edges = max([0, math.floor(self.edge_mask_percent * M)])\n",
    "        \n",
    "        mask_nodes = random.sample(list(range(N)), num_mask_nodes)\n",
    "        mask_edges_single = random.sample(list(range(M)), num_mask_edges)\n",
    "        mask_edges = [2*i for i in mask_edges_single] + [2*i+1 for i in mask_edges_single]\n",
    "\n",
    "        node_feat_new = deepcopy(node_feat)\n",
    "        for atom_idx in mask_nodes:\n",
    "            node_feat_new[atom_idx, :] = torch.tensor([len(ATOM_LIST), 0])\n",
    "        edge_index_new = torch.zeros((2, 2*(M - num_mask_edges)), dtype=torch.long)\n",
    "        edge_attr_new = torch.zeros((2*(M - num_mask_edges), 2), dtype=torch.long)\n",
    "        count = 0\n",
    "        for bond_idx in range(2*M):\n",
    "            if bond_idx not in mask_edges:\n",
    "                edge_index_new[:, count] = edge_index[:, bond_idx]\n",
    "                edge_attr_new[count, :] = edge_attr[bond_idx, :]\n",
    "                count += 1\n",
    "        return Data(x=node_feat_new, edge_index=edge_index_new, edge_attr=edge_attr_new)\n",
    "\n",
    "    def tokenize(self, item):\n",
    "        return self.tokenizer(item, truncation=True, max_length=512, padding='max_length')\n",
    "\n",
    "    def mlm(self, tensor):\n",
    "        rand = torch.rand(tensor.shape)\n",
    "        # mask random 15% where token is not 0 <s>, 1 <pad>, or 2 <s/>\n",
    "        mask_arr = (rand < .15) * (tensor != 0) * (tensor != 1) * (tensor != 2)\n",
    "        selection = torch.flatten(mask_arr.nonzero()).tolist()\n",
    "        # mask tensor, token == 4 is our mask token\n",
    "        tensor[selection] = 4\n",
    "        return tensor\n",
    "\n",
    "    def apply_mlm(self, sample):\n",
    "        labels = torch.tensor(sample.input_ids)\n",
    "        attention_mask = torch.tensor(sample.attention_mask)\n",
    "        input_ids = self.mlm(labels.detach().clone())\n",
    "        return Data(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        node_feat, edge_index, edge_attr, num_nodes, num_edges = self.get_graph_from_smiles(self.dataset['Smiles'][index])\n",
    "\n",
    "        data_i = self.get_augmented_graph_copy(node_feat, edge_index, edge_attr, num_nodes, num_edges)\n",
    "        data_j = self.get_augmented_graph_copy(node_feat, edge_index, edge_attr, num_nodes, num_edges)\n",
    "\n",
    "        ecfp = self.dataset['ecfp1'][index]\n",
    "        data_for_bert = self.apply_mlm(self.tokenize(ecfp))\n",
    "        return data_for_bert, data_i, data_j\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def get(self):\n",
    "        pass\n",
    "    def len(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "990d0346-4204-4edd-a947-de80e85a685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name_bert = 'molberto_ecfp0_2M'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_bert)\n",
    "dataset = MoleculeDataset(dataframe, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef882144-9701-4c8e-b169-0e11113172b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "num_train = len(dataset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = int(np.floor(config['dataset']['valid_size'] * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset, batch_size=config['batch_size'], sampler=train_sampler,\n",
    "    num_workers=config['dataset']['num_workers'], drop_last=True\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    dataset, batch_size=config['batch_size'], sampler=valid_sampler,\n",
    "    num_workers=config['dataset']['num_workers'], drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a56144-3102-4a21-b86b-8e851dc97026",
   "metadata": {},
   "source": [
    "### Create Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66254640-38dc-4da6-bccf-634174aca108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "if config['model_type'] == 'gin':\n",
    "    from MolCLR.models.ginet_molclr import GINet as GraphModel\n",
    "elif config['model_type'] == 'gcn':\n",
    "    from MolCLR.models.gcn_molclr import GCN as GraphModel\n",
    "else:\n",
    "    raise ValueError('GNN model is not defined in config.')\n",
    "from MolCLR.utils.nt_xent import NTXentLoss\n",
    "\n",
    "class MolecularBertGraph(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MolecularBertGraph, self).__init__()\n",
    "        self.batch_size = config['batch_size']\n",
    "\n",
    "        roberta_config = RobertaConfig(\n",
    "            vocab_size=30_522,\n",
    "            max_position_embeddings=514,\n",
    "            hidden_size=768,\n",
    "            num_attention_heads=12,\n",
    "            num_hidden_layers=6,\n",
    "            type_vocab_size=1\n",
    "        )\n",
    "        self.bert = RobertaForMaskedLM(roberta_config)\n",
    "        \n",
    "        self.graph_model = GraphModel(**config['model'])\n",
    "        # self.graph_model = self._load_pre_trained_weights(self.graph_model)\n",
    "\n",
    "        self.out_graph_linear = torch.nn.Linear(768 * 2, 768, bias=True)\n",
    "\n",
    "        # contrastive loss for MolCLR\n",
    "        self.nt_xent_criterion = NTXentLoss(device, self.batch_size, **config['loss'])\n",
    "        # cosine distance as loss between models\n",
    "        self.cosine_sim = torch.nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, bert_batch, graph_batch1, graph_batch2):\n",
    "        bert_output = self.bert(input_ids=bert_batch['input_ids'].view(self.batch_size, -1), \n",
    "                                 attention_mask=bert_batch['attention_mask'].view(self.batch_size, -1),\n",
    "                                 labels=bert_batch['labels'].view(self.batch_size, -1), output_hidden_states=True)\n",
    "        bert_loss = bert_output.loss\n",
    "        bert_emb = bert_output.hidden_states[0][:, 0, :] # take emb for CLS token\n",
    "\n",
    "        graph_loss, hidden_states_1, hidden_states_2 = self.graph_step(graph_batch1, graph_batch2)\n",
    "        graph_emb = self.out_graph_linear(torch.cat((hidden_states_1, hidden_states_2), dim=-1))\n",
    "\n",
    "        bimodal_loss = ((1 - self.cosine_sim(bert_emb, graph_emb))**2).mean()\n",
    "        return bert_loss, graph_loss, bimodal_loss\n",
    "\n",
    "    def graph_step(self, xis, xjs):\n",
    "        # get the representations and the projections\n",
    "        ris, zis = self.graph_model(xis)  # [N,C]\n",
    "    \n",
    "        # get the representations and the projections\n",
    "        rjs, zjs = self.graph_model(xjs)  # [N,C]\n",
    "    \n",
    "        # normalize projection feature vectors\n",
    "        zis = torch.nn.functional.normalize(zis, dim=1)\n",
    "        zjs = torch.nn.functional.normalize(zjs, dim=1)\n",
    "    \n",
    "        loss = self.nt_xent_criterion(zis, zjs)\n",
    "        return loss, ris, rjs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08055762-1a53-49e6-8cd7-71161852abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MolecularBertGraph().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b0c73-a897-407e-a8c7-158283f97de4",
   "metadata": {},
   "source": [
    "### Define utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04001413-80f4-4663-ab5d-a107e839c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = config['epochs']\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), config['init_lr'], \n",
    "    weight_decay=eval(config['weight_decay'])\n",
    ")\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "#     optimizer, T_max=config['epochs']-config['warm_up'], \n",
    "#     eta_min=0, last_epoch=-1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54ef89d4-b7ac-4a86-ab14-67383fd0f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33morlov-aleksei53\u001b[0m (\u001b[33mmoleculary-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alexeyorlov53/Transformers-for-Molecules/bert+MolCLR/wandb/run-20240528_130020-63kpwmt8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/moleculary-ai/efcp_transformer/runs/63kpwmt8' target=\"_blank\">RobertaForMaskedLM + MolCLR (GCN) equal_coeffs</a></strong> to <a href='https://wandb.ai/moleculary-ai/efcp_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/moleculary-ai/efcp_transformer' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/moleculary-ai/efcp_transformer/runs/63kpwmt8' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer/runs/63kpwmt8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/moleculary-ai/efcp_transformer/runs/63kpwmt8?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f3e4d4cd310>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"efcp_transformer\",\n",
    "    name=\"RobertaForMaskedLM + MolCLR (GCN) equal_coeffs\",\n",
    "    config={}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a951a1-7c75-47bd-b275-9693eb22fe95",
   "metadata": {},
   "source": [
    "### Training (with validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f660cb2-97a3-4102-b85e-c6e23c84429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = config['loss_params']['alpha']\n",
    "beta = config['loss_params']['beta']\n",
    "gamma = config['loss_params']['gamma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5ea2e1e-8d54-4f03-914f-6b6328b5da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96388fce-0dc6-43a3-b800-5133ea476cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    train_tqdm = tqdm(train_dataloader, unit=\"batch\")\n",
    "    train_tqdm.set_description(f'Epoch {epoch_counter}')\n",
    "    bert_loss_sum, graph_model_loss_sum, bimodal_loss_sum, loss_sum = 0, 0, 0, 0\n",
    "    \n",
    "    model.train()\n",
    "    for (bert_batch, graph_batch1, graph_batch2) in train_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        bert_batch = bert_batch.to(device)\n",
    "        graph_batch1 = graph_batch1.to(device)\n",
    "        graph_batch2 = graph_batch2.to(device)\n",
    "\n",
    "        bert_loss, graph_loss, bimodal_loss = model(bert_batch, graph_batch1, graph_batch2)\n",
    "\n",
    "        loss = alpha * bert_loss + beta * graph_loss + gamma * bimodal_loss\n",
    "        loss.backward()\n",
    "\n",
    "        bert_loss_sum += bert_loss.item()\n",
    "        graph_model_loss_sum += graph_loss.item()\n",
    "        bimodal_loss_sum += bimodal_loss.item()\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_tqdm.set_postfix(loss=loss.item(), bert_loss=bert_loss.item(), graph_loss=graph_loss.item(), bimodal_loss=bimodal_loss.item())\n",
    "    return bert_loss_sum / len(train_dataloader), graph_model_loss_sum / len(train_dataloader), bimodal_loss_sum / len(train_dataloader), loss_sum / len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7af92a73-77ad-4bd7-9727-7bb5337fd001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop():\n",
    "    eval_tqdm = tqdm(eval_dataloader, unit=\"batch\")\n",
    "    eval_tqdm.set_description(f'Epoch {epoch_counter}')\n",
    "    bert_loss_sum, graph_model_loss_sum, bimodal_loss_sum, loss_sum = 0, 0, 0, 0\n",
    "    \n",
    "    model.eval()\n",
    "    for (bert_batch, graph_batch1, graph_batch2) in eval_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        bert_batch = bert_batch.to(device)\n",
    "        graph_batch1 = graph_batch1.to(device)\n",
    "        graph_batch2 = graph_batch2.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bert_loss, graph_loss, bimodal_loss = model(bert_batch, graph_batch1, graph_batch2)\n",
    "\n",
    "        loss = alpha * bert_loss + beta * graph_loss + gamma * bimodal_loss\n",
    "\n",
    "        bert_loss_sum += bert_loss.item()\n",
    "        graph_model_loss_sum += graph_loss.item()\n",
    "        bimodal_loss_sum += bimodal_loss.item()\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        eval_tqdm.set_postfix(loss=loss.item(), bert_loss=bert_loss.item(), graph_loss=graph_loss.item(), bimodal_loss=bimodal_loss.item())\n",
    "    return bert_loss_sum / len(eval_dataloader), graph_model_loss_sum / len(eval_dataloader), bimodal_loss_sum / len(eval_dataloader), loss_sum / len(eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6bb042f-7934-4e54-b202-fe736e4396d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|███████████████████████████| 31/31 [00:08<00:00,  3.67batch/s, bert_loss=10.5, bimodal_loss=1.07, graph_loss=4.14, loss=15.7]\n"
     ]
    }
   ],
   "source": [
    "bert_loss, graph_loss, bimodal_loss, loss = eval_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60dc9568-bd68-4dbd-a797-d756f697c73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_loss = 10.479248785203502\n",
      "graph_loss =  4.141095822857272\n",
      "bimodal_loss = 1.068702132471146\n",
      "sum of losses = 15.689046705922772\n"
     ]
    }
   ],
   "source": [
    "print('bert_loss =', bert_loss)\n",
    "print('graph_loss = ', graph_loss)\n",
    "print('bimodal_loss =', bimodal_loss)\n",
    "print('sum of losses =', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dabd36a8-610d-4760-a984-79bf17e5b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "model_checkpoints_folder = os.path.join('ckpts')\n",
    "dir_name = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "log_dir = os.path.join(model_checkpoints_folder, dir_name)\n",
    "_save_config_file(config, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7a426df-461e-4180-9186-c5471b4df374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.403, bimodal_loss=0.00326, graph_loss=1.43, loss=1.83]\n",
      "Epoch 0: 100%|███████████████████████| 31/31 [00:07<00:00,  4.28batch/s, bert_loss=0.374, bimodal_loss=0.000325, graph_loss=1.33, loss=1.7]\n",
      "Epoch 1: 100%|█████████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.356, bimodal_loss=0.00301, graph_loss=1.01, loss=1.37]\n",
      "Epoch 1: 100%|███████████████████████| 31/31 [00:07<00:00,  4.32batch/s, bert_loss=0.366, bimodal_loss=9.13e-6, graph_loss=1.14, loss=1.51]\n",
      "Epoch 2: 100%|█████████████████████| 281/281 [02:40<00:00,  1.76batch/s, bert_loss=0.383, bimodal_loss=0.00281, graph_loss=1.42, loss=1.81]\n",
      "Epoch 2: 100%|███████████████████████| 31/31 [00:07<00:00,  4.17batch/s, bert_loss=0.402, bimodal_loss=6.76e-6, graph_loss=1.44, loss=1.84]\n",
      "Epoch 3: 100%|████████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.396, bimodal_loss=0.00269, graph_loss=0.561, loss=0.96]\n",
      "Epoch 3: 100%|██████████████████████| 31/31 [00:07<00:00,  4.01batch/s, bert_loss=0.409, bimodal_loss=7.83e-7, graph_loss=0.973, loss=1.38]\n",
      "Epoch 4: 100%|█████████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.376, bimodal_loss=0.00266, graph_loss=0.725, loss=1.1]\n",
      "Epoch 4: 100%|██████████████████████| 31/31 [00:07<00:00,  4.19batch/s, bert_loss=0.385, bimodal_loss=2.15e-7, graph_loss=0.649, loss=1.03]\n",
      "Epoch 5: 100%|███████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.387, bimodal_loss=0.00259, graph_loss=0.484, loss=0.873]\n",
      "Epoch 5: 100%|██████████████████████| 31/31 [00:07<00:00,  4.14batch/s, bert_loss=0.354, bimodal_loss=2.95e-7, graph_loss=0.721, loss=1.07]\n",
      "Epoch 6: 100%|███████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.387, bimodal_loss=0.00271, graph_loss=0.493, loss=0.883]\n",
      "Epoch 6: 100%|██████████████████████| 31/31 [00:07<00:00,  4.06batch/s, bert_loss=0.353, bimodal_loss=1.56e-7, graph_loss=0.705, loss=1.06]\n",
      "Epoch 7: 100%|████████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.379, bimodal_loss=0.00287, graph_loss=0.61, loss=0.992]\n",
      "Epoch 7: 100%|██████████████████████| 31/31 [00:07<00:00,  4.19batch/s, bert_loss=0.386, bimodal_loss=2.85e-7, graph_loss=0.354, loss=0.74]\n",
      "Epoch 8: 100%|███████████████████| 281/281 [02:41<00:00,  1.74batch/s, bert_loss=0.402, bimodal_loss=0.00278, graph_loss=0.497, loss=0.902]\n",
      "Epoch 8: 100%|██████████████████████| 31/31 [00:07<00:00,  4.00batch/s, bert_loss=0.374, bimodal_loss=3.2e-7, graph_loss=0.414, loss=0.788]\n",
      "Epoch 9: 100%|███████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.395, bimodal_loss=0.00268, graph_loss=0.537, loss=0.935]\n",
      "Epoch 9: 100%|██████████████████████| 31/31 [00:07<00:00,  3.88batch/s, bert_loss=0.41, bimodal_loss=3.23e-7, graph_loss=0.503, loss=0.913]\n",
      "Epoch 10: 100%|███████████████████| 281/281 [02:41<00:00,  1.74batch/s, bert_loss=0.394, bimodal_loss=0.00255, graph_loss=0.664, loss=1.06]\n",
      "Epoch 10: 100%|█████████████████████| 31/31 [00:08<00:00,  3.84batch/s, bert_loss=0.365, bimodal_loss=4.48e-7, graph_loss=0.657, loss=1.02]\n",
      "Epoch 11: 100%|██████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.402, bimodal_loss=0.00277, graph_loss=0.501, loss=0.906]\n",
      "Epoch 11: 100%|████████████████████| 31/31 [00:07<00:00,  4.11batch/s, bert_loss=0.403, bimodal_loss=8.34e-7, graph_loss=0.591, loss=0.994]\n",
      "Epoch 12: 100%|██████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.384, bimodal_loss=0.00257, graph_loss=0.358, loss=0.745]\n",
      "Epoch 12: 100%|█████████████████████| 31/31 [00:07<00:00,  3.89batch/s, bert_loss=0.385, bimodal_loss=1.5e-6, graph_loss=0.504, loss=0.889]\n",
      "Epoch 13: 100%|███████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.374, bimodal_loss=0.00274, graph_loss=0.41, loss=0.787]\n",
      "Epoch 13: 100%|████████████████████| 31/31 [00:07<00:00,  3.92batch/s, bert_loss=0.371, bimodal_loss=9.31e-7, graph_loss=0.433, loss=0.804]\n",
      "Epoch 14: 100%|████████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.383, bimodal_loss=0.0028, graph_loss=0.35, loss=0.736]\n",
      "Epoch 14: 100%|█████████████████████| 31/31 [00:07<00:00,  3.90batch/s, bert_loss=0.549, bimodal_loss=2.16e-6, graph_loss=0.501, loss=1.05]\n",
      "Epoch 15: 100%|██████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.387, bimodal_loss=0.00255, graph_loss=0.238, loss=0.628]\n",
      "Epoch 15: 100%|████████████████████| 31/31 [00:07<00:00,  4.07batch/s, bert_loss=0.378, bimodal_loss=9.99e-7, graph_loss=0.488, loss=0.866]\n",
      "Epoch 16: 100%|██████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.382, bimodal_loss=0.00269, graph_loss=0.322, loss=0.706]\n",
      "Epoch 16: 100%|█████████████████████| 31/31 [00:07<00:00,  4.03batch/s, bert_loss=0.364, bimodal_loss=1.91e-6, graph_loss=0.395, loss=0.76]\n",
      "Epoch 17: 100%|███████████████████| 281/281 [02:41<00:00,  1.74batch/s, bert_loss=0.376, bimodal_loss=0.0027, graph_loss=0.464, loss=0.843]\n",
      "Epoch 17: 100%|████████████████████| 31/31 [00:07<00:00,  4.07batch/s, bert_loss=0.369, bimodal_loss=4.52e-6, graph_loss=0.279, loss=0.648]\n",
      "Epoch 18: 100%|███████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.39, bimodal_loss=0.00294, graph_loss=0.387, loss=0.781]\n",
      "Epoch 18: 100%|████████████████████████| 31/31 [00:07<00:00,  3.96batch/s, bert_loss=0.392, bimodal_loss=2e-6, graph_loss=0.29, loss=0.681]\n",
      "Epoch 19: 100%|██████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.374, bimodal_loss=0.00294, graph_loss=0.377, loss=0.754]\n",
      "Epoch 19: 100%|████████████████████| 31/31 [00:07<00:00,  4.00batch/s, bert_loss=0.396, bimodal_loss=3.28e-6, graph_loss=0.427, loss=0.823]\n",
      "Epoch 20: 100%|███████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.358, bimodal_loss=0.00296, graph_loss=0.671, loss=1.03]\n",
      "Epoch 20: 100%|█████████████████████| 31/31 [00:07<00:00,  4.07batch/s, bert_loss=0.378, bimodal_loss=1.19e-6, graph_loss=0.53, loss=0.908]\n",
      "Epoch 21: 100%|████████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.374, bimodal_loss=0.0027, graph_loss=0.22, loss=0.597]\n",
      "Epoch 21: 100%|████████████████████| 31/31 [00:07<00:00,  4.12batch/s, bert_loss=0.399, bimodal_loss=2.81e-6, graph_loss=0.253, loss=0.652]\n",
      "Epoch 22: 100%|██████████████████| 281/281 [02:47<00:00,  1.68batch/s, bert_loss=0.384, bimodal_loss=0.00293, graph_loss=0.424, loss=0.811]\n",
      "Epoch 22: 100%|██████████████████████| 31/31 [00:08<00:00,  3.71batch/s, bert_loss=0.404, bimodal_loss=2.3e-6, graph_loss=0.33, loss=0.734]\n",
      "Epoch 23: 100%|██████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.403, bimodal_loss=0.00267, graph_loss=0.251, loss=0.656]\n",
      "Epoch 23: 100%|████████████████████| 31/31 [00:08<00:00,  3.77batch/s, bert_loss=0.385, bimodal_loss=4.79e-6, graph_loss=0.244, loss=0.629]\n",
      "Epoch 24: 100%|████████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.398, bimodal_loss=0.00286, graph_loss=0.36, loss=0.76]\n",
      "Epoch 24: 100%|█████████████████████| 31/31 [00:07<00:00,  3.96batch/s, bert_loss=0.418, bimodal_loss=2.26e-6, graph_loss=0.34, loss=0.758]\n",
      "Epoch 25: 100%|██████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.387, bimodal_loss=0.00318, graph_loss=0.328, loss=0.719]\n",
      "Epoch 25: 100%|████████████████████| 31/31 [00:07<00:00,  3.89batch/s, bert_loss=0.412, bimodal_loss=4.66e-6, graph_loss=0.373, loss=0.785]\n",
      "Epoch 26: 100%|███████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.375, bimodal_loss=0.00302, graph_loss=0.21, loss=0.588]\n",
      "Epoch 26: 100%|████████████████████| 31/31 [00:07<00:00,  4.06batch/s, bert_loss=0.355, bimodal_loss=1.01e-5, graph_loss=0.492, loss=0.847]\n",
      "Epoch 27: 100%|██████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.392, bimodal_loss=0.00285, graph_loss=0.195, loss=0.589]\n",
      "Epoch 27: 100%|████████████████████| 31/31 [00:07<00:00,  4.03batch/s, bert_loss=0.423, bimodal_loss=2.78e-6, graph_loss=0.172, loss=0.595]\n",
      "Epoch 28: 100%|██████████████████| 281/281 [02:41<00:00,  1.74batch/s, bert_loss=0.486, bimodal_loss=0.00298, graph_loss=0.358, loss=0.847]\n",
      "Epoch 28: 100%|██████████████████████| 31/31 [00:07<00:00,  3.94batch/s, bert_loss=0.365, bimodal_loss=3.4e-6, graph_loss=0.205, loss=0.57]\n",
      "Epoch 29: 100%|███████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.379, bimodal_loss=0.0026, graph_loss=0.247, loss=0.629]\n",
      "Epoch 29: 100%|████████████████████| 31/31 [00:07<00:00,  3.96batch/s, bert_loss=0.428, bimodal_loss=4.42e-6, graph_loss=0.513, loss=0.941]\n",
      "Epoch 30: 100%|█████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.423, bimodal_loss=0.00278, graph_loss=0.0902, loss=0.516]\n",
      "Epoch 30: 100%|████████████████████| 31/31 [00:07<00:00,  4.02batch/s, bert_loss=0.376, bimodal_loss=2.52e-6, graph_loss=0.182, loss=0.558]\n",
      "Epoch 31: 100%|████████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.41, bimodal_loss=0.0028, graph_loss=0.171, loss=0.583]\n",
      "Epoch 31: 100%|█████████████████████| 31/31 [00:07<00:00,  4.08batch/s, bert_loss=0.338, bimodal_loss=2.29e-6, graph_loss=0.13, loss=0.469]\n",
      "Epoch 32: 100%|██████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.371, bimodal_loss=0.00302, graph_loss=0.234, loss=0.607]\n",
      "Epoch 32: 100%|████████████████████| 31/31 [00:07<00:00,  4.13batch/s, bert_loss=0.375, bimodal_loss=3.94e-6, graph_loss=0.241, loss=0.616]\n",
      "Epoch 33: 100%|██████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.433, bimodal_loss=0.00278, graph_loss=0.339, loss=0.775]\n",
      "Epoch 33: 100%|████████████████████| 31/31 [00:07<00:00,  4.11batch/s, bert_loss=0.396, bimodal_loss=3.95e-6, graph_loss=0.215, loss=0.611]\n",
      "Epoch 34: 100%|███████████████████| 281/281 [02:41<00:00,  1.74batch/s, bert_loss=0.374, bimodal_loss=0.00279, graph_loss=0.13, loss=0.507]\n",
      "Epoch 34: 100%|████████████████████| 31/31 [00:07<00:00,  4.04batch/s, bert_loss=0.389, bimodal_loss=2.71e-6, graph_loss=0.377, loss=0.766]\n",
      "Epoch 35: 100%|██████████████████| 281/281 [02:41<00:00,  1.74batch/s, bert_loss=0.374, bimodal_loss=0.00298, graph_loss=0.204, loss=0.581]\n",
      "Epoch 35: 100%|█████████████████████| 31/31 [00:07<00:00,  4.06batch/s, bert_loss=0.39, bimodal_loss=6.08e-6, graph_loss=0.315, loss=0.706]\n",
      "Epoch 36: 100%|██████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.457, bimodal_loss=0.00267, graph_loss=0.281, loss=0.741]\n",
      "Epoch 36: 100%|█████████████████████| 31/31 [00:07<00:00,  4.02batch/s, bert_loss=0.363, bimodal_loss=4.62e-6, graph_loss=0.27, loss=0.632]\n",
      "Epoch 37: 100%|██████████████████| 281/281 [02:41<00:00,  1.75batch/s, bert_loss=0.375, bimodal_loss=0.00311, graph_loss=0.136, loss=0.515]\n",
      "Epoch 37: 100%|████████████████████| 31/31 [00:07<00:00,  3.94batch/s, bert_loss=0.369, bimodal_loss=2.45e-6, graph_loss=0.311, loss=0.681]\n",
      "Epoch 38: 100%|███████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.421, bimodal_loss=0.0029, graph_loss=0.175, loss=0.599]\n",
      "Epoch 38: 100%|████████████████████| 31/31 [00:07<00:00,  3.96batch/s, bert_loss=0.419, bimodal_loss=7.64e-6, graph_loss=0.303, loss=0.722]\n",
      "Epoch 39: 100%|██████████████████| 281/281 [02:41<00:00,  1.74batch/s, bert_loss=0.392, bimodal_loss=0.00314, graph_loss=0.216, loss=0.611]\n",
      "Epoch 39: 100%|████████████████████| 31/31 [00:07<00:00,  3.89batch/s, bert_loss=0.386, bimodal_loss=4.28e-6, graph_loss=0.182, loss=0.568]\n",
      "Epoch 40: 100%|████████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.378, bimodal_loss=0.00273, graph_loss=0.24, loss=0.62]\n",
      "Epoch 40: 100%|████████████████████████| 31/31 [00:07<00:00,  3.96batch/s, bert_loss=0.45, bimodal_loss=1.6e-6, graph_loss=0.26, loss=0.71]\n",
      "Epoch 41: 100%|███████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.385, bimodal_loss=0.0028, graph_loss=0.383, loss=0.771]\n",
      "Epoch 41: 100%|████████████████████| 31/31 [00:07<00:00,  4.08batch/s, bert_loss=0.374, bimodal_loss=2.94e-6, graph_loss=0.399, loss=0.773]\n",
      "Epoch 42: 100%|███████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.373, bimodal_loss=0.0029, graph_loss=0.126, loss=0.502]\n",
      "Epoch 42: 100%|████████████████████| 31/31 [00:07<00:00,  4.00batch/s, bert_loss=0.386, bimodal_loss=4.22e-6, graph_loss=0.153, loss=0.539]\n",
      "Epoch 43: 100%|██████████████████| 281/281 [02:39<00:00,  1.77batch/s, bert_loss=0.392, bimodal_loss=0.00279, graph_loss=0.324, loss=0.719]\n",
      "Epoch 43: 100%|████████████████████| 31/31 [00:07<00:00,  3.99batch/s, bert_loss=0.367, bimodal_loss=4.38e-6, graph_loss=0.177, loss=0.544]\n",
      "Epoch 44: 100%|███████████████████| 281/281 [02:38<00:00,  1.77batch/s, bert_loss=0.363, bimodal_loss=0.00298, graph_loss=0.105, loss=0.47]\n",
      "Epoch 44: 100%|███████████████████████| 31/31 [00:07<00:00,  4.03batch/s, bert_loss=0.364, bimodal_loss=5e-6, graph_loss=0.211, loss=0.575]\n",
      "Epoch 45: 100%|███████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.463, bimodal_loss=0.00272, graph_loss=0.324, loss=0.79]\n",
      "Epoch 45: 100%|████████████████████| 31/31 [00:07<00:00,  3.92batch/s, bert_loss=0.368, bimodal_loss=4.27e-6, graph_loss=0.114, loss=0.482]\n",
      "Epoch 46: 100%|██████████████████| 281/281 [02:39<00:00,  1.77batch/s, bert_loss=0.392, bimodal_loss=0.00278, graph_loss=0.146, loss=0.541]\n",
      "Epoch 46: 100%|████████████████████| 31/31 [00:07<00:00,  4.08batch/s, bert_loss=0.361, bimodal_loss=4.18e-6, graph_loss=0.202, loss=0.563]\n",
      "Epoch 47: 100%|██████████████████| 281/281 [02:38<00:00,  1.77batch/s, bert_loss=0.426, bimodal_loss=0.00312, graph_loss=0.259, loss=0.688]\n",
      "Epoch 47: 100%|█████████████████████| 31/31 [00:07<00:00,  4.04batch/s, bert_loss=0.428, bimodal_loss=3.7e-6, graph_loss=0.224, loss=0.652]\n",
      "Epoch 48: 100%|██████████████████| 281/281 [02:38<00:00,  1.77batch/s, bert_loss=0.372, bimodal_loss=0.00259, graph_loss=0.181, loss=0.555]\n",
      "Epoch 48: 100%|████████████████████| 31/31 [00:07<00:00,  4.16batch/s, bert_loss=0.575, bimodal_loss=3.88e-6, graph_loss=0.213, loss=0.788]\n",
      "Epoch 49: 100%|████████████████████| 281/281 [02:38<00:00,  1.77batch/s, bert_loss=0.362, bimodal_loss=0.00264, graph_loss=0.235, loss=0.6]\n",
      "Epoch 49: 100%|█████████████████████| 31/31 [00:07<00:00,  4.04batch/s, bert_loss=0.388, bimodal_loss=4.4e-6, graph_loss=0.144, loss=0.532]\n",
      "Epoch 50: 100%|██████████████████| 281/281 [02:46<00:00,  1.69batch/s, bert_loss=0.371, bimodal_loss=0.00312, graph_loss=0.115, loss=0.489]\n",
      "Epoch 50: 100%|████████████████████| 31/31 [00:07<00:00,  4.11batch/s, bert_loss=0.389, bimodal_loss=1.52e-6, graph_loss=0.204, loss=0.593]\n",
      "Epoch 51: 100%|███████████████████| 281/281 [02:38<00:00,  1.77batch/s, bert_loss=0.385, bimodal_loss=0.0029, graph_loss=0.163, loss=0.551]\n",
      "Epoch 51: 100%|█████████████████████| 31/31 [00:07<00:00,  4.36batch/s, bert_loss=0.395, bimodal_loss=2.83e-5, graph_loss=0.19, loss=0.585]\n",
      "Epoch 52: 100%|██████████████████| 281/281 [02:37<00:00,  1.78batch/s, bert_loss=0.358, bimodal_loss=0.00291, graph_loss=0.225, loss=0.586]\n",
      "Epoch 52: 100%|█████████████████████| 31/31 [00:07<00:00,  4.30batch/s, bert_loss=0.381, bimodal_loss=6.45e-6, graph_loss=0.18, loss=0.561]\n",
      "Epoch 53: 100%|██████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.412, bimodal_loss=0.00283, graph_loss=0.224, loss=0.638]\n",
      "Epoch 53: 100%|█████████████████████| 31/31 [00:07<00:00,  4.37batch/s, bert_loss=0.354, bimodal_loss=8.72e-6, graph_loss=0.28, loss=0.634]\n",
      "Epoch 54: 100%|████████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.4, bimodal_loss=0.00281, graph_loss=0.104, loss=0.506]\n",
      "Epoch 54: 100%|████████████████████| 31/31 [00:07<00:00,  4.26batch/s, bert_loss=0.371, bimodal_loss=6.56e-6, graph_loss=0.211, loss=0.582]\n",
      "Epoch 55: 100%|███████████████████| 281/281 [02:38<00:00,  1.77batch/s, bert_loss=0.344, bimodal_loss=0.00271, graph_loss=0.13, loss=0.477]\n",
      "Epoch 55: 100%|█████████████████████| 31/31 [00:07<00:00,  4.16batch/s, bert_loss=0.436, bimodal_loss=4.19e-6, graph_loss=0.16, loss=0.596]\n",
      "Epoch 56: 100%|██████████████████| 281/281 [02:38<00:00,  1.77batch/s, bert_loss=0.376, bimodal_loss=0.00275, graph_loss=0.154, loss=0.533]\n",
      "Epoch 56: 100%|████████████████████| 31/31 [00:07<00:00,  4.37batch/s, bert_loss=0.376, bimodal_loss=7.5e-5, graph_loss=0.0776, loss=0.453]\n",
      "Epoch 57: 100%|██████████████████| 281/281 [02:38<00:00,  1.77batch/s, bert_loss=0.395, bimodal_loss=0.00277, graph_loss=0.156, loss=0.553]\n",
      "Epoch 57: 100%|████████████████████| 31/31 [00:07<00:00,  4.35batch/s, bert_loss=0.403, bimodal_loss=4.01e-6, graph_loss=0.191, loss=0.594]\n",
      "Epoch 58: 100%|███████████████████| 281/281 [02:38<00:00,  1.77batch/s, bert_loss=0.376, bimodal_loss=0.0031, graph_loss=0.182, loss=0.561]\n",
      "Epoch 58: 100%|████████████████████| 31/31 [00:07<00:00,  4.26batch/s, bert_loss=0.475, bimodal_loss=1.03e-5, graph_loss=0.184, loss=0.659]\n",
      "Epoch 59: 100%|███████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.36, bimodal_loss=0.00264, graph_loss=0.179, loss=0.542]\n",
      "Epoch 59: 100%|████████████████████| 31/31 [00:07<00:00,  4.22batch/s, bert_loss=0.366, bimodal_loss=1.16e-5, graph_loss=0.395, loss=0.762]\n",
      "Epoch 60: 100%|██████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.394, bimodal_loss=0.00304, graph_loss=0.141, loss=0.539]\n",
      "Epoch 60: 100%|███████████████████| 31/31 [00:07<00:00,  4.23batch/s, bert_loss=0.382, bimodal_loss=5.54e-6, graph_loss=0.0716, loss=0.453]\n",
      "Epoch 61: 100%|██████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.401, bimodal_loss=0.00294, graph_loss=0.122, loss=0.525]\n",
      "Epoch 61: 100%|████████████████████| 31/31 [00:07<00:00,  4.22batch/s, bert_loss=0.361, bimodal_loss=1.57e-6, graph_loss=0.478, loss=0.839]\n",
      "Epoch 62: 100%|██████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.364, bimodal_loss=0.00274, graph_loss=0.143, loss=0.509]\n",
      "Epoch 62: 100%|█████████████████████| 31/31 [00:07<00:00,  4.02batch/s, bert_loss=0.391, bimodal_loss=1.85e-6, graph_loss=0.279, loss=0.67]\n",
      "Epoch 63: 100%|██████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.369, bimodal_loss=0.00308, graph_loss=0.142, loss=0.514]\n",
      "Epoch 63: 100%|████████████████████| 31/31 [00:07<00:00,  4.23batch/s, bert_loss=0.368, bimodal_loss=2.82e-5, graph_loss=0.148, loss=0.516]\n",
      "Epoch 64: 100%|██████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.369, bimodal_loss=0.00295, graph_loss=0.182, loss=0.554]\n",
      "Epoch 64: 100%|█████████████████████| 31/31 [00:07<00:00,  4.07batch/s, bert_loss=0.375, bimodal_loss=3.3e-6, graph_loss=0.194, loss=0.569]\n",
      "Epoch 65: 100%|███████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.383, bimodal_loss=0.0029, graph_loss=0.167, loss=0.553]\n",
      "Epoch 65: 100%|█████████████████████| 31/31 [00:07<00:00,  4.07batch/s, bert_loss=0.422, bimodal_loss=5.42e-6, graph_loss=0.258, loss=0.68]\n",
      "Epoch 66: 100%|███████████████████| 281/281 [02:39<00:00,  1.77batch/s, bert_loss=0.371, bimodal_loss=0.00277, graph_loss=0.116, loss=0.49]\n",
      "Epoch 66: 100%|████████████████████| 31/31 [00:07<00:00,  4.12batch/s, bert_loss=0.433, bimodal_loss=2.87e-6, graph_loss=0.184, loss=0.617]\n",
      "Epoch 67: 100%|█████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.403, bimodal_loss=0.00334, graph_loss=0.0703, loss=0.476]\n",
      "Epoch 67: 100%|████████████████████| 31/31 [00:07<00:00,  3.96batch/s, bert_loss=0.427, bimodal_loss=8.62e-6, graph_loss=0.162, loss=0.589]\n",
      "Epoch 68: 100%|█████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.398, bimodal_loss=0.00288, graph_loss=0.0577, loss=0.458]\n",
      "Epoch 68: 100%|████████████████████| 31/31 [00:08<00:00,  3.86batch/s, bert_loss=0.406, bimodal_loss=3.74e-6, graph_loss=0.127, loss=0.533]\n",
      "Epoch 69: 100%|███████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.436, bimodal_loss=0.00285, graph_loss=0.141, loss=0.58]\n",
      "Epoch 69: 100%|█████████████████████| 31/31 [00:07<00:00,  3.99batch/s, bert_loss=0.381, bimodal_loss=7.22e-6, graph_loss=0.24, loss=0.621]\n",
      "Epoch 70: 100%|████████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.385, bimodal_loss=0.0027, graph_loss=0.12, loss=0.508]\n",
      "Epoch 70: 100%|████████████████████| 31/31 [00:08<00:00,  3.86batch/s, bert_loss=0.371, bimodal_loss=4.37e-6, graph_loss=0.241, loss=0.612]\n",
      "Epoch 71: 100%|███████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.401, bimodal_loss=0.00272, graph_loss=0.15, loss=0.554]\n",
      "Epoch 71: 100%|████████████████████| 31/31 [00:07<00:00,  3.96batch/s, bert_loss=0.411, bimodal_loss=4.97e-6, graph_loss=0.336, loss=0.746]\n",
      "Epoch 72: 100%|███████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.37, bimodal_loss=0.00321, graph_loss=0.149, loss=0.522]\n",
      "Epoch 72: 100%|█████████████████████| 31/31 [00:07<00:00,  4.03batch/s, bert_loss=0.354, bimodal_loss=6.2e-6, graph_loss=0.141, loss=0.494]\n",
      "Epoch 73: 100%|██████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.439, bimodal_loss=0.00287, graph_loss=0.211, loss=0.653]\n",
      "Epoch 73: 100%|████████████████████| 31/31 [00:07<00:00,  4.05batch/s, bert_loss=0.407, bimodal_loss=2.19e-6, graph_loss=0.141, loss=0.548]\n",
      "Epoch 74: 100%|█████████████████| 281/281 [02:39<00:00,  1.77batch/s, bert_loss=0.389, bimodal_loss=0.00265, graph_loss=0.0755, loss=0.467]\n",
      "Epoch 74: 100%|████████████████████| 31/31 [00:07<00:00,  4.04batch/s, bert_loss=0.361, bimodal_loss=3.31e-6, graph_loss=0.257, loss=0.619]\n",
      "Epoch 75: 100%|██████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.399, bimodal_loss=0.00262, graph_loss=0.147, loss=0.549]\n",
      "Epoch 75: 100%|█████████████████████| 31/31 [00:08<00:00,  3.82batch/s, bert_loss=0.414, bimodal_loss=3.26e-6, graph_loss=0.17, loss=0.584]\n",
      "Epoch 76: 100%|██████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.468, bimodal_loss=0.00287, graph_loss=0.277, loss=0.748]\n",
      "Epoch 76: 100%|████████████████████| 31/31 [00:07<00:00,  3.94batch/s, bert_loss=0.382, bimodal_loss=9.06e-6, graph_loss=0.141, loss=0.523]\n",
      "Epoch 77: 100%|███████████████████| 281/281 [02:39<00:00,  1.77batch/s, bert_loss=0.367, bimodal_loss=0.0029, graph_loss=0.171, loss=0.541]\n",
      "Epoch 77: 100%|████████████████████| 31/31 [00:07<00:00,  3.97batch/s, bert_loss=0.425, bimodal_loss=6.61e-6, graph_loss=0.286, loss=0.711]\n",
      "Epoch 78: 100%|██████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.394, bimodal_loss=0.00286, graph_loss=0.108, loss=0.505]\n",
      "Epoch 78: 100%|████████████████████| 31/31 [00:07<00:00,  3.95batch/s, bert_loss=0.368, bimodal_loss=1.88e-6, graph_loss=0.225, loss=0.593]\n",
      "Epoch 79: 100%|███████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.404, bimodal_loss=0.0028, graph_loss=0.0737, loss=0.48]\n",
      "Epoch 79: 100%|████████████████████| 31/31 [00:07<00:00,  4.03batch/s, bert_loss=0.394, bimodal_loss=1.82e-5, graph_loss=0.175, loss=0.569]\n",
      "Epoch 80: 100%|██████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.383, bimodal_loss=0.00277, graph_loss=0.0944, loss=0.48]\n",
      "Epoch 80: 100%|████████████████████| 31/31 [00:07<00:00,  4.02batch/s, bert_loss=0.365, bimodal_loss=3.45e-6, graph_loss=0.122, loss=0.488]\n",
      "Epoch 81: 100%|██████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.361, bimodal_loss=0.00275, graph_loss=0.202, loss=0.566]\n",
      "Epoch 81: 100%|████████████████████| 31/31 [00:07<00:00,  4.04batch/s, bert_loss=0.401, bimodal_loss=7.11e-6, graph_loss=0.187, loss=0.588]\n",
      "Epoch 82: 100%|██████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.391, bimodal_loss=0.00302, graph_loss=0.064, loss=0.458]\n",
      "Epoch 82: 100%|████████████████████| 31/31 [00:07<00:00,  4.09batch/s, bert_loss=0.388, bimodal_loss=1.84e-6, graph_loss=0.249, loss=0.638]\n",
      "Epoch 83: 100%|██████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.381, bimodal_loss=0.00292, graph_loss=0.222, loss=0.606]\n",
      "Epoch 83: 100%|█████████████████████| 31/31 [00:07<00:00,  4.15batch/s, bert_loss=0.457, bimodal_loss=3.3e-6, graph_loss=0.116, loss=0.573]\n",
      "Epoch 84: 100%|██████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.393, bimodal_loss=0.00297, graph_loss=0.107, loss=0.503]\n",
      "Epoch 84: 100%|████████████████████| 31/31 [00:08<00:00,  3.82batch/s, bert_loss=0.416, bimodal_loss=4.78e-6, graph_loss=0.226, loss=0.642]\n",
      "Epoch 85: 100%|███████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.41, bimodal_loss=0.00291, graph_loss=0.119, loss=0.532]\n",
      "Epoch 85: 100%|████████████████████| 31/31 [00:07<00:00,  3.91batch/s, bert_loss=0.394, bimodal_loss=4.49e-6, graph_loss=0.134, loss=0.527]\n",
      "Epoch 86: 100%|███████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.36, bimodal_loss=0.00291, graph_loss=0.181, loss=0.543]\n",
      "Epoch 86: 100%|██████████████████████| 31/31 [00:07<00:00,  3.95batch/s, bert_loss=0.361, bimodal_loss=6.6e-6, graph_loss=0.25, loss=0.611]\n",
      "Epoch 87: 100%|███████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.391, bimodal_loss=0.0027, graph_loss=0.229, loss=0.623]\n",
      "Epoch 87: 100%|█████████████████████| 31/31 [00:07<00:00,  4.04batch/s, bert_loss=0.386, bimodal_loss=2.93e-6, graph_loss=0.204, loss=0.59]\n",
      "Epoch 88: 100%|██████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.346, bimodal_loss=0.00282, graph_loss=0.213, loss=0.562]\n",
      "Epoch 88: 100%|████████████████████| 31/31 [00:07<00:00,  4.07batch/s, bert_loss=0.379, bimodal_loss=3.02e-6, graph_loss=0.252, loss=0.631]\n",
      "Epoch 89: 100%|█████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.405, bimodal_loss=0.00257, graph_loss=0.0614, loss=0.469]\n",
      "Epoch 89: 100%|█████████████████████| 31/31 [00:07<00:00,  4.08batch/s, bert_loss=0.39, bimodal_loss=2.13e-6, graph_loss=0.258, loss=0.648]\n",
      "Epoch 90: 100%|██████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.368, bimodal_loss=0.00292, graph_loss=0.122, loss=0.493]\n",
      "Epoch 90: 100%|████████████████████| 31/31 [00:07<00:00,  4.10batch/s, bert_loss=0.397, bimodal_loss=3.46e-6, graph_loss=0.178, loss=0.575]\n",
      "Epoch 91: 100%|██████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.387, bimodal_loss=0.00295, graph_loss=0.135, loss=0.525]\n",
      "Epoch 91: 100%|████████████████████| 31/31 [00:07<00:00,  4.07batch/s, bert_loss=0.388, bimodal_loss=2.85e-6, graph_loss=0.168, loss=0.556]\n",
      "Epoch 92: 100%|███████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.41, bimodal_loss=0.00268, graph_loss=0.123, loss=0.535]\n",
      "Epoch 92: 100%|████████████████████| 31/31 [00:07<00:00,  3.88batch/s, bert_loss=0.396, bimodal_loss=9.37e-6, graph_loss=0.183, loss=0.579]\n",
      "Epoch 93: 100%|██████████████████| 281/281 [02:39<00:00,  1.77batch/s, bert_loss=0.365, bimodal_loss=0.00297, graph_loss=0.257, loss=0.625]\n",
      "Epoch 93: 100%|█████████████████████| 31/31 [00:07<00:00,  4.06batch/s, bert_loss=0.389, bimodal_loss=1.72e-6, graph_loss=0.17, loss=0.559]\n",
      "Epoch 94: 100%|██████████████████| 281/281 [02:40<00:00,  1.75batch/s, bert_loss=0.377, bimodal_loss=0.00277, graph_loss=0.115, loss=0.494]\n",
      "Epoch 94: 100%|█████████████████████| 31/31 [00:08<00:00,  3.84batch/s, bert_loss=0.424, bimodal_loss=8.7e-6, graph_loss=0.127, loss=0.551]\n",
      "Epoch 95: 100%|████████████████████| 281/281 [02:50<00:00,  1.65batch/s, bert_loss=0.369, bimodal_loss=0.00311, graph_loss=0.127, loss=0.5]\n",
      "Epoch 95: 100%|█████████████████████| 31/31 [00:07<00:00,  4.24batch/s, bert_loss=0.39, bimodal_loss=2.4e-6, graph_loss=0.0659, loss=0.456]\n",
      "Epoch 96: 100%|██████████████████| 281/281 [02:39<00:00,  1.76batch/s, bert_loss=0.381, bimodal_loss=0.00272, graph_loss=0.139, loss=0.523]\n",
      "Epoch 96: 100%|█████████████████████| 31/31 [00:07<00:00,  3.88batch/s, bert_loss=0.421, bimodal_loss=3.45e-6, graph_loss=0.19, loss=0.611]\n",
      "Epoch 97: 100%|███████████████████| 281/281 [02:49<00:00,  1.65batch/s, bert_loss=0.39, bimodal_loss=0.00303, graph_loss=0.126, loss=0.519]\n",
      "Epoch 97: 100%|█████████████████████| 31/31 [00:07<00:00,  3.94batch/s, bert_loss=0.374, bimodal_loss=5.9e-6, graph_loss=0.429, loss=0.803]\n",
      "Epoch 98: 100%|█████████████████| 281/281 [02:46<00:00,  1.69batch/s, bert_loss=0.379, bimodal_loss=0.00308, graph_loss=0.0979, loss=0.479]\n",
      "Epoch 98: 100%|████████████████████| 31/31 [00:08<00:00,  3.83batch/s, bert_loss=0.382, bimodal_loss=7.74e-6, graph_loss=0.115, loss=0.497]\n",
      "Epoch 99: 100%|█████████████████| 281/281 [02:41<00:00,  1.74batch/s, bert_loss=0.369, bimodal_loss=0.00272, graph_loss=0.0944, loss=0.466]\n",
      "Epoch 99: 100%|████████████████████| 31/31 [00:07<00:00,  4.26batch/s, bert_loss=0.469, bimodal_loss=3.88e-6, graph_loss=0.145, loss=0.614]\n"
     ]
    }
   ],
   "source": [
    "n_iter = 0\n",
    "valid_n_iter = 0\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "for epoch_counter in range(num_epoch):\n",
    "    bert_loss, graph_loss, bimodal_loss, loss = train_loop()\n",
    "\n",
    "    wandb.log({\"bert_loss/train\": bert_loss}, step=epoch_counter)\n",
    "    wandb.log({\"graph_loss/train\": graph_loss}, step=epoch_counter)\n",
    "    wandb.log({\"bimodal_loss/train\": bimodal_loss}, step=epoch_counter)\n",
    "    wandb.log({\"loss/train\": loss}, step=epoch_counter)\n",
    "\n",
    "    bert_loss, graph_loss, bimodal_loss, loss = eval_loop()\n",
    "\n",
    "    wandb.log({\"bert_loss/eval\": bert_loss}, step=epoch_counter)\n",
    "    wandb.log({\"graph_loss/eval\": graph_loss}, step=epoch_counter)\n",
    "    wandb.log({\"bimodal_loss/eval\": bimodal_loss}, step=epoch_counter)\n",
    "    wandb.log({\"loss/eval\": loss}, step=epoch_counter)\n",
    "    \n",
    "    if loss < best_valid_loss:\n",
    "        best_valid_loss = loss\n",
    "        torch.save(model.state_dict(), os.path.join(log_dir, 'model.pth'))\n",
    "    \n",
    "    if (epoch_counter + 1) % config['save_every_n_epochs'] == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(log_dir, 'model_{}.pth'.format(str(epoch_counter))))\n",
    "\n",
    "    # # warmup for the first few epochs\n",
    "    # if epoch_counter >= config['warm_up']:\n",
    "        # wandb.log({\"cosine_lr_decay\": scheduler.get_last_lr()[0]}, step=epoch_counter)\n",
    "        # scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a38ad7b9-f82b-4936-bd4f-26266cbb6b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>bert_loss/eval</td><td>▇▇▆▅▆█▅▃▃▄▃▆▄▆▄▄▄▂▁▂▃▄▂▃▃▄▂▃▁▃▂▂▂▃▂▃▂▄▃▂</td></tr><tr><td>bert_loss/train</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>bimodal_loss/eval</td><td>█▁▁▁▁▁▁▁▁▁▂▁▁▁▂▂▁▁▁▁▁▂▁▁▁▂▁▁▂▁▂▁▂▁▁▁▁▁▁▁</td></tr><tr><td>bimodal_loss/train</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>graph_loss/eval</td><td>█▆▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>graph_loss/train</td><td>█▅▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/eval</td><td>█▆▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train</td><td>█▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>bert_loss/eval</td><td>0.39652</td></tr><tr><td>bert_loss/train</td><td>0.39203</td></tr><tr><td>bimodal_loss/eval</td><td>0.0</td></tr><tr><td>bimodal_loss/train</td><td>0.00285</td></tr><tr><td>graph_loss/eval</td><td>0.17174</td></tr><tr><td>graph_loss/train</td><td>0.14427</td></tr><tr><td>loss/eval</td><td>0.56826</td></tr><tr><td>loss/train</td><td>0.53914</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">RobertaForMaskedLM + MolCLR (GCN) equal_coeffs</strong> at: <a href='https://wandb.ai/moleculary-ai/efcp_transformer/runs/63kpwmt8' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer/runs/63kpwmt8</a><br/> View project at: <a href='https://wandb.ai/moleculary-ai/efcp_transformer' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240528_130020-63kpwmt8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fac871-4da6-496e-b1a6-e4e21259c1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe8dbd0-e453-4cd9-aeb3-4f88a4590b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dd4c9d-9edf-4d11-9d74-cf0c1bf90ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53c9ed63-eaf7-4e39-b91b-cb8d9aa00ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_batch = bert_batch.to('cpu')\n",
    "# graph_batch1 = graph_batch1.to('cpu')\n",
    "# graph_batch2 = graph_batch2.to('cpu')\n",
    "# del bert_batch, graph_batch1, graph_batch2\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1da8492a2d9f4cc9bd27b799fdd17cdc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5275c8a0514448c295b7387f93fab06d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_a816ca4ebe5d4022a1471f348c9cf3eb",
       "style": "IPY_MODEL_592ab1ddb9c246d6bffd8c63ec4af0f8"
      }
     },
     "54630b51b8d74f53944ecabf20ea6aa3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "592ab1ddb9c246d6bffd8c63ec4af0f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "6be428f1c9e0414ca160072a9be1cdf3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7b2ece2f9d6e4c539ee898005c2e1554": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "7ea3598694a34e56bfe1ed14ee4ea443": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_cd8f5935bc00493fb10dfb493a60d7a7",
       "style": "IPY_MODEL_7b2ece2f9d6e4c539ee898005c2e1554",
       "value": "0.682 MB of 0.682 MB uploaded (0.003 MB deduped)\r"
      }
     },
     "877dea417ba14f09a172c71805b7e4ae": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "98e2fb0daffd445b9b0879d1d9dc5e30": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a816ca4ebe5d4022a1471f348c9cf3eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cd8f5935bc00493fb10dfb493a60d7a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d0ee4193d2a64dbd878c22fad0564766": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_ebb8ee908aab4d08a3c8c1e73aa054bb",
       "max": 1,
       "style": "IPY_MODEL_6be428f1c9e0414ca160072a9be1cdf3"
      }
     },
     "e71096f6a47b42a480b7fe5a17de7157": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_1da8492a2d9f4cc9bd27b799fdd17cdc",
       "max": 1,
       "style": "IPY_MODEL_98e2fb0daffd445b9b0879d1d9dc5e30",
       "value": 1
      }
     },
     "ebb8ee908aab4d08a3c8c1e73aa054bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f5be5f4a84224a109c246412b2d64df1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5275c8a0514448c295b7387f93fab06d",
        "IPY_MODEL_d0ee4193d2a64dbd878c22fad0564766"
       ],
       "layout": "IPY_MODEL_877dea417ba14f09a172c71805b7e4ae"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
