{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d6f265e-e433-48e9-a783-950e8b12260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi | grep 300W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bed803df-3509-4228-abe8-be4e6eccae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f646d954-eebe-4ed1-8fd6-20c01f0bebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import wandb\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1e3494b-0296-4bb8-9b07-48a55533413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.require(\"service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f53265f1-214e-4d58-814f-6f7c7b05f907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/nick1899/.netrc\n"
     ]
    }
   ],
   "source": [
    "!python3 -m wandb login eb7b1964fb84cd81de96b2a273ecf2bb6254aeac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d70ee9-b6f5-415a-b98e-a6dc92cd6d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d91b4-c4dc-4dba-a04c-e786601aab87",
   "metadata": {},
   "source": [
    "### Upload config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4478096-04a8-4fe1-b1b9-8a8720750818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 32, 'warm_up': 2, 'epochs': 40, 'load_model': 'None', 'save_every_n_epochs': 1, 'fp16_precision': False, 'init_lr': 0.0001, 'weight_decay': '1e-5', 'gpu': 'cuda:0', 'graph_model_type': 'gcn', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 768, 'drop_ratio': 0, 'pool': 'mean'}, 'aug': 'node', 'dataset': {'num_workers': 12, 'valid_size': 0.1, 'data_path': 'data/pubchem-10m-clean.txt'}, 'loss': {'temperature': 0.1, 'use_cosine_similarity': True}, 'loss_params': {'alpha': 1, 'beta': 1.5, 'gamma': 3}}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "config = yaml.load(open(\"config-graphormer.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4b16779-4235-42fe-9091-531e93496fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 32\n"
     ]
    }
   ],
   "source": [
    "#config['batch_size'] = 16\n",
    "config['num_workers'] = 1\n",
    "print('batch_size =', config['batch_size'])\n",
    "#config['gpu'] = 0\n",
    "batch_size = config['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8caa6fa0-9b19-4665-81b8-b122a0c18ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('running on device:', config['gpu'])\n",
    "device = torch.device(config['gpu']) if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe4d265d-3a4d-4ec1-b616-18805240e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_config_file(config, log_dir):\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    with open(os.path.join(log_dir, 'config.yml'), 'w') as outfile:\n",
    "        yaml.dump(config, outfile, default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d58c5-eb7a-4420-b0db-f743e2167213",
   "metadata": {},
   "source": [
    "### Upload and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d5561e6-2477-4ead-9fae-91cae781bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe = pd.read_csv(\"cleared_pubchem10m-ecfp1.csv\", usecols = ['smiles', 'ecfp1'])\n",
    "dataframe = pd.read_csv(\"cleared_pubchem10m-ecfp1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63571013-4600-4c29-87ae-00b8429d7613",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a62f8524-4166-4a33-b7ab-cf07e9a9c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe = dataframe.sample(3200*4)\n",
    "dataframe = dataframe.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7bd0143-4bb2-429e-b4e9-229aaf9daddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[['2246728737', '3217380708', '3218693969', '3218693969', '3217380708', '2245384272', '2245273601', '847961216', '2246699815', '864942730', '2245900962', '2245900962', '3388977530', '2246728737', '3217380708', '3218693969', '3218693969', '3218693969', '3218693969', '3218693969', '3217380708', '3218693969', '3218693969', '3218693969', '3218693969', '3218693969', '3217380708', '3218693969', '3218693969', '3218693969', '3218693969', '3218693969', '3218693969', '3218693969']]\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['ecfp1'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cae64520-4a9c-453f-949a-737bbd58bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this because pandas thinks columns with arrays are strings\n",
    "def preprocess_data_dataset(df, column):\n",
    "    for row in tqdm(range(len(df))):\n",
    "        str_ints = eval(df.iloc[row][column])[0] # change for ecfp2 and so on\n",
    "        str_fingerprint = ' '.join(str_ints)\n",
    "        df.at[row, column] = str_fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1eb1370-13e9-4505-88da-b216213a873f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3185.21it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocess_data_dataset(dataframe, 'ecfp1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d257fa40-d3c6-47a4-985a-49ad07f5ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe = dataframe.rename(columns={'smiles': 'Smiles'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c3e904d-a6dc-4369-a9b9-59e7817bdc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>smiles</th>\n",
       "      <th>ecfp1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>236909</td>\n",
       "      <td>236909</td>\n",
       "      <td>Cc1ccc(CC(NC(=O)C#C[Si](C)(c2ccccc2)c2ccccc2)c...</td>\n",
       "      <td>2246728737 3217380708 3218693969 3218693969 32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6991312</td>\n",
       "      <td>6991312</td>\n",
       "      <td>COc1ccc(CNc2ccc(C(=O)N3CCCCCC3)cn2)cc1</td>\n",
       "      <td>2246728737 864674487 3217380708 3218693969 321...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4650989</td>\n",
       "      <td>4650989</td>\n",
       "      <td>C=CCNC(=O)CC(C(=O)[O-])c1ccncc1</td>\n",
       "      <td>2246997334 2246703798 2245384272 847961216 224...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1607453</td>\n",
       "      <td>1607453</td>\n",
       "      <td>COC(=O)c1ccc(NC2(C(F)(F)F)CC2)nn1</td>\n",
       "      <td>2246728737 864674487 2246699815 864942730 3217...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6424995</td>\n",
       "      <td>6424995</td>\n",
       "      <td>CC(C)n1ccc(NC(=O)NCc2ccn(C3CCCC3)n2)n1</td>\n",
       "      <td>2246728737 2245273601 2246728737 2092489639 32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1604060</td>\n",
       "      <td>1604060</td>\n",
       "      <td>COc1cccc(NC(=O)CSc2nnc(NN=Cc3cccc(Cl)c3)n2N)c1</td>\n",
       "      <td>2246728737 864674487 3217380708 3218693969 321...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>9605734</td>\n",
       "      <td>9605734</td>\n",
       "      <td>CCCN(Cc1ccc(N)cc1)c1nc(C)cc(C)n1</td>\n",
       "      <td>2246728737 2245384272 2245384272 848128881 224...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>7607186</td>\n",
       "      <td>7607186</td>\n",
       "      <td>COc1ccc(c2ncncc2CC(=O)[O-])cc1C</td>\n",
       "      <td>2246728737 864674487 3217380708 3218693969 321...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>3774350</td>\n",
       "      <td>3774350</td>\n",
       "      <td>CC1C[NH+](CC(O)COCc2ccccc2Cl)CC(C)O1</td>\n",
       "      <td>2246728737 2976033787 2968968094 2143075994 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>3176037</td>\n",
       "      <td>3176037</td>\n",
       "      <td>CCCCCCCCCO[Si](CC)(CC)Oc1cc(C)ccc1Cl</td>\n",
       "      <td>2246728737 2245384272 2245384272 2245384272 22...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.1  Unnamed: 0  \\\n",
       "0          236909      236909   \n",
       "1         6991312     6991312   \n",
       "2         4650989     4650989   \n",
       "3         1607453     1607453   \n",
       "4         6424995     6424995   \n",
       "..            ...         ...   \n",
       "995       1604060     1604060   \n",
       "996       9605734     9605734   \n",
       "997       7607186     7607186   \n",
       "998       3774350     3774350   \n",
       "999       3176037     3176037   \n",
       "\n",
       "                                                smiles  \\\n",
       "0    Cc1ccc(CC(NC(=O)C#C[Si](C)(c2ccccc2)c2ccccc2)c...   \n",
       "1               COc1ccc(CNc2ccc(C(=O)N3CCCCCC3)cn2)cc1   \n",
       "2                      C=CCNC(=O)CC(C(=O)[O-])c1ccncc1   \n",
       "3                    COC(=O)c1ccc(NC2(C(F)(F)F)CC2)nn1   \n",
       "4               CC(C)n1ccc(NC(=O)NCc2ccn(C3CCCC3)n2)n1   \n",
       "..                                                 ...   \n",
       "995     COc1cccc(NC(=O)CSc2nnc(NN=Cc3cccc(Cl)c3)n2N)c1   \n",
       "996                   CCCN(Cc1ccc(N)cc1)c1nc(C)cc(C)n1   \n",
       "997                    COc1ccc(c2ncncc2CC(=O)[O-])cc1C   \n",
       "998               CC1C[NH+](CC(O)COCc2ccccc2Cl)CC(C)O1   \n",
       "999               CCCCCCCCCO[Si](CC)(CC)Oc1cc(C)ccc1Cl   \n",
       "\n",
       "                                                 ecfp1  \n",
       "0    2246728737 3217380708 3218693969 3218693969 32...  \n",
       "1    2246728737 864674487 3217380708 3218693969 321...  \n",
       "2    2246997334 2246703798 2245384272 847961216 224...  \n",
       "3    2246728737 864674487 2246699815 864942730 3217...  \n",
       "4    2246728737 2245273601 2246728737 2092489639 32...  \n",
       "..                                                 ...  \n",
       "995  2246728737 864674487 3217380708 3218693969 321...  \n",
       "996  2246728737 2245384272 2245384272 848128881 224...  \n",
       "997  2246728737 864674487 3217380708 3218693969 321...  \n",
       "998  2246728737 2976033787 2968968094 2143075994 22...  \n",
       "999  2246728737 2245384272 2245384272 2245384272 22...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0acb3637-c146-45dc-a1d3-d9ef1363d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe = dataframe.drop(columns=['ecfp2', 'ecfp3', 'Molecular Weight', 'Bioactivities', 'AlogP', 'Polar Surface Area', 'CX Acidic pKa', 'CX Basic pKa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88a96536-673f-41df-a6b6-b3c021891847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287894eb-5d8b-4d23-ab1c-9d3a63a35b8c",
   "metadata": {},
   "source": [
    "### Create Molecule Dataset\n",
    "##### It will generate torch_geometric.data.Data objects for both bert and GIN/GCN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c61aaf7e-b46e-42f0-8981-67d2006f7f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "ATOM_LIST = list(range(1,119))\n",
    "CHIRALITY_LIST = [\n",
    "    Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "    Chem.rdchem.ChiralType.CHI_OTHER\n",
    "]\n",
    "BOND_LIST = [\n",
    "    Chem.rdchem.BondType.SINGLE, \n",
    "    Chem.rdchem.BondType.DOUBLE, \n",
    "    Chem.rdchem.BondType.TRIPLE, \n",
    "    Chem.rdchem.BondType.AROMATIC\n",
    "]\n",
    "BONDDIR_LIST = [\n",
    "    Chem.rdchem.BondDir.NONE,\n",
    "    Chem.rdchem.BondDir.ENDUPRIGHT,\n",
    "    Chem.rdchem.BondDir.ENDDOWNRIGHT\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f130cdad-b93d-4aff-934f-b95cddba0c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick1899/anaconda3/envs/mol/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name_bert = 'molberto_ecfp0_2M'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_bert)\n",
    "        # Creating a new column by applying the function\n",
    "#         self.dataset['graph'] = self.dataset['Smiles'].apply(self.get_graph_from_smiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "371b4fd8-4b62-40e4-8d17-583f2c68e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from torch_geometric.data import Data, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8eadbf35-57e6-4ff4-9a36-883970fb8c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nick1899/anaconda3/envs/mol/lib/python3.9/site-packages/transformers/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3dd5d0f-d5a9-4041-9587-52c79d9d2d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers.models.graphormer.collating_graphormer import GraphormerDataCollator\n",
    "import transformers\n",
    "#from transformers.models.graphormer.collating_graphormer import preprocess_item, GraphormerDataCollator\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GraphormerForGraphClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from transformers import AdamW, get_scheduler\n",
    "#import transformers.models.graphormer.collating_graphormer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7339ed50-d66c-4b3c-bbc9-d7d284918b88",
   "metadata": {},
   "source": [
    "# Graphormer collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0654c00-0422-471e-9c5f-a64bb73c8718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import is_cython_available\n",
    "if is_cython_available():\n",
    "\n",
    "    import pyximport\n",
    "\n",
    "    pyximport.install(setup_args={\"include_dirs\": np.get_include()})\n",
    "    \n",
    "    from transformers.models.graphormer import algos_graphormer as algos_graphormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ac996bb-f7c3-4b94-ac4e-8c574821a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Mapping\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers.utils import is_cython_available, requires_backends\n",
    "\n",
    "\n",
    "if is_cython_available():\n",
    "    import pyximport\n",
    "\n",
    "    pyximport.install(setup_args={\"include_dirs\": np.get_include()})\n",
    "    import sys\n",
    "    sys.path.append('algos_graphormer.so')\n",
    "    import algos_graphormer\n",
    "\n",
    "\n",
    "def convert_to_single_emb(x, offset: int = 512):\n",
    "    feature_num = x.shape[1] if len(x.shape) > 1 else 1\n",
    "    feature_offset = 1 + np.arange(0, feature_num * offset, offset, dtype=np.int64)\n",
    "    x = x + feature_offset\n",
    "    return x\n",
    "\n",
    "\n",
    "def preprocess_item(item, keep_features=True):\n",
    "    requires_backends(preprocess_item, [\"cython\"])\n",
    "\n",
    "    if keep_features and \"edge_attr\" in item.keys():  # edge_attr\n",
    "        edge_attr = np.asarray(item[\"edge_attr\"], dtype=np.int64)\n",
    "    else:\n",
    "        edge_attr = np.ones((len(item[\"edge_index\"][0]), 1), dtype=np.int64)  # same embedding for all\n",
    "\n",
    "    if keep_features and \"node_feat\" in item.keys():  # input_nodes\n",
    "        node_feature = np.asarray(item[\"node_feat\"], dtype=np.int64)\n",
    "    else:\n",
    "        node_feature = np.ones((item[\"num_nodes\"], 1), dtype=np.int64)  # same embedding for all\n",
    "\n",
    "    edge_index = np.asarray(item[\"edge_index\"], dtype=np.int64)\n",
    "\n",
    "    input_nodes = convert_to_single_emb(node_feature) + 1\n",
    "    num_nodes = item[\"num_nodes\"]\n",
    "\n",
    "    if len(edge_attr.shape) == 1:\n",
    "        edge_attr = edge_attr[:, None]\n",
    "    attn_edge_type = np.zeros([num_nodes, num_nodes, edge_attr.shape[-1]], dtype=np.int64)\n",
    "    attn_edge_type[edge_index[0], edge_index[1]] = convert_to_single_emb(edge_attr) + 1\n",
    "\n",
    "    # node adj matrix [num_nodes, num_nodes] bool\n",
    "    adj = np.zeros([num_nodes, num_nodes], dtype=bool)\n",
    "    adj[edge_index[0], edge_index[1]] = True\n",
    "\n",
    "    shortest_path_result, path = algos_graphormer.floyd_warshall(adj)\n",
    "    max_dist = np.amax(shortest_path_result)\n",
    "\n",
    "    input_edges = algos_graphormer.gen_edge_input(max_dist, path, attn_edge_type)\n",
    "    attn_bias = np.zeros([num_nodes + 1, num_nodes + 1], dtype=np.single)  # with graph token\n",
    "\n",
    "    # combine\n",
    "    item[\"input_nodes\"] = input_nodes + 1  # we shift all indices by one for padding\n",
    "    item[\"attn_bias\"] = attn_bias\n",
    "    item[\"attn_edge_type\"] = attn_edge_type\n",
    "    item[\"spatial_pos\"] = shortest_path_result.astype(np.int64) + 1  # we shift all indices by one for padding\n",
    "    item[\"in_degree\"] = np.sum(adj, axis=1).reshape(-1) + 1  # we shift all indices by one for padding\n",
    "    item[\"out_degree\"] = item[\"in_degree\"]  # for undirected graph\n",
    "    item[\"input_edges\"] = input_edges + 1  # we shift all indices by one for padding\n",
    "    if \"labels\" not in item:\n",
    "        item[\"labels\"] = item[\"y\"]\n",
    "\n",
    "    return item\n",
    "\n",
    "\n",
    "class GraphormerDataCollator:\n",
    "    def __init__(self, spatial_pos_max=20, on_the_fly_processing=False):\n",
    "        if not is_cython_available():\n",
    "            raise ImportError(\"Graphormer preprocessing needs Cython (pyximport)\")\n",
    "\n",
    "        self.spatial_pos_max = spatial_pos_max\n",
    "        self.on_the_fly_processing = on_the_fly_processing\n",
    "\n",
    "    def __call__(self, features: List[dict]) -> Dict[str, Any]:\n",
    "        if self.on_the_fly_processing:\n",
    "            features = [preprocess_item(i) for i in features]\n",
    "\n",
    "        if not isinstance(features[0], Mapping):\n",
    "            features = [vars(f) for f in features]\n",
    "        batch = {}\n",
    "\n",
    "        max_node_num = max(len(i[\"input_nodes\"]) for i in features)\n",
    "        node_feat_size = len(features[0][\"input_nodes\"][0])\n",
    "        edge_feat_size = len(features[0][\"attn_edge_type\"][0][0])\n",
    "        max_dist = max(len(i[\"input_edges\"][0][0]) for i in features)\n",
    "        edge_input_size = len(features[0][\"input_edges\"][0][0][0])\n",
    "        batch_size = len(features)\n",
    "\n",
    "        batch[\"attn_bias\"] = torch.zeros(batch_size, max_node_num + 1, max_node_num + 1, dtype=torch.float)\n",
    "        batch[\"attn_edge_type\"] = torch.zeros(batch_size, max_node_num, max_node_num, edge_feat_size, dtype=torch.long)\n",
    "        batch[\"spatial_pos\"] = torch.zeros(batch_size, max_node_num, max_node_num, dtype=torch.long)\n",
    "        batch[\"in_degree\"] = torch.zeros(batch_size, max_node_num, dtype=torch.long)\n",
    "        batch[\"input_nodes\"] = torch.zeros(batch_size, max_node_num, node_feat_size, dtype=torch.long)\n",
    "        batch[\"input_edges\"] = torch.zeros(\n",
    "            batch_size, max_node_num, max_node_num, max_dist, edge_input_size, dtype=torch.long\n",
    "        )\n",
    "\n",
    "        for ix, f in enumerate(features):\n",
    "            for k in [\"attn_bias\", \"attn_edge_type\", \"spatial_pos\", \"in_degree\", \"input_nodes\", \"input_edges\"]:\n",
    "                f[k] = torch.tensor(f[k])\n",
    "\n",
    "            if len(f[\"attn_bias\"][1:, 1:][f[\"spatial_pos\"] >= self.spatial_pos_max]) > 0:\n",
    "                f[\"attn_bias\"][1:, 1:][f[\"spatial_pos\"] >= self.spatial_pos_max] = float(\"-inf\")\n",
    "\n",
    "            batch[\"attn_bias\"][ix, : f[\"attn_bias\"].shape[0], : f[\"attn_bias\"].shape[1]] = f[\"attn_bias\"]\n",
    "            batch[\"attn_edge_type\"][ix, : f[\"attn_edge_type\"].shape[0], : f[\"attn_edge_type\"].shape[1], :] = f[\n",
    "                \"attn_edge_type\"\n",
    "            ]\n",
    "            batch[\"spatial_pos\"][ix, : f[\"spatial_pos\"].shape[0], : f[\"spatial_pos\"].shape[1]] = f[\"spatial_pos\"]\n",
    "            batch[\"in_degree\"][ix, : f[\"in_degree\"].shape[0]] = f[\"in_degree\"]\n",
    "            batch[\"input_nodes\"][ix, : f[\"input_nodes\"].shape[0], :] = f[\"input_nodes\"]\n",
    "            batch[\"input_edges\"][\n",
    "                ix, : f[\"input_edges\"].shape[0], : f[\"input_edges\"].shape[1], : f[\"input_edges\"].shape[2], :\n",
    "            ] = f[\"input_edges\"]\n",
    "\n",
    "        batch[\"out_degree\"] = batch[\"in_degree\"]\n",
    "\n",
    "        sample = features[0][\"labels\"]\n",
    "        if len(sample) == 1:  # one task\n",
    "            if isinstance(sample[0], float):  # regression\n",
    "                batch[\"labels\"] = torch.from_numpy(np.concatenate([i[\"labels\"] for i in features]))\n",
    "            else:  # binary classification\n",
    "                batch[\"labels\"] = torch.from_numpy(np.concatenate([i[\"labels\"] for i in features]))\n",
    "        else:  # multi task classification, left to float to keep the NaNs\n",
    "            batch[\"labels\"] = torch.from_numpy(np.stack([i[\"labels\"] for i in features], axis=0))\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2186d88e-71b5-4a1e-965f-0e5e7cda686e",
   "metadata": {},
   "source": [
    "# Data prerpoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0fc974c-3918-4c87-9643-8dda1b0b55f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getshortest_path(datapoint):\n",
    "    num_nodes=len(datapoint['node_feat'])\n",
    "    edge_index = datapoint['edge_index']\n",
    "    adj = np.zeros([num_nodes, num_nodes], dtype=bool)\n",
    "    adj[edge_index[0], edge_index[1]] = True\n",
    "    shortest_path_result, path = algos_graphormer.floyd_warshall(adj)\n",
    "    max_dist = np.amax(shortest_path_result)\n",
    "    return {\"max_dist\":max_dist, \"path\": path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "761f8c98-1858-4570-a139-4177015321e9",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def processItemForGraphormer(graph, yi):\n",
    "    processed = preprocess_item(\n",
    "                 {\"node_feat\":graph.x.tolist(),\n",
    "                 \"edge_index\":graph.edge_index.tolist(),\n",
    "                 \"edge_attr\":graph.edge_attr.tolist(),\n",
    "                 \"num_nodes\":len(graph.x),\n",
    "                 'y': yi\n",
    "                })\n",
    "    processed['attn_edge_type_ORIG'] = np.array(processed['attn_edge_type'])+0\n",
    "    processed['input_nodes_ORIG'] = np.array(processed['input_nodes'])+0\n",
    "    return processed\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5d0bdc0-89c4-494d-a76a-5cc7071cca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Dataset\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, dataset: pd.DataFrame, tokenizer, node_mask_percent=0.15, edge_mask_percent=0.2):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.node_mask_percent = node_mask_percent\n",
    "        self.edge_mask_percent = edge_mask_percent\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.model_max_len = 512\n",
    "        \n",
    "        self.yi = torch.tensor(np.full(( 768), 1).tolist()) \n",
    "        \n",
    "        self.dataset['tokens'] = self.dataset['ecfp1'].progress_apply(self.tokenize)\n",
    "        self.dataset['graph'] = self.dataset['smiles'].progress_apply(self.get_graph_from_smiles)\n",
    "        \n",
    "    \n",
    "        self.dataset['graphormerdata'] = self.dataset['graph'].progress_apply(\n",
    "              lambda graph:   processItemForGraphormer(graph, self.yi)                    \n",
    "        )                                                     \n",
    "                                                                     \n",
    "        self.dataset['graphormerdataRAW'] = self.dataset['graph'].progress_apply(\n",
    "              lambda graph:\n",
    "#       preprocess_item(\n",
    "                 {\"node_feat\":graph.x.tolist(),\n",
    "                 \"edge_index\":graph.edge_index.tolist(),\n",
    "                 \"edge_attr\":graph.edge_attr.tolist(),\n",
    "                 \"num_nodes\":len(graph.x),\n",
    "                 'y': self.yi\n",
    "                }\n",
    "#         )                      \n",
    "        ) \n",
    "        \n",
    "        self.dataset['shortest_path'] = self.dataset['graphormerdataRAW'].progress_apply(\n",
    "                   lambda datapoint:\n",
    "                        getshortest_path(datapoint)\n",
    "                    )\n",
    "        \n",
    "        self.maskedGraphAtom = torch.tensor([[len(ATOM_LIST),0]],dtype=torch.long)\n",
    "        self.edgeGraphMask = torch.tensor([len(BOND_LIST) + 1, len(BONDDIR_LIST)], dtype=torch.long)\n",
    "        \n",
    " \n",
    "    def get_graph_from_smiles(self, smiles):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return torch.tensor([[], []], dtype=torch.long), \\\n",
    "                    torch.tensor(np.array([]), dtype=torch.long), \\\n",
    "                    torch.tensor(np.array([]), dtype=torch.long), \\\n",
    "                    0\n",
    "    \n",
    "        N = mol.GetNumAtoms()\n",
    "        M = mol.GetNumBonds()\n",
    "    \n",
    "        type_idx = []\n",
    "        chirality_idx = []\n",
    "        atomic_number = []\n",
    "        \n",
    "        for atom in mol.GetAtoms():\n",
    "            type_idx.append(ATOM_LIST.index(atom.GetAtomicNum()))\n",
    "            chirality_idx.append(CHIRALITY_LIST.index(atom.GetChiralTag()))\n",
    "            atomic_number.append(atom.GetAtomicNum())\n",
    "        \n",
    "        x1 = torch.tensor(type_idx, dtype=torch.long).view(-1,1)\n",
    "        x2 = torch.tensor(chirality_idx, dtype=torch.long).view(-1,1)\n",
    "        node_feat = torch.cat([x1, x2], dim=-1)\n",
    "    \n",
    "        row, col, edge_feat = [], [], []\n",
    "        for bond in mol.GetBonds():\n",
    "            start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            row += [start, end]\n",
    "            col += [end, start]\n",
    "            \n",
    "            edge_feat.append([\n",
    "                BOND_LIST.index(bond.GetBondType()),\n",
    "                BONDDIR_LIST.index(bond.GetBondDir())\n",
    "            ])\n",
    "            edge_feat.append([\n",
    "                BOND_LIST.index(bond.GetBondType()),\n",
    "                BONDDIR_LIST.index(bond.GetBondDir())\n",
    "            ])\n",
    "    \n",
    "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "        edge_attr = torch.tensor(edge_feat, dtype=torch.long)\n",
    "        num_nodes = N\n",
    "        num_edges = M\n",
    "        return Data(x=node_feat, edge_index=edge_index, edge_attr=edge_attr)\n",
    "         \n",
    "\n",
    "    def get_augmented_graph_copy(self, node_feat, edge_index, edge_attr, N, M):\n",
    "        num_mask_nodes = max([1, math.floor(self.node_mask_percent * N)])\n",
    "        #num_mask_nodes = 1\n",
    "        num_mask_edges = max([0, math.floor(self.edge_mask_percent * M)])\n",
    "\n",
    "        \n",
    "        mask_nodes = random.sample(list(range(N)), num_mask_nodes)\n",
    "        mask_edges_single = random.sample(list(range(M)), num_mask_edges)\n",
    "        \n",
    "        \n",
    "        mask_edges = [2*i for i in mask_edges_single] + [2*i+1 for i in mask_edges_single]\n",
    "\n",
    "        \n",
    "        node_feat_new = deepcopy(node_feat)\n",
    "        \n",
    "        node_feat_new[mask_nodes] = (node_feat_new[mask_nodes][:,:]*0 + self.maskedGraphAtom)\n",
    "            \n",
    "        edge_attr_new = edge_attr\n",
    "        edge_attr_new[mask_edges] =  self.edgeGraphMask\n",
    "\n",
    "        return Data(x=node_feat_new, edge_index=edge_index, edge_attr=edge_attr_new)\n",
    "\n",
    "    def tokenize(self, item):\n",
    "        sample = self.tokenizer(item, truncation=True, max_length=512, padding='max_length')\n",
    "        return (torch.tensor(sample.input_ids), \n",
    "                torch.tensor(sample.attention_mask), \n",
    "                torch.tensor(sample.input_ids)\n",
    "               )\n",
    "        return Data(input_ids=sample.input_ids, attention_mask=sample.attention_mask, labels=sample.input_ids)\n",
    "\n",
    "    def mlm(self, tensor):\n",
    "        rand = torch.rand(tensor.shape)\n",
    "        # mask random 15% where token is not 0 <s>, 1 <pad>, or 2 <s/>\n",
    "        mask_arr = (rand < .15) * (tensor != 0) * (tensor != 1) * (tensor != 2)\n",
    "        selection = torch.flatten(mask_arr.nonzero()).tolist()\n",
    "        # mask tensor, token == 4 is our mask token\n",
    "        tensor[selection] = 4\n",
    "        return tensor\n",
    "\n",
    "    def apply_mlm(self, sample):\n",
    "        labels = torch.tensor(sample.input_ids)\n",
    "        attention_mask = torch.tensor(sample.attention_mask)\n",
    "        input_ids = self.mlm(labels.detach().clone())\n",
    "        return Data(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        bert = self.dataset['tokens'][index]\n",
    "        graph = self.dataset['graph'][index]\n",
    "        \n",
    "        return graph, bert\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def get(self):\n",
    "        pass\n",
    "    def len(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89dfd5e8-5cb6-4aa0-878b-6444d9e89652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2325.19it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1611.44it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:09<00:00, 100.22it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 40838.36it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:03<00:00, 295.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.459450960159302"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = time.time()\n",
    "dataset = MoleculeDataset(dataframe, tokenizer)\n",
    "time.time()-st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "977ce22d-d2e3-44ea-89c6-28a6e7591584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#st = time.time()\n",
    "#for j in range(len(dataset)):\n",
    "    #dd = dataset[j]\n",
    "#time.time()-st, len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73aa3bda-ba68-4ed7-910b-09da213e8f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers.models.graphormer.collating_graphormer import GraphormerDataCollator\n",
    "import transformers\n",
    "#from transformers.models.graphormer.collating_graphormer import preprocess_item, GraphormerDataCollator\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58bbedee-eeb8-4690-ab2c-d940025e49e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45effc79-94ba-40da-a5c6-7478a0a29057",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "num_train = len(dataset)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = int(np.floor(config['dataset']['valid_size'] * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# config['dataset']['num_workers'] = 1\n",
    "# train_dataloader = DataLoader(\n",
    "#     dataset, batch_size=config['batch_size'], sampler=train_sampler,\n",
    "#     num_workers=config['dataset']['num_workers'], drop_last=True\n",
    "# )\n",
    "\n",
    "# epoch_counter = 0\n",
    "# train_tqdm = tqdm(train_dataloader, unit=\"batch\")\n",
    "# train_tqdm.set_description(f'Epoch {epoch_counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b33bf7fb-6ba9-4c4c-9e09-7e1b6f57cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def pad_to_shape(tensor, target_shape):\n",
    "    current_shape = tensor.shape\n",
    "    num_dims = len(current_shape)\n",
    "    \n",
    "    if num_dims != len(target_shape):\n",
    "        raise ValueError(f\"Tensor has {num_dims} dimensions but target shape has {len(target_shape)} dimensions.\")\n",
    "    \n",
    "    # Calculate padding needed for each dimension\n",
    "    padding = []\n",
    "    for i in range(num_dims - 1, -1, -1):  # Iterate from the last dimension backwards\n",
    "        if target_shape[i] < current_shape[i]:\n",
    "            raise ValueError(f\"Target shape at dimension {i} is smaller than the tensor shape.\")\n",
    "        padding.append(0)  # No padding on the left\n",
    "        padding.append(target_shape[i] - current_shape[i])  # Right side padding\n",
    "        \n",
    "\n",
    "    # Apply padding\n",
    "    padded_tensor = F.pad(tensor, padding)\n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c7d51a4-aac5-428e-aceb-ae743f590634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pyximport\\n\\npyximport.install(setup_args={\"include_dirs\": np.get_include()})\\n\\nfrom transformers.models.graphormer.collating_graphormer import algos_graphormer\\n\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pyximport\n",
    "\n",
    "pyximport.install(setup_args={\"include_dirs\": np.get_include()})\n",
    "\n",
    "from transformers.models.graphormer.collating_graphormer import algos_graphormer\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3681481a-7c4b-4681-8014-d92839659201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.node_mask_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2b48f968-3fe8-40a7-88f8-1619524f9198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import copy\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "class CustomBatchDataset(IterableDataset):\n",
    "    \n",
    "    def setTrain(self,train = True):\n",
    "        if train:\n",
    "            self.sample_id = self.train_idx\n",
    "            self.train = True\n",
    "        else:\n",
    "            self.sample_id = self.valid_idx\n",
    "            self.train = False\n",
    "    \n",
    "    def __init__(self, dataframe,dataset, train_idx,valid_idx, batch_size):\n",
    "        \n",
    "        self.y = torch.tensor(np.full((config['batch_size'] * 2, 768), 1).tolist())\n",
    "        self.yi = torch.tensor(np.full(( 768), 1).tolist())\n",
    "        self.dataset=dataset\n",
    "        self.dataframe = dataframe\n",
    "        self.sample_id = train_idx\n",
    "        self.valid_idx = valid_idx\n",
    "        self.train_idx = train_idx\n",
    "        self.batch_size = batch_size\n",
    "        self.train = True\n",
    "        \n",
    "        input_ids = [e[0] for e in dataframe['tokens']]\n",
    "        self.input_ids = torch.stack(input_ids)\n",
    "\n",
    "        attention_mask = [e[1] for e in dataframe['tokens']]\n",
    "        self.attention_mask = torch.stack(attention_mask)\n",
    "\n",
    "        labels = [e[2] for e in dataframe['tokens']]\n",
    "        self.labels = torch.stack(labels)\n",
    "\n",
    "        self.graphs = [e for e in dataframe['graph']]\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Custom iterator that yields batches of data and labels.\n",
    "        \"\"\"\n",
    "        # Get the total number of samples\n",
    "        total_samples = (len(self.sample_id)//self.batch_size)*self.batch_size\n",
    "        if self.train:\n",
    "            np.random.shuffle(self.sample_id)\n",
    "        # Yield minibatches\n",
    "        for i in range(0, total_samples, self.batch_size):\n",
    "             \n",
    "            S = self.sample_id[ i : i + self.batch_size]\n",
    "            \n",
    "            inp_Idx =  self.input_ids[S]\n",
    "            rand = torch.rand(inp_Idx.shape)\n",
    "            mask_arr = (rand < .15) * (inp_Idx != 0) * (inp_Idx != 1) * (inp_Idx != 2)\n",
    "            inp_Idx[mask_arr] = 4\n",
    "            atte = self.attention_mask[S]\n",
    "            labe = self.labels[S]\n",
    "            \n",
    "            SS = S+S\n",
    "            graphdataProcessed =  [ self.dataframe.graphormerdata[i] for i in SS]\n",
    "\n",
    "            shortestPath =  [ self.dataframe.shortest_path[i] for i in SS]\n",
    "            selId = [ \n",
    "                    random.sample(\n",
    "                     list(range(len(g[\"node_feat\"]))),  \n",
    "                                      max([1, math.floor(self.dataset.node_mask_percent \n",
    "                                                         * len(g[\"node_feat\"]))])\n",
    "                             )\n",
    "                                            for g  in  graphdataProcessed ]\n",
    "             \n",
    "            \n",
    "            for g, selidi in zip(graphdataProcessed,selId):\n",
    "                g[\"input_nodes\"]= g[\"input_nodes_ORIG\"] +0\n",
    "                for s in selidi:\n",
    "                    g[\"input_nodes\"][s,0]=self.dataset.maskedGraphAtom[0][0]\n",
    "            \n",
    "#             eselId = [ \n",
    "#                      random.sample(\n",
    "#                      list(range(len(g['edge_index'][0])//2)),  \n",
    "#                       max([0, math.floor(self.dataset.edge_mask_percent * (len(g['edge_index'][0])//2))])\n",
    "#              )\n",
    "#                             for g   in   graphdataProcessed ] \n",
    "            \n",
    "            eselId = [ \n",
    "                 random.sample(\n",
    "                                 list(range(len(g['edge_index'][0])//2)),  \n",
    "                                  max([0, math.floor(self.dataset.edge_mask_percent * (len(g['edge_index'][0])//2))])\n",
    "                         )\n",
    "                                        for g   in   graphdataProcessed ]\n",
    "            for g, eid  in zip(graphdataProcessed, eselId):\n",
    "                g['attn_edge_type'] = np.array(g['attn_edge_type_ORIG'])+0\n",
    "                for e in eid:\n",
    "                    fn = g['edge_index'][0][2*e]\n",
    "                    tn = g['edge_index'][1][2*e]\n",
    "                    g['attn_edge_type'][fn,tn,:] = self.dataset.edgeGraphMask\n",
    "                    fn = g['edge_index'][0][2*e+1]\n",
    "                    tn = g['edge_index'][1][2*e+1]\n",
    "                    g['attn_edge_type'][fn,tn,:] = self.dataset.edgeGraphMask\n",
    "            \n",
    "            \n",
    "            for g, sp  in zip(graphdataProcessed, shortestPath):\n",
    "                edge_attr = g['attn_edge_type']\n",
    "                \n",
    "                input_edges = algos_graphormer.gen_edge_input(sp['max_dist'], sp['path'], edge_attr)\n",
    "\n",
    "                g['input_edges'] = input_edges+1\n",
    "            \n",
    "            yield inp_Idx, atte, labe, graphdataProcessed#, g1, g2 \n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_idx)//self.batch_size\n",
    "        else:\n",
    "            return len(self.valid_idx)//self.batch_size\n",
    "\n",
    "\n",
    "custom_batch_dataset = CustomBatchDataset(dataframe, dataset, train_idx,valid_idx, batch_size)\n",
    "\n",
    "#LST=[]\n",
    "#j=0\n",
    "#st = time.time()\n",
    "#for batch_idx, (inp_Idx, atte, labe, graphdataProcessed   ) in tqdm(enumerate(custom_batch_dataset)):\n",
    "    #j+=1\n",
    "#     print(graphdataProcessed[0]['input_edges'].shape)\n",
    "    #if j > 1:\n",
    "        #break\n",
    "      \n",
    "   \n",
    "#(time.time()-st) ,0.7877202033996582, 0.512915849685669\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a56144-3102-4a21-b86b-8e851dc97026",
   "metadata": {},
   "source": [
    "### Create Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0430d192-999e-4e08-aa35-c8b764917137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GraphormerForGraphClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.models.graphormer.collating_graphormer import GraphormerDataCollator\n",
    "import transformers\n",
    "from transformers.models.graphormer.collating_graphormer import preprocess_item, GraphormerDataCollator\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "model_name_base = 'graphormer-base-pcqm4mv1'\n",
    "model_name = 'clefourrier/graphormer-base-pcqm4mv1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e390bc35-7de5-453e-a995-2d86f0bbf3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTXentLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, device, batch_size, temperature, use_cosine_similarity):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.device = device\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.mask_samples_from_same_repr = self._get_correlated_mask().type(torch.bool)\n",
    "        self.similarity_function = self._get_similarity_function(use_cosine_similarity)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    def _get_similarity_function(self, use_cosine_similarity):\n",
    "        if use_cosine_similarity:\n",
    "            self._cosine_similarity = torch.nn.CosineSimilarity(dim=-1)\n",
    "            return self._cosine_simililarity\n",
    "        else:\n",
    "            return self._dot_simililarity\n",
    "\n",
    "    def _get_correlated_mask(self):\n",
    "        diag = np.eye(2 * self.batch_size)\n",
    "        l1 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=-self.batch_size)\n",
    "        l2 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=self.batch_size)\n",
    "        mask = torch.from_numpy((diag + l1 + l2))\n",
    "        mask = (1 - mask).type(torch.bool)\n",
    "        return mask.to(self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def _dot_simililarity(x, y):\n",
    "        v = torch.tensordot(x.unsqueeze(1), y.T.unsqueeze(0), dims=2)\n",
    "        # x shape: (N, 1, C)\n",
    "        # y shape: (1, C, 2N)\n",
    "        # v shape: (N, 2N)\n",
    "        return v\n",
    "\n",
    "    def _cosine_simililarity(self, x, y):\n",
    "        # x shape: (N, 1, C)\n",
    "        # y shape: (1, 2N, C)\n",
    "        # v shape: (N, 2N)\n",
    "        v = self._cosine_similarity(x.unsqueeze(1), y.unsqueeze(0))\n",
    "        return v\n",
    "\n",
    "    def forward(self, zis, zjs):\n",
    "        representations = torch.cat([zjs, zis], dim=0)\n",
    "\n",
    "        similarity_matrix = self.similarity_function(representations, representations)\n",
    "\n",
    "        # filter out the scores from the positive samples\n",
    "        l_pos = torch.diag(similarity_matrix, self.batch_size)\n",
    "        r_pos = torch.diag(similarity_matrix, -self.batch_size)\n",
    "        positives = torch.cat([l_pos, r_pos]).view(2 * self.batch_size, 1)\n",
    "        negatives = similarity_matrix[self.mask_samples_from_same_repr].view(2 * self.batch_size, -1)\n",
    "\n",
    "        logits = torch.cat((positives, negatives), dim=1)\n",
    "        logits = logits.abs() + 0.0001\n",
    "        logits = torch.log(logits)\n",
    "        logits /= self.temperature\n",
    "        \n",
    "        labels = torch.zeros(2 * self.batch_size).to(self.device).long()\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        return loss / (2 * self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "93944aa4-9c68-4e6e-9d99-229bd5b70db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GraphormerForGraphClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "\n",
    "class GraphormerDataCollator_():\n",
    "    def __init__(self):\n",
    "        self.data_collator = GraphormerDataCollator()\n",
    "\n",
    "    def __call__(self, features):\n",
    "        for mol in features:\n",
    "            if mol['num_nodes'] == 1:\n",
    "                features.remove(mol)\n",
    "        return self.data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "54ef89d4-b7ac-4a86-ab14-67383fd0f726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:o7zfa61e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>bert_loss/train</td><td>███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>bimodal_loss/train</td><td>▇▅▅▇▄▅▅▆▄▅█▃▇▆▅▅▅▅▃▅▄▄▂▇▃▄▃▃▃▅▅▃▄▄▃▅▁▄</td></tr><tr><td>graph_loss/train</td><td>████▇▆▆▆▅▅▄▅▄▃▃▃▃▂▂▂▃▂▃▂▂▂▂▁▁▁▁▂▁▂▁▁▁▁</td></tr><tr><td>loss/train</td><td>█▆▆█▆▅▆▇▅▅█▄▇▆▅▅▅▅▃▅▄▄▂▆▃▄▃▃▃▅▅▃▄▃▃▄▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>bert_loss/train</td><td>5.06958</td></tr><tr><td>bimodal_loss/train</td><td>26.57698</td></tr><tr><td>graph_loss/train</td><td>0.18155</td></tr><tr><td>loss/train</td><td>85.07286</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">RobertaForMaskedLM + Graphormer-speed-up-1m</strong> at: <a href='https://wandb.ai/moleculary-ai/efcp_transformer/runs/o7zfa61e' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer/runs/o7zfa61e</a><br/> View project at: <a href='https://wandb.ai/moleculary-ai/efcp_transformer' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240916_223159-o7zfa61e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:o7zfa61e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nick1899/Transformers-for-Molecules/bert+MolCLR/wandb/run-20240916_223820-4jzqls4c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/moleculary-ai/efcp_transformer/runs/4jzqls4c' target=\"_blank\">RobertaForMaskedLM + Graphormer-speed-up-1m</a></strong> to <a href='https://wandb.ai/moleculary-ai/efcp_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/moleculary-ai/efcp_transformer' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/moleculary-ai/efcp_transformer/runs/4jzqls4c' target=\"_blank\">https://wandb.ai/moleculary-ai/efcp_transformer/runs/4jzqls4c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/moleculary-ai/efcp_transformer/runs/4jzqls4c?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f120eccff70>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 0 is less than current step: 136. Dropping entry: {'bert_loss/train': 7.302694156252104, '_timestamp': 1726516099.8814602}).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 0 is less than current step: 136. Dropping entry: {'graph_loss/train': 1.4482482733397648, '_timestamp': 1726516099.8816173}).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 0 is less than current step: 136. Dropping entry: {'bimodal_loss/train': 25.60427369742558, '_timestamp': 1726516099.8816998}).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 0 is less than current step: 136. Dropping entry: {'loss/train': 86.28788862557246, '_timestamp': 1726516099.8818762}).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 0 is less than current step: 136. Dropping entry: {'bert_loss/eval': 4.076030254364014, '_timestamp': 1726516111.7001872}).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 0 is less than current step: 136. Dropping entry: {'graph_loss/eval': 0.1263503059744835, '_timestamp': 1726516111.7004616}).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 0 is less than current step: 136. Dropping entry: {'bimodal_loss/eval': 35.73015022277832, '_timestamp': 1726516111.7005558}).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 0 is less than current step: 136. Dropping entry: {'loss/eval': 111.45600891113281, '_timestamp': 1726516111.700636}).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 1 is less than current step: 248. Dropping entry: {'bert_loss/train': 3.388412381040639, '_timestamp': 1726516248.5797064}).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 1 is less than current step: 248. Dropping entry: {'graph_loss/train': 0.18992823079742235, '_timestamp': 1726516248.5798476}).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 1 is less than current step: 248. Dropping entry: {'bimodal_loss/train': 23.81428764606344, '_timestamp': 1726516248.579926}).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 1 is less than current step: 248. Dropping entry: {'loss/train': 75.11616673962823, '_timestamp': 1726516248.5800028}).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 1 is less than current step: 248. Dropping entry: {'bert_loss/eval': 1.1615713238716125, '_timestamp': 1726516260.3028262}).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 1 is less than current step: 248. Dropping entry: {'graph_loss/eval': 0.009895066556055099, '_timestamp': 1726516260.3034801}).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 1 is less than current step: 248. Dropping entry: {'bimodal_loss/eval': 17.59123468399048, '_timestamp': 1726516260.3036752}).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m (User provided step: 1 is less than current step: 248. Dropping entry: {'loss/eval': 53.950117111206055, '_timestamp': 1726516260.3038137}).\n"
     ]
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"efcp_transformer\",\n",
    "    name=\"RobertaForMaskedLM + Graphormer-speed-up-1m\",\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "66254640-38dc-4da6-bccf-634174aca108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick1899/anaconda3/envs/mol/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at clefourrier/graphormer-base-pcqm4mv1 were not used when initializing GraphormerForGraphClassification: ['classifier.classifier.weight']\n",
      "- This IS expected if you are initializing GraphormerForGraphClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GraphormerForGraphClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import RobertaConfig\n",
    "from torch import nn\n",
    "\n",
    "class MolecularBertGraph(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MolecularBertGraph, self).__init__()\n",
    "        self.batch_size = config['batch_size']\n",
    "\n",
    "        roberta_config = RobertaConfig(\n",
    "            vocab_size=30_522,\n",
    "            max_position_embeddings=514,\n",
    "            hidden_size=768,\n",
    "            num_attention_heads=12,\n",
    "            num_hidden_layers=6,\n",
    "            type_vocab_size=1\n",
    "        )\n",
    "        self.bert = RobertaForMaskedLM(roberta_config)\n",
    "        \n",
    "        self.data_collator = GraphormerDataCollator_()\n",
    "        \n",
    "        self.graph_model = GraphormerForGraphClassification.from_pretrained(\n",
    "            model_name, \n",
    "            num_classes=1,\n",
    "            ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    "            ).to(device)     # GraphModel(**config['model'])\n",
    "        self.graph_model.classifier = nn.Identity()\n",
    "#         print(self.graph_model)\n",
    "        # self.graph_model = self._load_pre_trained_weights(self.graph_model)\n",
    "\n",
    "        self.out_graph_linear = torch.nn.Linear(768 * 2, 768, bias=True)\n",
    "\n",
    "        self.out_graph_projection1 = torch.nn.Linear(768, 768, bias=True)\n",
    "\n",
    "        self.bn1_graph = nn.BatchNorm1d(768)\n",
    "\n",
    "        self.out_graph_projection2 = torch.nn.Linear(768, 768, bias=True)\n",
    "\n",
    "        self.bn2_graph = nn.BatchNorm1d(768)\n",
    "\n",
    "        self.out_bert_projection1 = torch.nn.Linear(768, 768, bias=True)\n",
    "\n",
    "        self.bn1_bert = nn.BatchNorm1d(768)\n",
    "\n",
    "        self.out_bert_projection2 = torch.nn.Linear(768, 768, bias=True)\n",
    "        \n",
    "        self.bn2_bert = nn.BatchNorm1d(768)\n",
    "\n",
    "        # contrastive loss for MolCLR\n",
    "        self.nt_xent_criterion = NTXentLoss(device, self.batch_size, **config['loss'])\n",
    "        # cosine distance as loss between models\n",
    "        self.cosine_sim = torch.nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, inp_Idx, atte, labe, graphdataProcessed):\n",
    "#         inp_Idx, atte, labe, g1, g2\n",
    "\n",
    "\n",
    "#         ST =time.time()\n",
    "        bert_output = self.bert(input_ids=inp_Idx, \n",
    "                                 attention_mask=atte,\n",
    "                                 labels=labe, output_hidden_states=True)\n",
    "#         print(\"BERT\", time.time()-ST);ST=time.time()\n",
    "        bert_loss = bert_output.loss\n",
    "        bert_emb = bert_output.hidden_states[0][:, 0, :] # take emb for CLS token\n",
    "\n",
    "#         print(\"BERT-OUT\", time.time()-ST);ST=time.time()\n",
    "        graph_loss, hidden_states_1, hidden_states_2 = self.graph_step(graphdataProcessed)\n",
    "#         print(\"GRAPH\", time.time()-ST);ST=time.time()\n",
    "    \n",
    "        graph_emb = self.out_graph_linear(torch.cat((hidden_states_1, hidden_states_2), dim=-1)).mean(axis=0)\n",
    "        graph_emb_projected1 = self.out_graph_projection1(graph_emb)\n",
    "        graph_emb_projected_bn1 = self.bn1_graph(graph_emb_projected1)\n",
    "        graph_emb_projected2 = self.out_graph_projection2(torch.nn.functional.relu(graph_emb_projected_bn1))\n",
    "        graph_emb_projected_bn2 = self.bn2_graph(graph_emb_projected2)\n",
    "        #bert projections:\n",
    "        bert_emb_projected1 = self.out_bert_projection1(bert_emb)\n",
    "        bert_emb_projected_bn1 = self.bn1_bert(bert_emb_projected1)\n",
    "        bert_emb_projected2 = self.out_bert_projection2(torch.nn.functional.relu(bert_emb_projected_bn1))\n",
    "        bert_emb_projected_bn2 = self.bn2_bert(bert_emb_projected2)\n",
    "#         print(\"embed proj\",time.time()-ST);ST = time.time()\n",
    "        bimodal_loss = self.nt_xent_criterion(bert_emb_projected_bn2, graph_emb_projected_bn2)\n",
    "#         print(\"bimodal_loss\",time.time()-ST);\n",
    "#         print(\"PROJECTIONS AND LOSS\", time.time()-ST);ST=time.time()\n",
    "        return bert_loss, graph_loss, bimodal_loss, graph_emb_projected_bn2, bert_emb_projected_bn2\n",
    "\n",
    "    def graph_step(self, graphdataProcessed):\n",
    "         \n",
    "         \n",
    "#         ST=time.time() \n",
    "        batch = {}\n",
    "        for k in ['attn_bias', 'attn_edge_type', 'spatial_pos', 'in_degree', 'input_nodes', 'input_edges', 'out_degree', 'labels']:\n",
    "            shp = np.max([  np.array(e[k]).shape for e in graphdataProcessed], 0)\n",
    "            batch[k] = torch.stack([pad_to_shape(torch.tensor(e[k]), shp) for e in graphdataProcessed])\n",
    "        \n",
    "        \n",
    "#         print(\"GRAPH -data\", time.time()-ST);ST=time.time() \n",
    "        input_batch = { k: v.to(device) for k, v in batch.items() }\n",
    "#         print(\"GRAPH -data moving\", time.time()-ST);ST=time.time()\n",
    "        outputs = self.graph_model(**input_batch)\n",
    "        # get the representations and the projections\n",
    "#         print(\"GRAPHHORMER\", time.time()-ST);ST=time.time()\n",
    "        zis = outputs.logits[:config['batch_size']]\n",
    "        zjs = outputs.logits[config['batch_size']:]\n",
    "\n",
    "        ris = outputs.hidden_states[0][:, 0:config['batch_size'], :].to(device)\n",
    "        rjs = outputs.hidden_states[0][:, config['batch_size']:config['batch_size']*2, :].to(device)\n",
    "        \n",
    "        zis = torch.nn.functional.normalize(zis, dim=1)\n",
    "        zjs = torch.nn.functional.normalize(zjs, dim=1)\n",
    "    \n",
    "         \n",
    "        loss = self.nt_xent_criterion(zis, zjs)\n",
    "#         print(\"GRAPH-loss\", time.time()-ST);ST=time.time()\n",
    "        return loss, ris, rjs\n",
    "        \n",
    "#for batch_idx, (inp_Idx, atte, labe, graphdataProcessed  ) in enumerate(custom_batch_dataset):\n",
    "    #break\n",
    "\n",
    "model = MolecularBertGraph().to(device);\n",
    "#out = model(inp_Idx.to(device), atte.to(device), labe.to(device), graphdataProcessed)\n",
    "\n",
    "\n",
    "#st = time.time()\n",
    "\n",
    "#out = model(inp_Idx.to(device), atte.to(device), labe.to(device), graphdataProcessed)\n",
    "\n",
    "#print((time.time()-st)*8000/60/60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a563c1-f4c8-45e2-919e-b8a43e5f0d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f9b7a197-1abb-47e5-abff-f58ab18cfa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# st = time.time()\n",
    "# for batch_idx, (inp_Idx, atte, labe, graphdataProcessed ) in tqdm(enumerate(custom_batch_dataset)):\n",
    "#     optimizer.zero_grad()\n",
    "    \n",
    "#     bert_loss, graph_loss, bimodal_loss, emb1, emb2 = model(inp_Idx.to(device), atte.to(device), labe.to(device), graphdataProcessed)\n",
    "\n",
    "#     loss =   bert_loss +   graph_loss +   bimodal_loss\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "# (time.time()-st) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d910a1-9fe6-4742-ac2b-99c4a73d5cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14c3206c-624e-4a8a-9831-6efe0ffdeba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum([w.numel() for w in model.parameters()])/(1024**3)*4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b0c73-a897-407e-a8c7-158283f97de4",
   "metadata": {},
   "source": [
    "### Define utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "04001413-80f4-4663-ab5d-a107e839c5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epoch = config['epochs']\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), config['init_lr'], \n",
    "    weight_decay=eval(config['weight_decay'])\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=config['epochs']-config['warm_up'], \n",
    "    eta_min=0, last_epoch=-1\n",
    ")\n",
    "num_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a951a1-7c75-47bd-b275-9693eb22fe95",
   "metadata": {},
   "source": [
    "### Training (with validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4f660cb2-97a3-4102-b85e-c6e23c84429e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1.5, 3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = config['loss_params']['alpha']\n",
    "beta = config['loss_params']['beta']\n",
    "gamma = config['loss_params']['gamma']\n",
    "alpha, beta, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b5ea2e1e-8d54-4f03-914f-6b6328b5da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_counter = 0\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8a6377bd-fe1d-4246-8fc6-00b34bfbc04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(custom_batch_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "96388fce-0dc6-43a3-b800-5133ea476cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    custom_batch_dataset.setTrain(True)\n",
    "    train_tqdm = tqdm(custom_batch_dataset, unit=\"batch\")\n",
    "    train_tqdm.set_description(f'Epoch {epoch_counter}')\n",
    "    bert_loss_sum, graph_model_loss_sum, bimodal_loss_sum, loss_sum = 0, 0, 0, 0\n",
    "    model.train()\n",
    "         \n",
    "    for inp_Idx, atte, labe, graphdataProcessed in train_tqdm:\n",
    "     \n",
    "        if True:\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "            bert_loss, graph_loss, bimodal_loss, emb1, emb2 = model(inp_Idx.to(device), atte.to(device), labe.to(device), graphdataProcessed)\n",
    "    \n",
    "            loss = alpha * bert_loss + beta * graph_loss + gamma * bimodal_loss\n",
    "            loss.backward()\n",
    "    \n",
    " \n",
    "            bert_loss_sum += bert_loss.item()\n",
    "            graph_model_loss_sum += graph_loss.item()\n",
    "            bimodal_loss_sum += bimodal_loss.item()\n",
    "            loss_sum += loss.item()\n",
    "    \n",
    "            #wandb.log({“bert_loss/train”:bert_loss, “graph_loss/train”: graph_loss, \"bimodal_loss/train\": bimodal_loss, \"loss/train\": loss})\n",
    "            wandb.log({\"bert_loss/train\": bert_loss })\n",
    "            wandb.log({\"graph_loss/train\": graph_loss})\n",
    "            wandb.log({\"bimodal_loss/train\": bimodal_loss})\n",
    "            wandb.log({\"loss/train\": loss})\n",
    "    \n",
    "    \n",
    "            optimizer.step()\n",
    "#             train_tqdm.set_postfix(loss=loss.item(), bert_loss=bert_loss.item(), graph_loss=graph_loss.item(), bimodal_loss=bimodal_loss.item())\n",
    "             \n",
    "#         except:\n",
    "#             continue\n",
    "          \n",
    "    return bert_loss_sum / len(custom_batch_dataset), graph_model_loss_sum / len(custom_batch_dataset), bimodal_loss_sum / len(custom_batch_dataset), loss_sum / len(custom_batch_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f0f6af-e371-4aab-9c7e-1354b39a36e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7af92a73-77ad-4bd7-9727-7bb5337fd001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop():\n",
    "#     eval_tqdm = tqdm(eval_dataloader, unit=\"batch\")\n",
    "#     eval_tqdm.set_description(f'Epoch {epoch_counter}')\n",
    "    bert_loss_sum, graph_model_loss_sum, bimodal_loss_sum, loss_sum = 0, 0, 0, 0\n",
    "    \n",
    "    model.eval()\n",
    "    custom_batch_dataset.setTrain(False)\n",
    "     \n",
    "         \n",
    "    for batch_idx, (inp_Idx, atte, labe, graphdataProcessed  ) in enumerate(custom_batch_dataset):\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                bert_loss, graph_loss, bimodal_loss, emb1, emb2 = model(inp_Idx.to(device), atte.to(device), labe.to(device), graphdataProcessed)\n",
    "\n",
    "    \n",
    "            loss = alpha * bert_loss + beta * graph_loss + gamma * bimodal_loss\n",
    "    \n",
    "            bert_loss_sum += bert_loss.item()\n",
    "            graph_model_loss_sum += graph_loss.item()\n",
    "            bimodal_loss_sum += bimodal_loss.item()\n",
    "            loss_sum += loss.item()\n",
    "    \n",
    "#             eval_tqdm.set_postfix(loss=loss.item(), bert_loss=bert_loss.item(), graph_loss=graph_loss.item(), bimodal_loss=bimodal_loss.item())\n",
    "        \n",
    "        except:\n",
    "            continue\n",
    "    return bert_loss_sum / len(custom_batch_dataset), graph_model_loss_sum / len(custom_batch_dataset), bimodal_loss_sum / len(custom_batch_dataset), loss_sum / len(custom_batch_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec2b30-1b08-48ca-8e72-4b8e2c8b6f96",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dabd36a8-610d-4760-a984-79bf17e5b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "model_checkpoints_folder = os.path.join('ckpts')\n",
    "dir_name = datetime.now().strftime('%b%d_%H-%M-%S_graphormer_edge_masking')\n",
    "log_dir = os.path.join(model_checkpoints_folder, dir_name)\n",
    "_save_config_file(config, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c7a426df-461e-4180-9186-c5471b4df374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                                           | 0/29 [00:00<?, ?batch/s]/tmp/ipykernel_3510167/582002969.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[k] = torch.stack([pad_to_shape(torch.tensor(e[k]), shp) for e in graphdataProcessed])\n",
      "Epoch 0:  97%|██████████████████████████████████████████████████████████████████████████████████████████████▌   | 28/29 [02:28<00:05,  5.32s/batch]\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████████████████████████████████████████████████▌   | 28/29 [02:15<00:04,  4.85s/batch]\n",
      "Epoch 2:   7%|██████▊                                                                                            | 2/29 [00:14<03:09,  7.03s/batch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m best_valid_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_counter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epoch):\n\u001b[0;32m----> 6\u001b[0m     bert_loss, graph_loss, bimodal_loss, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_loss/train\u001b[39m\u001b[38;5;124m\"\u001b[39m: bert_loss}, step\u001b[38;5;241m=\u001b[39mepoch_counter)\n\u001b[1;32m      9\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph_loss/train\u001b[39m\u001b[38;5;124m\"\u001b[39m: graph_loss}, step\u001b[38;5;241m=\u001b[39mepoch_counter)\n",
      "Cell \u001b[0;32mIn[87], line 14\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 14\u001b[0m     bert_loss, graph_loss, bimodal_loss, emb1, emb2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp_Idx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matte\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraphdataProcessed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m bert_loss \u001b[38;5;241m+\u001b[39m beta \u001b[38;5;241m*\u001b[39m graph_loss \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m bimodal_loss\n\u001b[1;32m     17\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/mol/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mol/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[69], line 67\u001b[0m, in \u001b[0;36mMolecularBertGraph.forward\u001b[0;34m(self, inp_Idx, atte, labe, graphdataProcessed)\u001b[0m\n\u001b[1;32m     64\u001b[0m         bert_emb \u001b[38;5;241m=\u001b[39m bert_output\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m0\u001b[39m, :] \u001b[38;5;66;03m# take emb for CLS token\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m#         print(\"BERT-OUT\", time.time()-ST);ST=time.time()\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m         graph_loss, hidden_states_1, hidden_states_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraphdataProcessed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#         print(\"GRAPH\", time.time()-ST);ST=time.time()\u001b[39;00m\n\u001b[1;32m     70\u001b[0m         graph_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_graph_linear(torch\u001b[38;5;241m.\u001b[39mcat((hidden_states_1, hidden_states_2), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[69], line 93\u001b[0m, in \u001b[0;36mMolecularBertGraph.graph_step\u001b[0;34m(self, graphdataProcessed)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattn_bias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattn_edge_type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspatial_pos\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min_degree\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_edges\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout_degree\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     92\u001b[0m             shp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax([  np\u001b[38;5;241m.\u001b[39marray(e[k])\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m graphdataProcessed], \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m             batch[k] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpad_to_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraphdataProcessed\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m#         print(\"GRAPH -data\", time.time()-ST);ST=time.time() \u001b[39;00m\n\u001b[1;32m     97\u001b[0m         input_batch \u001b[38;5;241m=\u001b[39m { k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems() }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_iter = 0\n",
    "valid_n_iter = 0\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "for epoch_counter in range(num_epoch):\n",
    "    bert_loss, graph_loss, bimodal_loss, loss = train_loop()\n",
    "\n",
    "    wandb.log({\"bert_loss/train\": bert_loss}, step=epoch_counter)\n",
    "    wandb.log({\"graph_loss/train\": graph_loss}, step=epoch_counter)\n",
    "    wandb.log({\"bimodal_loss/train\": bimodal_loss}, step=epoch_counter)\n",
    "    wandb.log({\"loss/train\": loss}, step=epoch_counter)\n",
    "\n",
    "    bert_loss, graph_loss, bimodal_loss, loss = eval_loop()\n",
    "\n",
    "    wandb.log({\"bert_loss/eval\": bert_loss}, step=epoch_counter)\n",
    "    wandb.log({\"graph_loss/eval\": graph_loss}, step=epoch_counter)\n",
    "    wandb.log({\"bimodal_loss/eval\": bimodal_loss}, step=epoch_counter)\n",
    "    wandb.log({\"loss/eval\": loss}, step=epoch_counter)\n",
    "    \n",
    "    if loss < best_valid_loss:\n",
    "        best_valid_loss = loss\n",
    "        torch.save(model.state_dict(), os.path.join(log_dir, 'model.pth'))\n",
    "    \n",
    "    if (epoch_counter + 1) % config['save_every_n_epochs'] == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(log_dir, 'model_{}.pth'.format(str(epoch_counter))))\n",
    "\n",
    "    # warmup for the first few epochs\n",
    "    if epoch_counter >= config['warm_up']:\n",
    "        #wandb.log({\"cosine_lr_decay\": scheduler.get_last_lr()[0]}, step=epoch_counter)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ad7b9-f82b-4936-bd4f-26266cbb6b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c9ed63-eaf7-4e39-b91b-cb8d9aa00ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_batch_dataset.setTrain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2bef0-71a8-4192-a267-8858f2540103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "001bd6968bdf44c29ce98b0f2f43cfca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_697fd7a81c534176934ecbb1bab6aea4",
       "max": 128,
       "style": "IPY_MODEL_0d11365ec2c84fef8dae7febae85cd61",
       "value": 128
      }
     },
     "01bcf7bb0bd64d5ba65aa7e77c2d7766": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_28f517833bfc448a8eda2de07410524b",
       "style": "IPY_MODEL_ec1a43e481a0461895189cad09ed1643"
      }
     },
     "034248d93c724da98b492f23696f004d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_49f0cd3b748d402b9cd56bfbb5e5ef45",
       "style": "IPY_MODEL_f69eacdc16b34ec79e8194cfcce639ec",
       "value": "Map: 100%"
      }
     },
     "06fb79eaedd542f5999cfa567dc1c0c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "07c43f7c3dda454db945282137b6955e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3f62283153414e66a5d08b95525f4fc8",
       "style": "IPY_MODEL_33a7596352db4470a7baf43cf7f4af95",
       "value": " 128/128 [00:01&lt;00:00, 84.04 examples/s]"
      }
     },
     "08e3c2b3bbee41a3af0c93d2e814100a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_aec4f986c470429ea7ac919821ef307e",
       "max": 128,
       "style": "IPY_MODEL_d1e5e029624b45f39b4a3a7e1d8c97c1",
       "value": 128
      }
     },
     "0d11365ec2c84fef8dae7febae85cd61": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "10454f3f548044a38b93cd87f33de5b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "120ff4ba880241a3bf54b45bbbc608c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "12c8f9cf3a3b4a0b9d64ceba8a6e59d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "12db03965c26447fa8eda1a68bba2479": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "12de4ab0f5c44c829b489a757c15d6d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7ae98f58290c411dbb869e72c872d462",
        "IPY_MODEL_001bd6968bdf44c29ce98b0f2f43cfca",
        "IPY_MODEL_1c937906af9b474489766dbd132681ba"
       ],
       "layout": "IPY_MODEL_ef7255e151cc49e5a569a801f925ef98"
      }
     },
     "14627d7ddb2441e38cefe1364e15312d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_2a1152efc937483c834550b9e0c93a06",
       "max": 128,
       "style": "IPY_MODEL_29f1707225504c9291a5033550bc5c7f",
       "value": 128
      }
     },
     "160ef709d89d4432a9e636cb96c31b93": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "17f93152a35e4882a5fbc2d661ac0e2a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_49fa17bed0ce42ef97d8ec78193f14fd",
       "style": "IPY_MODEL_f3900afec7f3412e9a32a5e1a0159cf2",
       "value": " 128/128 [00:01&lt;00:00, 100.34 examples/s]"
      }
     },
     "19a732e7382c4341bb3d0146035ad15b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1c937906af9b474489766dbd132681ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_63b7a47542d34c1899b515eddcaaf23c",
       "style": "IPY_MODEL_c09f3cf15b76434da0a2df8e8bc59b58",
       "value": " 128/128 [00:01&lt;00:00, 122.10 examples/s]"
      }
     },
     "1f39d98491ad455a8b4c8ac78b846f09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_50fbfc90c1fc49c4a2610a6c6b2f5882",
        "IPY_MODEL_14627d7ddb2441e38cefe1364e15312d",
        "IPY_MODEL_55a7116f3c9f41b2b2370f5af42b25c3"
       ],
       "layout": "IPY_MODEL_a357cd829c9e4459b3a388bab37cfea5"
      }
     },
     "1fc145cee352441fab2a53dca110a099": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_cc8806af10c34698b0e151752dbea9be",
       "style": "IPY_MODEL_f8dc05ae8d8547e68e9b386d1867ce80",
       "value": " 128/128 [00:01&lt;00:00, 101.60 examples/s]"
      }
     },
     "21b649a772e14288b6e4b6036d717649": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "23423d1863f14371aeb763cd5b918120": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "234dfebf7ba3461783121b4c1d93f46b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_555399d0e67e40cca8b69341290a434d",
        "IPY_MODEL_c8c1e8910b5945758e4b4690b006c7c5",
        "IPY_MODEL_8c08c27b60c8441caa1f142b9b0f5249"
       ],
       "layout": "IPY_MODEL_3909ca631e72473ea9d96a4c997d32cc"
      }
     },
     "23bae0161d64463b9d012541c3c9b20a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "24bf1e465f394cd49c67b90a59f5f8ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_89778ed8510748b89c649ed5fdd2cb52",
       "style": "IPY_MODEL_e68b085c599f4115b01e71b0266d5ac2",
       "value": " 128/128 [00:01&lt;00:00, 126.64 examples/s]"
      }
     },
     "28f517833bfc448a8eda2de07410524b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "298a7ea0a1e54e3ea7e54dfccba227a4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "29f1707225504c9291a5033550bc5c7f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2a1152efc937483c834550b9e0c93a06": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2d8718ff539b422ba150d19b1c68cf85": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2e14035df25f45c5a652a8f26bc16d5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_468a04876ddf4a7fad9956b6fe02610d",
       "max": 1,
       "style": "IPY_MODEL_d8feacb594104eceab7f006dd7b5f455"
      }
     },
     "2fc542818b684c57b999ce4bba15902d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "339178664a554eacb8679a4fb73583aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_c53a27d0baed42d9be4193d737abcb23",
       "max": 128,
       "style": "IPY_MODEL_714103c52ec946ed90402f2b4b8bea0a",
       "value": 128
      }
     },
     "33a7596352db4470a7baf43cf7f4af95": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "366fc6f1c3614cec8941cb49d9aed872": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_fd7c6787e1274899ba35fc4e9de188c7",
       "style": "IPY_MODEL_b60964fa17cd4a138e095ab8f16bc451",
       "value": " 11/1406 [07:05&lt;10:50:08, 27.96s/batch, bert_loss=9.28, bimodal_loss=28.7, graph_loss=3.22, loss=100]"
      }
     },
     "3909ca631e72473ea9d96a4c997d32cc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "394efdce51744e13b6d2828b280f5c3c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "39abe8a26f99473f9f318dee28f0bdd6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3e255d7459854b3dafca5b52e9178f66",
        "IPY_MODEL_08e3c2b3bbee41a3af0c93d2e814100a",
        "IPY_MODEL_17f93152a35e4882a5fbc2d661ac0e2a"
       ],
       "layout": "IPY_MODEL_dfca09d1256e4b0896deefa87d946cb6"
      }
     },
     "39b82e0a0882436a9e4302aec036d8a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3b15266bb8284ab08ae35c0465f27d78": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3e1a154666314b9399721d8952adfcd6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_be92955980a54daf881005f297b38db8",
       "style": "IPY_MODEL_9f4bbd7d49094f1786792a29e82a3a1a",
       "value": "Map: 100%"
      }
     },
     "3e255d7459854b3dafca5b52e9178f66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_69aa7eb736d94844ac2cb274ca6710cf",
       "style": "IPY_MODEL_e3d89def077d47ec9c87d23747da693c",
       "value": "Map: 100%"
      }
     },
     "3f62283153414e66a5d08b95525f4fc8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "40cb9b36f03e40c08caa687de04d2ab4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "41cc1ba4704e409688674182b17e9b22": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4200baa8f98544df9099f6299ba94760": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "468a04876ddf4a7fad9956b6fe02610d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "46907be027a94ad18b4bd1bf18852b9e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_01bcf7bb0bd64d5ba65aa7e77c2d7766",
        "IPY_MODEL_2e14035df25f45c5a652a8f26bc16d5d"
       ],
       "layout": "IPY_MODEL_f5559769f8144db1930c873fb955c0b1"
      }
     },
     "4759aff346d94af9a829675ef0faa6e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_7617098b901a4c8b82acac76734f88cd",
       "max": 128,
       "style": "IPY_MODEL_12c8f9cf3a3b4a0b9d64ceba8a6e59d8",
       "value": 128
      }
     },
     "49f0cd3b748d402b9cd56bfbb5e5ef45": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "49fa17bed0ce42ef97d8ec78193f14fd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4a3e9fb9819a457183a191759b1e96ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8c540762e8ca4828a5db5346ffb16ba7",
        "IPY_MODEL_4759aff346d94af9a829675ef0faa6e7",
        "IPY_MODEL_a762ff4e5f984156ae84c74b0c69259f"
       ],
       "layout": "IPY_MODEL_2d8718ff539b422ba150d19b1c68cf85"
      }
     },
     "50fbfc90c1fc49c4a2610a6c6b2f5882": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_917859aec4a64f55ab05ede560c8d7db",
       "style": "IPY_MODEL_61f94558aa2742a2a3fb213a24adebab",
       "value": "Map: 100%"
      }
     },
     "554394a61189405ca3edb3b767e19592": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "555399d0e67e40cca8b69341290a434d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3b15266bb8284ab08ae35c0465f27d78",
       "style": "IPY_MODEL_602277db52804760b93506e0be1c5bab",
       "value": "Map: 100%"
      }
     },
     "55a7116f3c9f41b2b2370f5af42b25c3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_554394a61189405ca3edb3b767e19592",
       "style": "IPY_MODEL_fff667d8a5874e1bbc693fa029fb5c4c",
       "value": " 128/128 [00:01&lt;00:00, 106.69 examples/s]"
      }
     },
     "569018a97fa74a429df43beaaabb812e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5e275b7fd7054499b5a00fc21deee91d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "602277db52804760b93506e0be1c5bab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "615889f560ec4ca7a337665267bcba8c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "61f94558aa2742a2a3fb213a24adebab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "630b7594d37743d0af78f060af96891e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d52da5826cea4b7eaa9ab6358e450118",
        "IPY_MODEL_804f1a2800094a45b3d9059c7bff3299",
        "IPY_MODEL_07c43f7c3dda454db945282137b6955e"
       ],
       "layout": "IPY_MODEL_615889f560ec4ca7a337665267bcba8c"
      }
     },
     "6361bf1073a844808b4ee93e1aa28bdf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_dc0ab4ed7e584329a2eda31ee4e2bb7f",
       "style": "IPY_MODEL_06fb79eaedd542f5999cfa567dc1c0c8"
      }
     },
     "63b7a47542d34c1899b515eddcaaf23c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "697fd7a81c534176934ecbb1bab6aea4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "69aa7eb736d94844ac2cb274ca6710cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6adc57ef1f594aaa91945c64a11d62cb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "714103c52ec946ed90402f2b4b8bea0a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "72613281e0454d8cbed666ce1a729a21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_9963e041aaba4efa88e605d2015b087f",
        "IPY_MODEL_8eb8425195224d1da095990369d40b62",
        "IPY_MODEL_24bf1e465f394cd49c67b90a59f5f8ea"
       ],
       "layout": "IPY_MODEL_b5c870a79122490d9b3231074fe86b43"
      }
     },
     "7617098b901a4c8b82acac76734f88cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7adb3853e5944a2a8eb2fd34f2f3a479": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_dd54aa09db66466297b36b8c21b36235",
       "style": "IPY_MODEL_5e275b7fd7054499b5a00fc21deee91d",
       "value": "Map: 100%"
      }
     },
     "7ae98f58290c411dbb869e72c872d462": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d07ec0d8b08741f0b381b75b613c4db1",
       "style": "IPY_MODEL_23423d1863f14371aeb763cd5b918120",
       "value": "Map: 100%"
      }
     },
     "7c4bf205ff8e4b528e65b0d9c0b3e3f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7d434cc4803140e1b88f75fd33d41aa8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "danger",
       "layout": "IPY_MODEL_b7f3c11c882d4d59af310e991ab83858",
       "max": 1406,
       "style": "IPY_MODEL_8557dd1e6c9848568150a61684f07db2",
       "value": 11
      }
     },
     "7e7dc9ade9f845f68ee1a766a5c8d7aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "804f1a2800094a45b3d9059c7bff3299": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_6adc57ef1f594aaa91945c64a11d62cb",
       "max": 128,
       "style": "IPY_MODEL_160ef709d89d4432a9e636cb96c31b93",
       "value": 128
      }
     },
     "8557dd1e6c9848568150a61684f07db2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "88e8392d1c3d4283a0a240299206a2fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_6361bf1073a844808b4ee93e1aa28bdf",
        "IPY_MODEL_9e6bfa5dceab436bb62357cbd8cad4ec"
       ],
       "layout": "IPY_MODEL_e8b682454d414d03ae0e605d57518a33"
      }
     },
     "89778ed8510748b89c649ed5fdd2cb52": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8c08c27b60c8441caa1f142b9b0f5249": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_12db03965c26447fa8eda1a68bba2479",
       "style": "IPY_MODEL_afb34d736f89404fa3617ec9ef6d0f97",
       "value": " 128/128 [00:01&lt;00:00, 100.67 examples/s]"
      }
     },
     "8c540762e8ca4828a5db5346ffb16ba7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_39b82e0a0882436a9e4302aec036d8a9",
       "style": "IPY_MODEL_40cb9b36f03e40c08caa687de04d2ab4",
       "value": "Map: 100%"
      }
     },
     "8eb8425195224d1da095990369d40b62": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_f364a73d6188473ab5d398ed4a63ed1a",
       "max": 128,
       "style": "IPY_MODEL_f45f60b8167d40f0aed6be4c8b318f11",
       "value": 128
      }
     },
     "917859aec4a64f55ab05ede560c8d7db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9963e041aaba4efa88e605d2015b087f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_569018a97fa74a429df43beaaabb812e",
       "style": "IPY_MODEL_10454f3f548044a38b93cd87f33de5b3",
       "value": "Map: 100%"
      }
     },
     "9bbf3dd351a449d6bcd4d491c740f2e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9e6bfa5dceab436bb62357cbd8cad4ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_394efdce51744e13b6d2828b280f5c3c",
       "max": 1,
       "style": "IPY_MODEL_fbeb43fbcd8740dbbba19002bf062667"
      }
     },
     "9f4bbd7d49094f1786792a29e82a3a1a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a18a1c895e204077b726973b0d6f6af8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a357cd829c9e4459b3a388bab37cfea5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a762ff4e5f984156ae84c74b0c69259f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e69877c43271411889e9a69139684c67",
       "style": "IPY_MODEL_efc1d0ab9a6c42d99c762b2cabf3d46e",
       "value": " 128/128 [00:02&lt;00:00, 82.69 examples/s]"
      }
     },
     "a8342c5f08d54af096065721ff943165": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a8a88138421640098599df34289f3dbe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_120ff4ba880241a3bf54b45bbbc608c4",
       "max": 128,
       "style": "IPY_MODEL_7e7dc9ade9f845f68ee1a766a5c8d7aa",
       "value": 128
      }
     },
     "aec4f986c470429ea7ac919821ef307e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "afb34d736f89404fa3617ec9ef6d0f97": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b0885e42995d4c3fa90abe0c52be3f68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b90125a84ff343c6934da0cbb93ffffe",
       "style": "IPY_MODEL_9bbf3dd351a449d6bcd4d491c740f2e2",
       "value": " 128/128 [00:01&lt;00:00, 85.69 examples/s]"
      }
     },
     "b18c608833ab438ab48c567ccb36ea05": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_41cc1ba4704e409688674182b17e9b22",
       "max": 128,
       "style": "IPY_MODEL_21b649a772e14288b6e4b6036d717649",
       "value": 128
      }
     },
     "b5c870a79122490d9b3231074fe86b43": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b60964fa17cd4a138e095ab8f16bc451": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b6bc0018c4d84a3d9f32ccb21e7cae07": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b7f3c11c882d4d59af310e991ab83858": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b90125a84ff343c6934da0cbb93ffffe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bc8d06e3de6a44c48507621d7ba24494": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_3e1a154666314b9399721d8952adfcd6",
        "IPY_MODEL_b18c608833ab438ab48c567ccb36ea05",
        "IPY_MODEL_b0885e42995d4c3fa90abe0c52be3f68"
       ],
       "layout": "IPY_MODEL_23bae0161d64463b9d012541c3c9b20a"
      }
     },
     "be92955980a54daf881005f297b38db8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bffb03f614e84faaaecb7f43e4fc94f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_034248d93c724da98b492f23696f004d",
        "IPY_MODEL_a8a88138421640098599df34289f3dbe",
        "IPY_MODEL_c87f3f624af8488eb9ed77b7edc1880e"
       ],
       "layout": "IPY_MODEL_a8342c5f08d54af096065721ff943165"
      }
     },
     "c09f3cf15b76434da0a2df8e8bc59b58": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c53a27d0baed42d9be4193d737abcb23": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c87f3f624af8488eb9ed77b7edc1880e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b6bc0018c4d84a3d9f32ccb21e7cae07",
       "style": "IPY_MODEL_7c4bf205ff8e4b528e65b0d9c0b3e3f8",
       "value": " 128/128 [00:01&lt;00:00, 75.79 examples/s]"
      }
     },
     "c8c1e8910b5945758e4b4690b006c7c5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_2fc542818b684c57b999ce4bba15902d",
       "max": 128,
       "style": "IPY_MODEL_e033acd08e0f4f219edff0fc1f01c883",
       "value": 128
      }
     },
     "cc8806af10c34698b0e151752dbea9be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d07ec0d8b08741f0b381b75b613c4db1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d1e5e029624b45f39b4a3a7e1d8c97c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d52da5826cea4b7eaa9ab6358e450118": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4200baa8f98544df9099f6299ba94760",
       "style": "IPY_MODEL_f847b2a36b064820b66bd52aa480f417",
       "value": "Map: 100%"
      }
     },
     "d7c7b911cc464a99980fc787fb04d0f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7adb3853e5944a2a8eb2fd34f2f3a479",
        "IPY_MODEL_339178664a554eacb8679a4fb73583aa",
        "IPY_MODEL_1fc145cee352441fab2a53dca110a099"
       ],
       "layout": "IPY_MODEL_f51569a661b14026ba0bd74e269ce2a6"
      }
     },
     "d8feacb594104eceab7f006dd7b5f455": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "dc0ab4ed7e584329a2eda31ee4e2bb7f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dd54aa09db66466297b36b8c21b36235": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dfca09d1256e4b0896deefa87d946cb6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e033acd08e0f4f219edff0fc1f01c883": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e3d89def077d47ec9c87d23747da693c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e68b085c599f4115b01e71b0266d5ac2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e69877c43271411889e9a69139684c67": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e8b682454d414d03ae0e605d57518a33": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "eb468119ed2e4729812d31e1af1e0767": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_fd59920bb81f4aa2a3bc143023fc37f3",
        "IPY_MODEL_7d434cc4803140e1b88f75fd33d41aa8",
        "IPY_MODEL_366fc6f1c3614cec8941cb49d9aed872"
       ],
       "layout": "IPY_MODEL_298a7ea0a1e54e3ea7e54dfccba227a4"
      }
     },
     "ec1a43e481a0461895189cad09ed1643": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "LabelStyleModel",
      "state": {
       "description_width": "",
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "ef7255e151cc49e5a569a801f925ef98": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "efc1d0ab9a6c42d99c762b2cabf3d46e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f364a73d6188473ab5d398ed4a63ed1a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f3900afec7f3412e9a32a5e1a0159cf2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f45f60b8167d40f0aed6be4c8b318f11": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f51569a661b14026ba0bd74e269ce2a6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f5559769f8144db1930c873fb955c0b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f69eacdc16b34ec79e8194cfcce639ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f847b2a36b064820b66bd52aa480f417": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f8dc05ae8d8547e68e9b386d1867ce80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fbeb43fbcd8740dbbba19002bf062667": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "fd59920bb81f4aa2a3bc143023fc37f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_19a732e7382c4341bb3d0146035ad15b",
       "style": "IPY_MODEL_a18a1c895e204077b726973b0d6f6af8",
       "value": "Epoch 0:   1%"
      }
     },
     "fd7c6787e1274899ba35fc4e9de188c7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fff667d8a5874e1bbc693fa029fb5c4c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
